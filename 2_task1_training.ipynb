{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:  Polyp Segmentation Task (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import image\n",
    "from numpy import savez_compressed\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "from gan import Generator, Discriminator  # models are in gan.py file\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_im_msk(img_files,msk_files):\n",
    "    \n",
    "    pure_img_names=[]\n",
    "    pure_msk_names=[]\n",
    "\n",
    "    for im in img_files:\n",
    "        pure_img_names+= [os.path.basename(im)]\n",
    "    for msk in msk_files:\n",
    "        pure_msk_names+= [os.path.basename(msk)]\n",
    "    \n",
    "    return(pure_img_names,pure_msk_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data from the resized set\n",
    "data_dir = \"./data/task1/resized/train/\"\n",
    "img_files = glob.glob(data_dir+\"images/*.jpg\")\n",
    "msk_files=glob.glob(data_dir+\"masks/*.jpg\")\n",
    "pure_img_names,pure_msk_names=match_im_msk(img_files,msk_files)\n",
    "pure_img_names==pure_msk_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the image data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std =[0.5, 0.5, 0.5],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loaded image ./data/task1/resized/train/images\\cju0qkwl35piu0993l0dewei2.jpg (3, 512, 512)\n",
      "> loaded image ./data/task1/resized/train/images\\cju5bdwa3aatx0818b79i18zf.jpg (3, 512, 512)\n",
      "> loaded mask ./data/task1/resized/train/masks\\cju0qkwl35piu0993l0dewei2.jpg (3, 512, 512)\n",
      "> loaded mask ./data/task1/resized/train/masks\\cju5bdwa3aatx0818b79i18zf.jpg (3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loaded_images = list()\n",
    "loaded_masks = list()\n",
    "\n",
    "for i,filename in enumerate(img_files):\n",
    "    # load image\n",
    "    img_data = image.imread(filename)\n",
    "    # store loaded image\n",
    "    img_data_tr=transform(img_data.copy()).numpy()\n",
    "    loaded_images.append(img_data_tr)\n",
    "    if i%500 ==0:\n",
    "        print('> loaded image %s %s' % (filename, img_data_tr.shape))\n",
    "for i,filename in enumerate(msk_files):\n",
    "    # load image\n",
    "    msk_data = image.imread(filename)\n",
    "    msk_data_tr=transform(msk_data.copy()).numpy()\n",
    "    # store loaded image\n",
    "    loaded_masks.append(msk_data_tr)\n",
    "    if i%500 ==0:\n",
    "        print('> loaded mask %s %s' % (filename, msk_data_tr.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can show some samples\n",
    "# s=np.random.randint(0,900)\n",
    "# print(s)\n",
    "# for j in range(s,s+1):\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     ax1.imshow(loaded_images[j])\n",
    "#     ax2.imshow(loaded_masks[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf= 64   # generator number of filters\n",
    "ndf= 64   # discriminator number of filters\n",
    "lrG=0.002 #0.0002\n",
    "lrD=0.002\n",
    "lamb=100  # loss weighting parameters\n",
    "beta1=0.5 \n",
    "beta2=0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples 1000\n",
      "image dim (3, 512, 512)\n",
      "mask  dim (3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples\",len(loaded_images))\n",
    "print(\"image dim\", loaded_images[0].shape)\n",
    "print(\"mask  dim\",loaded_masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): ConvBlock(\n",
      "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): ConvBlock(\n",
      "    (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "G = Generator(3, ngf, 3)\n",
    "D = Discriminator(6, ndf, 1)\n",
    "G.normal_weight_init(mean=0.0, std=0.02)\n",
    "D.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "print(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): ConvBlock(\n",
      "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv6): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv7): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv8): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv1): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv2): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv3): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv4): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv5): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv6): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv7): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv8): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to track the training epochs and runs\n",
    "\n",
    "if not os.path.exists('id_log.txt'):\n",
    "    with open('id_log.txt','w') as f:\n",
    "        f.write('0')\n",
    "with open('id_log.txt','r') as f:\n",
    "    run_id = int(f.read())\n",
    "    run_id+=1 \n",
    "\n",
    "with open('id_log.txt','w') as f:\n",
    "    f.write(str(run_id))\n",
    "\n",
    "model_dir = 'GAN_model/'\n",
    "save_dir='GAN_results/'\n",
    "ex_dir=save_dir+str(run_id)+\"/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "if not os.path.exists(ex_dir):\n",
    "    os.makedirs(ex_dir)\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "BCE_loss = torch.nn.BCELoss()#.cuda()\n",
    "L1_loss = torch.nn.L1Loss()#.cuda()\n",
    "\n",
    "# I used my laptop, cuda is an option\n",
    "#G.cuda()\n",
    "#D.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 1000 1000\n"
     ]
    }
   ],
   "source": [
    "# this is the last training trial, I used the whole dataset, but I sued 900:100 training:validation for parameters tuning\n",
    "\n",
    "\n",
    "split_point=len(loaded_images)\n",
    "X_train=np.array(loaded_images[:split_point])\n",
    "Y_train=np.array(loaded_masks[:split_point])\n",
    "im_train_names=np.array(pure_img_names[:split_point])\n",
    "msk_train_names=np.array(pure_msk_names[:split_point])\n",
    "\n",
    "# X_val=np.array(loaded_images[split_point:])\n",
    "# Y_val=np.array(loaded_masks[split_point:])\n",
    "# im_val_names=np.array(pure_img_names[split_point:])\n",
    "# msk_val_names=np.array(pure_msk_names[split_point:])\n",
    "\n",
    "print(\"train set:\", len(Y_train),len(X_train))\n",
    "# print(\"val   set:\", len(Y_val),len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset class for the loader \n",
    "class Seg(Dataset):\n",
    "    def __init__(self,X,Y,im_names,mask_names):\n",
    "\n",
    "        self.len=X.shape[0]\n",
    "        self.xdata=torch.from_numpy(X)\n",
    "        self.ydata=torch.from_numpy(Y)\n",
    "        \n",
    "        self.xnames=im_names\n",
    "        self.ynames=mask_names\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return (self.xdata[index],self.ydata[index],self.xnames[index],self.ynames[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to save sample during training\n",
    "# I created the function to save features from the middle of the netork\n",
    "# the purpouse was to use them in creating graph later, however I stick to the GANs in this phase\n",
    "\n",
    "def save_epoch_pairs(imgs_batch,\n",
    "                     masks_batch,\n",
    "                     gen_masks_batch, \n",
    "                     im_fn_batch,\n",
    "                     msk_fn_batch,\n",
    "                     im_dir_name,\n",
    "                     msk_dir_name,\n",
    "                     gen_msk_dir_name,\n",
    "                     feat_dir_name):\n",
    "    \n",
    "   \n",
    "    for i in range(len(imgs_batch)):\n",
    "        \n",
    "        im_fn =im_fn_batch[i]\n",
    "        msk_fn=msk_fn_batch[i]\n",
    "        \n",
    "\n",
    "        \n",
    "        gen_mask=gen_masks_batch[i].detach().numpy()#.permute(1,2,0).detach().numpy().astype(np.uint8)\n",
    "\n",
    "        save_genmsk_fn = gen_msk_dir_name + msk_fn\n",
    "        gen_mask_ = (((gen_mask -gen_mask.min()) * 255) / (gen_mask.max() - gen_mask.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        msk_gen = Image.fromarray(gen_mask_)\n",
    "        msk_gen.save(save_genmsk_fn)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        #save_msk_fn = msk_dir_name + msk_fn\n",
    "        #msk_real = Image.fromarray(target_msk)\n",
    "       # msk_real.save(save_msk_fn)\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "        \n",
    "        \n",
    "        # Saving features....\n",
    "#         save_feat_fn = feat_dir_name + msk_fn[:-3]+\"npz\"\n",
    "#         feat = features_batch[i].detach().numpy()\n",
    "#         savez_compressed(save_feat_fn, feat)\n",
    "        #torch.save(feat, save_feat_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size=8\n",
    "\n",
    "train_seg=Seg(X_train,Y_train,im_train_names,msk_train_names)\n",
    "train_loader=DataLoader(dataset=train_seg,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# val_seg=Seg(X_val,Y_val,im_val_names,msk_val_names)\n",
    "# val_loader=DataLoader(dataset=val_seg,batch_size=2,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizers\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/125], D_loss: 0.8207, G_loss: 101.8363\n",
      "Epoch [1/10], Step [2/125], D_loss: 3.5759, G_loss: 86.8924\n",
      "Epoch [1/10], Step [3/125], D_loss: 4.8366, G_loss: 68.9630\n",
      "Epoch [1/10], Step [4/125], D_loss: 1.8384, G_loss: 66.5402\n",
      "Epoch [1/10], Step [5/125], D_loss: 1.4256, G_loss: 49.0444\n",
      "Epoch [1/10], Step [6/125], D_loss: 1.3846, G_loss: 47.7315\n",
      "Epoch [1/10], Step [7/125], D_loss: 0.8463, G_loss: 37.9742\n",
      "Epoch [1/10], Step [8/125], D_loss: 0.9875, G_loss: 40.8745\n",
      "Epoch [1/10], Step [9/125], D_loss: 0.7572, G_loss: 28.5668\n",
      "Epoch [1/10], Step [10/125], D_loss: 0.9490, G_loss: 28.2638\n",
      "Epoch [1/10], Step [11/125], D_loss: 0.6944, G_loss: 21.5612\n",
      "Epoch [1/10], Step [12/125], D_loss: 0.7441, G_loss: 26.4606\n",
      "Epoch [1/10], Step [13/125], D_loss: 0.7678, G_loss: 31.3790\n",
      "Epoch [1/10], Step [14/125], D_loss: 0.7797, G_loss: 26.7046\n",
      "Epoch [1/10], Step [15/125], D_loss: 1.0174, G_loss: 36.2731\n",
      "Epoch [1/10], Step [16/125], D_loss: 0.9700, G_loss: 20.4294\n",
      "Epoch [1/10], Step [17/125], D_loss: 0.8010, G_loss: 25.1007\n",
      "Epoch [1/10], Step [18/125], D_loss: 0.7459, G_loss: 32.9869\n",
      "Epoch [1/10], Step [19/125], D_loss: 0.8236, G_loss: 25.3861\n",
      "Epoch [1/10], Step [20/125], D_loss: 0.8304, G_loss: 17.2682\n",
      "Epoch [1/10], Step [21/125], D_loss: 0.7490, G_loss: 28.7070\n",
      "Epoch [1/10], Step [22/125], D_loss: 0.7277, G_loss: 22.2343\n",
      "Epoch [1/10], Step [23/125], D_loss: 0.7280, G_loss: 25.5533\n",
      "Epoch [1/10], Step [24/125], D_loss: 0.7187, G_loss: 22.3538\n",
      "Epoch [1/10], Step [25/125], D_loss: 0.6690, G_loss: 42.3863\n",
      "Epoch [1/10], Step [26/125], D_loss: 0.6631, G_loss: 40.9961\n",
      "Epoch [1/10], Step [27/125], D_loss: 0.8318, G_loss: 44.2194\n",
      "Epoch [1/10], Step [28/125], D_loss: 0.7880, G_loss: 42.4705\n",
      "Epoch [1/10], Step [29/125], D_loss: 0.6317, G_loss: 44.7133\n",
      "Epoch [1/10], Step [30/125], D_loss: 0.6686, G_loss: 24.2142\n",
      "Epoch [1/10], Step [31/125], D_loss: 0.8429, G_loss: 28.2594\n",
      "Epoch [1/10], Step [32/125], D_loss: 0.7922, G_loss: 33.0077\n",
      "Epoch [1/10], Step [33/125], D_loss: 0.8155, G_loss: 35.8552\n",
      "Epoch [1/10], Step [34/125], D_loss: 0.5013, G_loss: 35.2281\n",
      "Epoch [1/10], Step [35/125], D_loss: 0.5034, G_loss: 25.2275\n",
      "Epoch [1/10], Step [36/125], D_loss: 0.7015, G_loss: 41.9140\n",
      "Epoch [1/10], Step [37/125], D_loss: 0.4708, G_loss: 26.9273\n",
      "Epoch [1/10], Step [38/125], D_loss: 0.8195, G_loss: 29.0411\n",
      "Epoch [1/10], Step [39/125], D_loss: 0.9372, G_loss: 34.0319\n",
      "Epoch [1/10], Step [40/125], D_loss: 0.7285, G_loss: 22.3174\n",
      "Epoch [1/10], Step [41/125], D_loss: 0.9846, G_loss: 13.7470\n",
      "Epoch [1/10], Step [42/125], D_loss: 0.7294, G_loss: 22.3635\n",
      "Epoch [1/10], Step [43/125], D_loss: 0.6175, G_loss: 31.3741\n",
      "Epoch [1/10], Step [44/125], D_loss: 0.4723, G_loss: 43.6062\n",
      "Epoch [1/10], Step [45/125], D_loss: 0.3459, G_loss: 30.2215\n",
      "Epoch [1/10], Step [46/125], D_loss: 0.3420, G_loss: 24.4354\n",
      "Epoch [1/10], Step [47/125], D_loss: 0.3386, G_loss: 24.8185\n",
      "Epoch [1/10], Step [48/125], D_loss: 0.4803, G_loss: 27.0443\n",
      "Epoch [1/10], Step [49/125], D_loss: 0.4754, G_loss: 18.4684\n",
      "Epoch [1/10], Step [50/125], D_loss: 0.4475, G_loss: 34.9113\n",
      "Epoch [1/10], Step [51/125], D_loss: 0.2550, G_loss: 43.7858\n",
      "Epoch [1/10], Step [52/125], D_loss: 0.2373, G_loss: 27.1007\n",
      "Epoch [1/10], Step [53/125], D_loss: 0.4132, G_loss: 50.5665\n",
      "Epoch [1/10], Step [54/125], D_loss: 0.8536, G_loss: 50.2165\n",
      "Epoch [1/10], Step [55/125], D_loss: 1.1311, G_loss: 34.4342\n",
      "Epoch [1/10], Step [56/125], D_loss: 0.8322, G_loss: 36.0754\n",
      "Epoch [1/10], Step [57/125], D_loss: 1.2252, G_loss: 25.2506\n",
      "Epoch [1/10], Step [58/125], D_loss: 0.7049, G_loss: 26.5457\n",
      "Epoch [1/10], Step [59/125], D_loss: 1.2943, G_loss: 35.7156\n",
      "Epoch [1/10], Step [60/125], D_loss: 1.4076, G_loss: 39.2772\n",
      "Epoch [1/10], Step [61/125], D_loss: 0.6267, G_loss: 19.6236\n",
      "Epoch [1/10], Step [62/125], D_loss: 0.9161, G_loss: 23.5216\n",
      "Epoch [1/10], Step [63/125], D_loss: 0.5357, G_loss: 49.4928\n",
      "Epoch [1/10], Step [64/125], D_loss: 0.4907, G_loss: 27.2464\n",
      "Epoch [1/10], Step [65/125], D_loss: 0.6493, G_loss: 25.0417\n",
      "Epoch [1/10], Step [66/125], D_loss: 0.4407, G_loss: 46.0402\n",
      "Epoch [1/10], Step [67/125], D_loss: 0.3897, G_loss: 25.5088\n",
      "Epoch [1/10], Step [68/125], D_loss: 0.6330, G_loss: 42.2716\n",
      "Epoch [1/10], Step [69/125], D_loss: 0.9616, G_loss: 39.9298\n",
      "Epoch [1/10], Step [70/125], D_loss: 1.0775, G_loss: 19.1776\n",
      "Epoch [1/10], Step [71/125], D_loss: 0.6302, G_loss: 37.8683\n",
      "Epoch [1/10], Step [72/125], D_loss: 0.4402, G_loss: 29.5277\n",
      "Epoch [1/10], Step [73/125], D_loss: 0.7990, G_loss: 46.7939\n",
      "Epoch [1/10], Step [74/125], D_loss: 0.6477, G_loss: 33.2815\n",
      "Epoch [1/10], Step [75/125], D_loss: 0.7047, G_loss: 21.1006\n",
      "Epoch [1/10], Step [76/125], D_loss: 0.3099, G_loss: 28.6621\n",
      "Epoch [1/10], Step [77/125], D_loss: 0.2266, G_loss: 49.4331\n",
      "Epoch [1/10], Step [78/125], D_loss: 0.2009, G_loss: 38.3235\n",
      "Epoch [1/10], Step [79/125], D_loss: 0.2105, G_loss: 31.3047\n",
      "Epoch [1/10], Step [80/125], D_loss: 0.2424, G_loss: 26.8468\n",
      "Epoch [1/10], Step [81/125], D_loss: 0.2206, G_loss: 37.4773\n",
      "Epoch [1/10], Step [82/125], D_loss: 0.3089, G_loss: 19.6042\n",
      "Epoch [1/10], Step [83/125], D_loss: 0.2450, G_loss: 46.5781\n",
      "Epoch [1/10], Step [84/125], D_loss: 0.3154, G_loss: 18.2884\n",
      "Epoch [1/10], Step [85/125], D_loss: 0.2241, G_loss: 49.2872\n",
      "Epoch [1/10], Step [86/125], D_loss: 0.3637, G_loss: 21.8968\n",
      "Epoch [1/10], Step [87/125], D_loss: 0.2109, G_loss: 43.7895\n",
      "Epoch [1/10], Step [88/125], D_loss: 0.3033, G_loss: 25.6020\n",
      "Epoch [1/10], Step [89/125], D_loss: 0.3602, G_loss: 35.7954\n",
      "Epoch [1/10], Step [90/125], D_loss: 0.2634, G_loss: 28.2898\n",
      "Epoch [1/10], Step [91/125], D_loss: 0.3610, G_loss: 41.5072\n",
      "Epoch [1/10], Step [92/125], D_loss: 0.0908, G_loss: 59.9465\n",
      "Epoch [1/10], Step [93/125], D_loss: 0.0631, G_loss: 41.7248\n",
      "Epoch [1/10], Step [94/125], D_loss: 0.1070, G_loss: 27.0992\n",
      "Epoch [1/10], Step [95/125], D_loss: 0.1578, G_loss: 17.4436\n",
      "Epoch [1/10], Step [96/125], D_loss: 0.0626, G_loss: 45.7323\n",
      "Epoch [1/10], Step [97/125], D_loss: 0.0863, G_loss: 43.1718\n",
      "Epoch [1/10], Step [98/125], D_loss: 0.0449, G_loss: 27.8972\n",
      "Epoch [1/10], Step [99/125], D_loss: 0.0541, G_loss: 32.1001\n",
      "Epoch [1/10], Step [100/125], D_loss: 0.0492, G_loss: 46.6350\n",
      "Epoch [1/10], Step [101/125], D_loss: 0.0657, G_loss: 28.1972\n",
      "Epoch [1/10], Step [102/125], D_loss: 0.0460, G_loss: 30.1588\n",
      "Epoch [1/10], Step [103/125], D_loss: 0.0667, G_loss: 32.1478\n",
      "Epoch [1/10], Step [104/125], D_loss: 0.1678, G_loss: 38.4429\n",
      "Epoch [1/10], Step [105/125], D_loss: 0.7538, G_loss: 37.5588\n",
      "Epoch [1/10], Step [106/125], D_loss: 1.0939, G_loss: 38.6981\n",
      "Epoch [1/10], Step [107/125], D_loss: 0.5162, G_loss: 32.4694\n",
      "Epoch [1/10], Step [108/125], D_loss: 0.4782, G_loss: 20.3599\n",
      "Epoch [1/10], Step [109/125], D_loss: 0.5190, G_loss: 26.3836\n",
      "Epoch [1/10], Step [110/125], D_loss: 0.7078, G_loss: 48.9919\n",
      "Epoch [1/10], Step [111/125], D_loss: 2.4693, G_loss: 47.3251\n",
      "Epoch [1/10], Step [112/125], D_loss: 1.9476, G_loss: 29.5812\n",
      "Epoch [1/10], Step [113/125], D_loss: 0.8598, G_loss: 24.6989\n",
      "Epoch [1/10], Step [114/125], D_loss: 1.0125, G_loss: 30.5253\n",
      "Epoch [1/10], Step [115/125], D_loss: 0.8756, G_loss: 40.7243\n",
      "Epoch [1/10], Step [116/125], D_loss: 0.7436, G_loss: 46.2602\n",
      "Epoch [1/10], Step [117/125], D_loss: 3.7904, G_loss: 30.0893\n",
      "Epoch [1/10], Step [118/125], D_loss: 1.5663, G_loss: 24.6599\n",
      "Epoch [1/10], Step [119/125], D_loss: 0.7162, G_loss: 19.6627\n",
      "Epoch [1/10], Step [120/125], D_loss: 0.4284, G_loss: 25.3377\n",
      "Epoch [1/10], Step [121/125], D_loss: 0.3052, G_loss: 26.9315\n",
      "Epoch [1/10], Step [122/125], D_loss: 0.2096, G_loss: 20.2532\n",
      "Epoch [1/10], Step [123/125], D_loss: 0.1236, G_loss: 28.1790\n",
      "Epoch [1/10], Step [124/125], D_loss: 0.2409, G_loss: 31.6194\n",
      "Epoch [1/10], Step [125/125], D_loss: 0.3509, G_loss: 30.2766\n",
      "Epoch [2/10], Step [1/125], D_loss: 0.6198, G_loss: 18.2459\n",
      "Epoch [2/10], Step [2/125], D_loss: 0.3020, G_loss: 34.5199\n",
      "Epoch [2/10], Step [3/125], D_loss: 0.2605, G_loss: 42.5318\n",
      "Epoch [2/10], Step [4/125], D_loss: 0.4235, G_loss: 29.1883\n",
      "Epoch [2/10], Step [5/125], D_loss: 0.3811, G_loss: 29.8917\n",
      "Epoch [2/10], Step [6/125], D_loss: 0.6611, G_loss: 19.7749\n",
      "Epoch [2/10], Step [7/125], D_loss: 0.2073, G_loss: 23.6147\n",
      "Epoch [2/10], Step [8/125], D_loss: 0.4712, G_loss: 35.8954\n",
      "Epoch [2/10], Step [9/125], D_loss: 1.0509, G_loss: 45.7274\n",
      "Epoch [2/10], Step [10/125], D_loss: 1.0066, G_loss: 33.2645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Step [11/125], D_loss: 0.9730, G_loss: 41.7404\n",
      "Epoch [2/10], Step [12/125], D_loss: 0.9440, G_loss: 49.5962\n",
      "Epoch [2/10], Step [13/125], D_loss: 0.5832, G_loss: 32.5513\n",
      "Epoch [2/10], Step [14/125], D_loss: 1.6083, G_loss: 36.4134\n",
      "Epoch [2/10], Step [15/125], D_loss: 1.4151, G_loss: 20.0023\n",
      "Epoch [2/10], Step [16/125], D_loss: 1.2524, G_loss: 23.5814\n",
      "Epoch [2/10], Step [17/125], D_loss: 1.6765, G_loss: 25.1462\n",
      "Epoch [2/10], Step [18/125], D_loss: 0.7223, G_loss: 21.3001\n",
      "Epoch [2/10], Step [19/125], D_loss: 1.3260, G_loss: 14.3448\n",
      "Epoch [2/10], Step [20/125], D_loss: 0.5670, G_loss: 48.4337\n",
      "Epoch [2/10], Step [21/125], D_loss: 0.3567, G_loss: 29.0761\n",
      "Epoch [2/10], Step [22/125], D_loss: 0.2613, G_loss: 19.9000\n",
      "Epoch [2/10], Step [23/125], D_loss: 0.3194, G_loss: 22.2057\n",
      "Epoch [2/10], Step [24/125], D_loss: 0.1638, G_loss: 45.4899\n",
      "Epoch [2/10], Step [25/125], D_loss: 0.2162, G_loss: 34.7242\n",
      "Epoch [2/10], Step [26/125], D_loss: 0.2472, G_loss: 34.3454\n",
      "Epoch [2/10], Step [27/125], D_loss: 0.5333, G_loss: 24.5422\n",
      "Epoch [2/10], Step [28/125], D_loss: 0.3168, G_loss: 19.6727\n",
      "Epoch [2/10], Step [29/125], D_loss: 0.3587, G_loss: 41.6314\n",
      "Epoch [2/10], Step [30/125], D_loss: 0.3439, G_loss: 30.5129\n",
      "Epoch [2/10], Step [31/125], D_loss: 0.6191, G_loss: 31.9478\n",
      "Epoch [2/10], Step [32/125], D_loss: 0.4579, G_loss: 27.9834\n",
      "Epoch [2/10], Step [33/125], D_loss: 0.3292, G_loss: 24.8097\n",
      "Epoch [2/10], Step [34/125], D_loss: 0.5101, G_loss: 35.6115\n",
      "Epoch [2/10], Step [35/125], D_loss: 0.3640, G_loss: 38.9346\n",
      "Epoch [2/10], Step [36/125], D_loss: 0.1716, G_loss: 37.1024\n",
      "Epoch [2/10], Step [37/125], D_loss: 0.6513, G_loss: 32.0230\n",
      "Epoch [2/10], Step [38/125], D_loss: 1.0223, G_loss: 18.2147\n",
      "Epoch [2/10], Step [39/125], D_loss: 1.1688, G_loss: 21.1453\n",
      "Epoch [2/10], Step [40/125], D_loss: 0.5785, G_loss: 29.1985\n",
      "Epoch [2/10], Step [41/125], D_loss: 0.5220, G_loss: 36.1289\n",
      "Epoch [2/10], Step [42/125], D_loss: 0.3692, G_loss: 25.3927\n",
      "Epoch [2/10], Step [43/125], D_loss: 0.4187, G_loss: 32.8524\n",
      "Epoch [2/10], Step [44/125], D_loss: 0.5638, G_loss: 23.3509\n",
      "Epoch [2/10], Step [45/125], D_loss: 0.3491, G_loss: 61.7548\n",
      "Epoch [2/10], Step [46/125], D_loss: 0.6228, G_loss: 44.0781\n",
      "Epoch [2/10], Step [47/125], D_loss: 0.3171, G_loss: 25.9767\n",
      "Epoch [2/10], Step [48/125], D_loss: 0.8431, G_loss: 29.3671\n",
      "Epoch [2/10], Step [49/125], D_loss: 1.5183, G_loss: 32.3677\n",
      "Epoch [2/10], Step [50/125], D_loss: 0.7843, G_loss: 24.4438\n",
      "Epoch [2/10], Step [51/125], D_loss: 0.3234, G_loss: 42.5350\n",
      "Epoch [2/10], Step [52/125], D_loss: 0.4687, G_loss: 37.8458\n",
      "Epoch [2/10], Step [53/125], D_loss: 0.5065, G_loss: 29.2453\n",
      "Epoch [2/10], Step [54/125], D_loss: 0.5929, G_loss: 31.9536\n",
      "Epoch [2/10], Step [55/125], D_loss: 0.8387, G_loss: 29.1557\n",
      "Epoch [2/10], Step [56/125], D_loss: 0.4736, G_loss: 38.7352\n",
      "Epoch [2/10], Step [57/125], D_loss: 0.1793, G_loss: 37.9816\n",
      "Epoch [2/10], Step [58/125], D_loss: 0.5571, G_loss: 30.4738\n",
      "Epoch [2/10], Step [59/125], D_loss: 0.9787, G_loss: 39.3180\n",
      "Epoch [2/10], Step [60/125], D_loss: 1.7776, G_loss: 20.6510\n",
      "Epoch [2/10], Step [61/125], D_loss: 0.8460, G_loss: 28.0367\n",
      "Epoch [2/10], Step [62/125], D_loss: 0.9594, G_loss: 20.0291\n",
      "Epoch [2/10], Step [63/125], D_loss: 0.5005, G_loss: 32.1208\n",
      "Epoch [2/10], Step [64/125], D_loss: 0.5162, G_loss: 27.6072\n",
      "Epoch [2/10], Step [65/125], D_loss: 0.2804, G_loss: 24.3820\n",
      "Epoch [2/10], Step [66/125], D_loss: 1.0024, G_loss: 26.7880\n",
      "Epoch [2/10], Step [67/125], D_loss: 0.3926, G_loss: 47.3708\n",
      "Epoch [2/10], Step [68/125], D_loss: 0.3434, G_loss: 34.8305\n",
      "Epoch [2/10], Step [69/125], D_loss: 0.3587, G_loss: 31.4274\n",
      "Epoch [2/10], Step [70/125], D_loss: 0.4047, G_loss: 29.2340\n",
      "Epoch [2/10], Step [71/125], D_loss: 0.5114, G_loss: 19.1377\n",
      "Epoch [2/10], Step [72/125], D_loss: 0.4436, G_loss: 42.3102\n",
      "Epoch [2/10], Step [73/125], D_loss: 0.4570, G_loss: 30.0733\n",
      "Epoch [2/10], Step [74/125], D_loss: 0.6394, G_loss: 39.8834\n",
      "Epoch [2/10], Step [75/125], D_loss: 1.3526, G_loss: 22.8680\n",
      "Epoch [2/10], Step [76/125], D_loss: 0.3005, G_loss: 41.5946\n",
      "Epoch [2/10], Step [77/125], D_loss: 0.0905, G_loss: 35.0642\n",
      "Epoch [2/10], Step [78/125], D_loss: 0.3638, G_loss: 27.3644\n",
      "Epoch [2/10], Step [79/125], D_loss: 0.2637, G_loss: 31.3967\n",
      "Epoch [2/10], Step [80/125], D_loss: 0.2660, G_loss: 40.8188\n",
      "Epoch [2/10], Step [81/125], D_loss: 0.2230, G_loss: 20.8583\n",
      "Epoch [2/10], Step [82/125], D_loss: 0.9768, G_loss: 27.0612\n",
      "Epoch [2/10], Step [83/125], D_loss: 1.1295, G_loss: 28.0908\n",
      "Epoch [2/10], Step [84/125], D_loss: 1.3088, G_loss: 33.1799\n",
      "Epoch [2/10], Step [85/125], D_loss: 0.4123, G_loss: 28.9731\n",
      "Epoch [2/10], Step [86/125], D_loss: 0.1434, G_loss: 38.4038\n",
      "Epoch [2/10], Step [87/125], D_loss: 0.2824, G_loss: 34.3427\n",
      "Epoch [2/10], Step [88/125], D_loss: 0.2432, G_loss: 24.5442\n",
      "Epoch [2/10], Step [89/125], D_loss: 0.5147, G_loss: 46.1505\n",
      "Epoch [2/10], Step [90/125], D_loss: 0.9864, G_loss: 35.4522\n",
      "Epoch [2/10], Step [91/125], D_loss: 1.0801, G_loss: 36.0775\n",
      "Epoch [2/10], Step [92/125], D_loss: 0.4371, G_loss: 46.8449\n",
      "Epoch [2/10], Step [93/125], D_loss: 0.5084, G_loss: 30.8933\n",
      "Epoch [2/10], Step [94/125], D_loss: 0.3506, G_loss: 29.4289\n",
      "Epoch [2/10], Step [95/125], D_loss: 0.2792, G_loss: 24.8856\n",
      "Epoch [2/10], Step [96/125], D_loss: 0.1353, G_loss: 28.0791\n",
      "Epoch [2/10], Step [97/125], D_loss: 0.4048, G_loss: 29.1966\n",
      "Epoch [2/10], Step [98/125], D_loss: 0.2826, G_loss: 28.0314\n",
      "Epoch [2/10], Step [99/125], D_loss: 0.2679, G_loss: 46.1039\n",
      "Epoch [2/10], Step [100/125], D_loss: 0.8332, G_loss: 28.7916\n",
      "Epoch [2/10], Step [101/125], D_loss: 0.8100, G_loss: 25.7809\n",
      "Epoch [2/10], Step [102/125], D_loss: 0.5245, G_loss: 24.7544\n",
      "Epoch [2/10], Step [103/125], D_loss: 0.9656, G_loss: 26.7380\n",
      "Epoch [2/10], Step [104/125], D_loss: 0.1200, G_loss: 38.1900\n",
      "Epoch [2/10], Step [105/125], D_loss: 0.3759, G_loss: 27.0237\n",
      "Epoch [2/10], Step [106/125], D_loss: 0.7962, G_loss: 32.6238\n",
      "Epoch [2/10], Step [107/125], D_loss: 0.4745, G_loss: 26.3771\n",
      "Epoch [2/10], Step [108/125], D_loss: 1.1504, G_loss: 27.0322\n",
      "Epoch [2/10], Step [109/125], D_loss: 1.9616, G_loss: 35.1916\n",
      "Epoch [2/10], Step [110/125], D_loss: 1.5384, G_loss: 35.9341\n",
      "Epoch [2/10], Step [111/125], D_loss: 0.6374, G_loss: 29.7421\n",
      "Epoch [2/10], Step [112/125], D_loss: 1.3206, G_loss: 20.9145\n",
      "Epoch [2/10], Step [113/125], D_loss: 0.2773, G_loss: 32.2586\n",
      "Epoch [2/10], Step [114/125], D_loss: 0.5984, G_loss: 43.5729\n",
      "Epoch [2/10], Step [115/125], D_loss: 0.6418, G_loss: 38.2997\n",
      "Epoch [2/10], Step [116/125], D_loss: 0.5995, G_loss: 21.5014\n",
      "Epoch [2/10], Step [117/125], D_loss: 0.1840, G_loss: 45.8237\n",
      "Epoch [2/10], Step [118/125], D_loss: 0.5458, G_loss: 27.4389\n",
      "Epoch [2/10], Step [119/125], D_loss: 0.5519, G_loss: 29.0855\n",
      "Epoch [2/10], Step [120/125], D_loss: 0.8485, G_loss: 28.5589\n",
      "Epoch [2/10], Step [121/125], D_loss: 0.1271, G_loss: 27.2428\n",
      "Epoch [2/10], Step [122/125], D_loss: 0.2357, G_loss: 34.3377\n",
      "Epoch [2/10], Step [123/125], D_loss: 0.2030, G_loss: 51.8964\n",
      "Epoch [2/10], Step [124/125], D_loss: 0.3116, G_loss: 28.1771\n",
      "Epoch [2/10], Step [125/125], D_loss: 0.1112, G_loss: 29.1721\n",
      "Epoch [3/10], Step [1/125], D_loss: 0.3033, G_loss: 31.1432\n",
      "Epoch [3/10], Step [2/125], D_loss: 0.6210, G_loss: 28.2867\n",
      "Epoch [3/10], Step [3/125], D_loss: 0.3721, G_loss: 26.4790\n",
      "Epoch [3/10], Step [4/125], D_loss: 0.6027, G_loss: 36.5753\n",
      "Epoch [3/10], Step [5/125], D_loss: 0.3437, G_loss: 41.5875\n",
      "Epoch [3/10], Step [6/125], D_loss: 0.4463, G_loss: 36.7478\n",
      "Epoch [3/10], Step [7/125], D_loss: 0.2734, G_loss: 28.4790\n",
      "Epoch [3/10], Step [8/125], D_loss: 0.6815, G_loss: 28.5758\n",
      "Epoch [3/10], Step [9/125], D_loss: 0.4387, G_loss: 31.9485\n",
      "Epoch [3/10], Step [10/125], D_loss: 0.3276, G_loss: 46.0186\n",
      "Epoch [3/10], Step [11/125], D_loss: 0.2274, G_loss: 46.7909\n",
      "Epoch [3/10], Step [12/125], D_loss: 1.2960, G_loss: 30.0737\n",
      "Epoch [3/10], Step [13/125], D_loss: 0.4767, G_loss: 31.6556\n",
      "Epoch [3/10], Step [14/125], D_loss: 1.0378, G_loss: 22.0189\n",
      "Epoch [3/10], Step [15/125], D_loss: 0.5138, G_loss: 21.2999\n",
      "Epoch [3/10], Step [16/125], D_loss: 0.2760, G_loss: 25.9468\n",
      "Epoch [3/10], Step [17/125], D_loss: 0.3244, G_loss: 43.5430\n",
      "Epoch [3/10], Step [18/125], D_loss: 0.3074, G_loss: 42.0649\n",
      "Epoch [3/10], Step [19/125], D_loss: 0.3678, G_loss: 21.6938\n",
      "Epoch [3/10], Step [20/125], D_loss: 0.3259, G_loss: 31.6555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Step [21/125], D_loss: 0.3077, G_loss: 30.7647\n",
      "Epoch [3/10], Step [22/125], D_loss: 0.3545, G_loss: 40.0739\n",
      "Epoch [3/10], Step [23/125], D_loss: 0.0820, G_loss: 37.0724\n",
      "Epoch [3/10], Step [24/125], D_loss: 0.1499, G_loss: 37.8397\n",
      "Epoch [3/10], Step [25/125], D_loss: 0.7976, G_loss: 27.2374\n",
      "Epoch [3/10], Step [26/125], D_loss: 1.1544, G_loss: 45.0643\n",
      "Epoch [3/10], Step [27/125], D_loss: 0.5131, G_loss: 32.4753\n",
      "Epoch [3/10], Step [28/125], D_loss: 0.9170, G_loss: 26.1090\n",
      "Epoch [3/10], Step [29/125], D_loss: 1.1206, G_loss: 31.3289\n",
      "Epoch [3/10], Step [30/125], D_loss: 0.2543, G_loss: 39.9177\n",
      "Epoch [3/10], Step [31/125], D_loss: 1.6109, G_loss: 32.5239\n",
      "Epoch [3/10], Step [32/125], D_loss: 0.6603, G_loss: 24.4811\n",
      "Epoch [3/10], Step [33/125], D_loss: 0.6432, G_loss: 39.5004\n",
      "Epoch [3/10], Step [34/125], D_loss: 0.4362, G_loss: 41.8999\n",
      "Epoch [3/10], Step [35/125], D_loss: 0.7485, G_loss: 26.6946\n",
      "Epoch [3/10], Step [36/125], D_loss: 0.3032, G_loss: 34.8607\n",
      "Epoch [3/10], Step [37/125], D_loss: 0.1450, G_loss: 34.3323\n",
      "Epoch [3/10], Step [38/125], D_loss: 0.7572, G_loss: 26.5538\n",
      "Epoch [3/10], Step [39/125], D_loss: 0.3351, G_loss: 34.4198\n",
      "Epoch [3/10], Step [40/125], D_loss: 0.2368, G_loss: 40.7961\n",
      "Epoch [3/10], Step [41/125], D_loss: 1.5205, G_loss: 21.2295\n",
      "Epoch [3/10], Step [42/125], D_loss: 0.4278, G_loss: 26.7841\n",
      "Epoch [3/10], Step [43/125], D_loss: 0.5158, G_loss: 31.8446\n",
      "Epoch [3/10], Step [44/125], D_loss: 0.0639, G_loss: 46.8108\n",
      "Epoch [3/10], Step [45/125], D_loss: 0.1372, G_loss: 30.4849\n",
      "Epoch [3/10], Step [46/125], D_loss: 0.5521, G_loss: 24.0628\n",
      "Epoch [3/10], Step [47/125], D_loss: 1.0179, G_loss: 31.6935\n",
      "Epoch [3/10], Step [48/125], D_loss: 0.4432, G_loss: 38.2637\n",
      "Epoch [3/10], Step [49/125], D_loss: 1.0854, G_loss: 28.9406\n",
      "Epoch [3/10], Step [50/125], D_loss: 1.3226, G_loss: 33.8725\n",
      "Epoch [3/10], Step [51/125], D_loss: 0.7657, G_loss: 30.5402\n",
      "Epoch [3/10], Step [52/125], D_loss: 1.0272, G_loss: 22.2902\n",
      "Epoch [3/10], Step [53/125], D_loss: 0.1819, G_loss: 34.0978\n",
      "Epoch [3/10], Step [54/125], D_loss: 0.0711, G_loss: 35.8104\n",
      "Epoch [3/10], Step [55/125], D_loss: 0.1812, G_loss: 30.3749\n",
      "Epoch [3/10], Step [56/125], D_loss: 0.3856, G_loss: 25.6271\n",
      "Epoch [3/10], Step [57/125], D_loss: 0.4812, G_loss: 41.2776\n",
      "Epoch [3/10], Step [58/125], D_loss: 0.3925, G_loss: 40.6926\n",
      "Epoch [3/10], Step [59/125], D_loss: 0.2527, G_loss: 41.3809\n",
      "Epoch [3/10], Step [60/125], D_loss: 1.5538, G_loss: 20.8272\n",
      "Epoch [3/10], Step [61/125], D_loss: 0.4693, G_loss: 28.1054\n",
      "Epoch [3/10], Step [62/125], D_loss: 0.3744, G_loss: 23.7170\n",
      "Epoch [3/10], Step [63/125], D_loss: 0.5994, G_loss: 25.7093\n",
      "Epoch [3/10], Step [64/125], D_loss: 0.5180, G_loss: 25.5161\n",
      "Epoch [3/10], Step [65/125], D_loss: 0.2897, G_loss: 34.1767\n",
      "Epoch [3/10], Step [66/125], D_loss: 0.3288, G_loss: 40.0501\n",
      "Epoch [3/10], Step [67/125], D_loss: 0.4031, G_loss: 21.1694\n",
      "Epoch [3/10], Step [68/125], D_loss: 0.4884, G_loss: 32.4918\n",
      "Epoch [3/10], Step [69/125], D_loss: 0.6492, G_loss: 28.9374\n",
      "Epoch [3/10], Step [70/125], D_loss: 0.4232, G_loss: 35.9786\n",
      "Epoch [3/10], Step [71/125], D_loss: 0.7773, G_loss: 25.2186\n",
      "Epoch [3/10], Step [72/125], D_loss: 0.2504, G_loss: 31.6514\n",
      "Epoch [3/10], Step [73/125], D_loss: 0.1599, G_loss: 24.4933\n",
      "Epoch [3/10], Step [74/125], D_loss: 0.4882, G_loss: 37.1598\n",
      "Epoch [3/10], Step [75/125], D_loss: 0.3784, G_loss: 27.0724\n",
      "Epoch [3/10], Step [76/125], D_loss: 0.2907, G_loss: 25.3849\n",
      "Epoch [3/10], Step [77/125], D_loss: 0.5479, G_loss: 22.7496\n",
      "Epoch [3/10], Step [78/125], D_loss: 0.9669, G_loss: 31.8727\n",
      "Epoch [3/10], Step [79/125], D_loss: 0.9971, G_loss: 31.1337\n",
      "Epoch [3/10], Step [80/125], D_loss: 0.3020, G_loss: 35.5451\n",
      "Epoch [3/10], Step [81/125], D_loss: 0.8023, G_loss: 30.1634\n",
      "Epoch [3/10], Step [82/125], D_loss: 0.2975, G_loss: 28.1374\n",
      "Epoch [3/10], Step [83/125], D_loss: 0.3651, G_loss: 27.3255\n",
      "Epoch [3/10], Step [84/125], D_loss: 0.2746, G_loss: 33.2202\n",
      "Epoch [3/10], Step [85/125], D_loss: 0.5290, G_loss: 18.0702\n",
      "Epoch [3/10], Step [86/125], D_loss: 0.2047, G_loss: 24.9359\n",
      "Epoch [3/10], Step [87/125], D_loss: 0.2982, G_loss: 31.7265\n",
      "Epoch [3/10], Step [88/125], D_loss: 0.4259, G_loss: 32.9781\n",
      "Epoch [3/10], Step [89/125], D_loss: 0.2380, G_loss: 39.2876\n",
      "Epoch [3/10], Step [90/125], D_loss: 0.1885, G_loss: 28.9668\n",
      "Epoch [3/10], Step [91/125], D_loss: 0.0456, G_loss: 36.6907\n",
      "Epoch [3/10], Step [92/125], D_loss: 0.3379, G_loss: 26.6220\n",
      "Epoch [3/10], Step [93/125], D_loss: 0.3766, G_loss: 35.2516\n",
      "Epoch [3/10], Step [94/125], D_loss: 0.8816, G_loss: 21.3825\n",
      "Epoch [3/10], Step [95/125], D_loss: 0.5758, G_loss: 27.8462\n",
      "Epoch [3/10], Step [96/125], D_loss: 1.2513, G_loss: 38.8347\n",
      "Epoch [3/10], Step [97/125], D_loss: 0.7752, G_loss: 31.9549\n",
      "Epoch [3/10], Step [98/125], D_loss: 1.9072, G_loss: 26.3310\n",
      "Epoch [3/10], Step [99/125], D_loss: 0.2639, G_loss: 28.8935\n",
      "Epoch [3/10], Step [100/125], D_loss: 0.4530, G_loss: 24.1541\n",
      "Epoch [3/10], Step [101/125], D_loss: 0.1193, G_loss: 22.5126\n",
      "Epoch [3/10], Step [102/125], D_loss: 0.2866, G_loss: 57.5540\n",
      "Epoch [3/10], Step [103/125], D_loss: 0.4690, G_loss: 46.2094\n",
      "Epoch [3/10], Step [104/125], D_loss: 0.7691, G_loss: 25.2035\n",
      "Epoch [3/10], Step [105/125], D_loss: 0.4265, G_loss: 29.6702\n",
      "Epoch [3/10], Step [106/125], D_loss: 0.5135, G_loss: 26.6547\n",
      "Epoch [3/10], Step [107/125], D_loss: 0.2196, G_loss: 27.3001\n",
      "Epoch [3/10], Step [108/125], D_loss: 0.2450, G_loss: 28.7061\n",
      "Epoch [3/10], Step [109/125], D_loss: 0.3252, G_loss: 30.4301\n",
      "Epoch [3/10], Step [110/125], D_loss: 0.4479, G_loss: 35.8342\n",
      "Epoch [3/10], Step [111/125], D_loss: 0.4155, G_loss: 25.0224\n",
      "Epoch [3/10], Step [112/125], D_loss: 0.7882, G_loss: 25.1805\n",
      "Epoch [3/10], Step [113/125], D_loss: 0.6571, G_loss: 22.1736\n",
      "Epoch [3/10], Step [114/125], D_loss: 0.9628, G_loss: 29.3149\n",
      "Epoch [3/10], Step [115/125], D_loss: 0.3886, G_loss: 30.4753\n",
      "Epoch [3/10], Step [116/125], D_loss: 0.1626, G_loss: 34.4918\n",
      "Epoch [3/10], Step [117/125], D_loss: 0.6498, G_loss: 22.2265\n",
      "Epoch [3/10], Step [118/125], D_loss: 1.2890, G_loss: 32.1654\n",
      "Epoch [3/10], Step [119/125], D_loss: 0.6575, G_loss: 34.9282\n",
      "Epoch [3/10], Step [120/125], D_loss: 1.3083, G_loss: 35.1073\n",
      "Epoch [3/10], Step [121/125], D_loss: 1.5574, G_loss: 37.7523\n",
      "Epoch [3/10], Step [122/125], D_loss: 0.6380, G_loss: 28.4782\n",
      "Epoch [3/10], Step [123/125], D_loss: 0.4663, G_loss: 29.6104\n",
      "Epoch [3/10], Step [124/125], D_loss: 0.2868, G_loss: 30.5893\n",
      "Epoch [3/10], Step [125/125], D_loss: 0.4762, G_loss: 17.4524\n",
      "Epoch [4/10], Step [1/125], D_loss: 0.8240, G_loss: 38.8554\n",
      "Epoch [4/10], Step [2/125], D_loss: 0.5219, G_loss: 27.1622\n",
      "Epoch [4/10], Step [3/125], D_loss: 0.4172, G_loss: 33.9238\n",
      "Epoch [4/10], Step [4/125], D_loss: 0.2469, G_loss: 29.6353\n",
      "Epoch [4/10], Step [5/125], D_loss: 0.3578, G_loss: 29.2594\n",
      "Epoch [4/10], Step [6/125], D_loss: 0.7946, G_loss: 25.7493\n",
      "Epoch [4/10], Step [7/125], D_loss: 1.7036, G_loss: 32.9161\n",
      "Epoch [4/10], Step [8/125], D_loss: 1.8028, G_loss: 23.3858\n",
      "Epoch [4/10], Step [9/125], D_loss: 0.1357, G_loss: 37.1920\n",
      "Epoch [4/10], Step [10/125], D_loss: 0.1986, G_loss: 30.0687\n",
      "Epoch [4/10], Step [11/125], D_loss: 0.4514, G_loss: 23.8106\n",
      "Epoch [4/10], Step [12/125], D_loss: 0.3966, G_loss: 30.6937\n",
      "Epoch [4/10], Step [13/125], D_loss: 0.2697, G_loss: 22.4724\n",
      "Epoch [4/10], Step [14/125], D_loss: 0.1281, G_loss: 36.9958\n",
      "Epoch [4/10], Step [15/125], D_loss: 0.3794, G_loss: 34.0797\n",
      "Epoch [4/10], Step [16/125], D_loss: 1.4280, G_loss: 20.6695\n",
      "Epoch [4/10], Step [17/125], D_loss: 0.4200, G_loss: 22.7310\n",
      "Epoch [4/10], Step [18/125], D_loss: 0.2255, G_loss: 32.2044\n",
      "Epoch [4/10], Step [19/125], D_loss: 0.2449, G_loss: 32.4319\n",
      "Epoch [4/10], Step [20/125], D_loss: 0.7728, G_loss: 20.0102\n",
      "Epoch [4/10], Step [21/125], D_loss: 0.9381, G_loss: 26.3684\n",
      "Epoch [4/10], Step [22/125], D_loss: 0.5742, G_loss: 26.2600\n",
      "Epoch [4/10], Step [23/125], D_loss: 0.3171, G_loss: 32.6548\n",
      "Epoch [4/10], Step [24/125], D_loss: 1.5639, G_loss: 22.1562\n",
      "Epoch [4/10], Step [25/125], D_loss: 0.8041, G_loss: 24.1139\n",
      "Epoch [4/10], Step [26/125], D_loss: 0.2734, G_loss: 22.2425\n",
      "Epoch [4/10], Step [27/125], D_loss: 0.4864, G_loss: 32.8702\n",
      "Epoch [4/10], Step [28/125], D_loss: 0.4285, G_loss: 47.8257\n",
      "Epoch [4/10], Step [29/125], D_loss: 0.2665, G_loss: 45.3686\n",
      "Epoch [4/10], Step [30/125], D_loss: 0.1273, G_loss: 49.6173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Step [31/125], D_loss: 0.6107, G_loss: 36.6452\n",
      "Epoch [4/10], Step [32/125], D_loss: 0.7256, G_loss: 31.3806\n",
      "Epoch [4/10], Step [33/125], D_loss: 0.9987, G_loss: 35.4280\n",
      "Epoch [4/10], Step [34/125], D_loss: 0.5276, G_loss: 36.5873\n",
      "Epoch [4/10], Step [35/125], D_loss: 0.2949, G_loss: 27.8499\n",
      "Epoch [4/10], Step [36/125], D_loss: 0.3605, G_loss: 30.9728\n",
      "Epoch [4/10], Step [37/125], D_loss: 0.4339, G_loss: 33.9018\n",
      "Epoch [4/10], Step [38/125], D_loss: 1.1758, G_loss: 28.6264\n",
      "Epoch [4/10], Step [39/125], D_loss: 0.2992, G_loss: 53.7267\n",
      "Epoch [4/10], Step [40/125], D_loss: 0.2223, G_loss: 36.7450\n",
      "Epoch [4/10], Step [41/125], D_loss: 0.3536, G_loss: 25.6838\n",
      "Epoch [4/10], Step [42/125], D_loss: 0.3954, G_loss: 35.9829\n",
      "Epoch [4/10], Step [43/125], D_loss: 0.6840, G_loss: 29.9420\n",
      "Epoch [4/10], Step [44/125], D_loss: 0.4196, G_loss: 41.8854\n",
      "Epoch [4/10], Step [45/125], D_loss: 0.4072, G_loss: 35.6728\n",
      "Epoch [4/10], Step [46/125], D_loss: 0.8024, G_loss: 33.0632\n",
      "Epoch [4/10], Step [47/125], D_loss: 0.3706, G_loss: 21.8131\n",
      "Epoch [4/10], Step [48/125], D_loss: 0.2736, G_loss: 22.6663\n",
      "Epoch [4/10], Step [49/125], D_loss: 0.3285, G_loss: 25.5704\n",
      "Epoch [4/10], Step [50/125], D_loss: 0.4959, G_loss: 35.4337\n",
      "Epoch [4/10], Step [51/125], D_loss: 0.5866, G_loss: 34.0490\n",
      "Epoch [4/10], Step [52/125], D_loss: 0.3088, G_loss: 31.0023\n",
      "Epoch [4/10], Step [53/125], D_loss: 1.3569, G_loss: 30.0933\n",
      "Epoch [4/10], Step [54/125], D_loss: 1.0019, G_loss: 29.8362\n",
      "Epoch [4/10], Step [55/125], D_loss: 0.8231, G_loss: 19.3391\n",
      "Epoch [4/10], Step [56/125], D_loss: 0.4610, G_loss: 27.9317\n",
      "Epoch [4/10], Step [57/125], D_loss: 0.2755, G_loss: 38.7092\n",
      "Epoch [4/10], Step [58/125], D_loss: 0.1648, G_loss: 24.9704\n",
      "Epoch [4/10], Step [59/125], D_loss: 0.7619, G_loss: 18.9504\n",
      "Epoch [4/10], Step [60/125], D_loss: 0.3213, G_loss: 19.2126\n",
      "Epoch [4/10], Step [61/125], D_loss: 0.4181, G_loss: 25.1109\n",
      "Epoch [4/10], Step [62/125], D_loss: 0.2545, G_loss: 26.7366\n",
      "Epoch [4/10], Step [63/125], D_loss: 0.5253, G_loss: 16.8152\n",
      "Epoch [4/10], Step [64/125], D_loss: 1.2447, G_loss: 20.2976\n",
      "Epoch [4/10], Step [65/125], D_loss: 0.3817, G_loss: 29.4027\n",
      "Epoch [4/10], Step [66/125], D_loss: 0.2830, G_loss: 33.5930\n",
      "Epoch [4/10], Step [67/125], D_loss: 1.2654, G_loss: 24.1349\n",
      "Epoch [4/10], Step [68/125], D_loss: 0.4968, G_loss: 31.5484\n",
      "Epoch [4/10], Step [69/125], D_loss: 0.2632, G_loss: 30.1165\n",
      "Epoch [4/10], Step [70/125], D_loss: 0.7446, G_loss: 23.6646\n",
      "Epoch [4/10], Step [71/125], D_loss: 0.5518, G_loss: 41.2639\n",
      "Epoch [4/10], Step [72/125], D_loss: 0.4890, G_loss: 23.6740\n",
      "Epoch [4/10], Step [73/125], D_loss: 0.2350, G_loss: 31.9483\n",
      "Epoch [4/10], Step [74/125], D_loss: 0.5405, G_loss: 20.8569\n",
      "Epoch [4/10], Step [75/125], D_loss: 0.5301, G_loss: 20.8334\n",
      "Epoch [4/10], Step [76/125], D_loss: 0.1762, G_loss: 27.7441\n",
      "Epoch [4/10], Step [77/125], D_loss: 0.2046, G_loss: 21.2700\n",
      "Epoch [4/10], Step [78/125], D_loss: 0.2523, G_loss: 36.4939\n",
      "Epoch [4/10], Step [79/125], D_loss: 0.3063, G_loss: 30.2493\n",
      "Epoch [4/10], Step [80/125], D_loss: 0.6365, G_loss: 28.7443\n",
      "Epoch [4/10], Step [81/125], D_loss: 0.4110, G_loss: 24.8587\n",
      "Epoch [4/10], Step [82/125], D_loss: 0.1659, G_loss: 26.2890\n",
      "Epoch [4/10], Step [83/125], D_loss: 0.1687, G_loss: 31.5576\n",
      "Epoch [4/10], Step [84/125], D_loss: 1.2674, G_loss: 16.3684\n",
      "Epoch [4/10], Step [85/125], D_loss: 0.4125, G_loss: 23.1436\n",
      "Epoch [4/10], Step [86/125], D_loss: 0.4170, G_loss: 35.7573\n",
      "Epoch [4/10], Step [87/125], D_loss: 0.2710, G_loss: 27.1883\n",
      "Epoch [4/10], Step [88/125], D_loss: 0.1512, G_loss: 25.1342\n",
      "Epoch [4/10], Step [89/125], D_loss: 0.1986, G_loss: 21.0687\n",
      "Epoch [4/10], Step [90/125], D_loss: 0.9549, G_loss: 24.1586\n",
      "Epoch [4/10], Step [91/125], D_loss: 0.3000, G_loss: 19.4560\n",
      "Epoch [4/10], Step [92/125], D_loss: 0.4182, G_loss: 19.0686\n",
      "Epoch [4/10], Step [93/125], D_loss: 0.1280, G_loss: 26.7561\n",
      "Epoch [4/10], Step [94/125], D_loss: 0.5223, G_loss: 27.0725\n",
      "Epoch [4/10], Step [95/125], D_loss: 0.8945, G_loss: 30.3824\n",
      "Epoch [4/10], Step [96/125], D_loss: 2.4191, G_loss: 34.1595\n",
      "Epoch [4/10], Step [97/125], D_loss: 0.3493, G_loss: 37.3984\n",
      "Epoch [4/10], Step [98/125], D_loss: 0.4748, G_loss: 54.7418\n",
      "Epoch [4/10], Step [99/125], D_loss: 0.2926, G_loss: 37.3581\n",
      "Epoch [4/10], Step [100/125], D_loss: 0.7220, G_loss: 30.6126\n",
      "Epoch [4/10], Step [101/125], D_loss: 0.1329, G_loss: 31.2016\n",
      "Epoch [4/10], Step [102/125], D_loss: 0.6462, G_loss: 34.0447\n",
      "Epoch [4/10], Step [103/125], D_loss: 1.1325, G_loss: 26.0112\n",
      "Epoch [4/10], Step [104/125], D_loss: 0.4346, G_loss: 34.3228\n",
      "Epoch [4/10], Step [105/125], D_loss: 1.4045, G_loss: 31.1923\n",
      "Epoch [4/10], Step [106/125], D_loss: 0.4629, G_loss: 28.8186\n",
      "Epoch [4/10], Step [107/125], D_loss: 0.6498, G_loss: 22.1148\n",
      "Epoch [4/10], Step [108/125], D_loss: 0.2345, G_loss: 41.8484\n",
      "Epoch [4/10], Step [109/125], D_loss: 0.3902, G_loss: 21.0153\n",
      "Epoch [4/10], Step [110/125], D_loss: 0.1514, G_loss: 37.4293\n",
      "Epoch [4/10], Step [111/125], D_loss: 0.4676, G_loss: 17.8566\n",
      "Epoch [4/10], Step [112/125], D_loss: 0.2487, G_loss: 27.0044\n",
      "Epoch [4/10], Step [113/125], D_loss: 0.1311, G_loss: 34.1247\n",
      "Epoch [4/10], Step [114/125], D_loss: 0.7723, G_loss: 28.4349\n",
      "Epoch [4/10], Step [115/125], D_loss: 1.3035, G_loss: 28.9249\n",
      "Epoch [4/10], Step [116/125], D_loss: 0.5588, G_loss: 32.9004\n",
      "Epoch [4/10], Step [117/125], D_loss: 0.5070, G_loss: 33.8035\n",
      "Epoch [4/10], Step [118/125], D_loss: 1.6202, G_loss: 18.1339\n",
      "Epoch [4/10], Step [119/125], D_loss: 0.7487, G_loss: 20.4951\n",
      "Epoch [4/10], Step [120/125], D_loss: 0.2292, G_loss: 30.0543\n",
      "Epoch [4/10], Step [121/125], D_loss: 0.5366, G_loss: 25.7972\n",
      "Epoch [4/10], Step [122/125], D_loss: 0.1284, G_loss: 27.6613\n",
      "Epoch [4/10], Step [123/125], D_loss: 0.3418, G_loss: 34.9658\n",
      "Epoch [4/10], Step [124/125], D_loss: 0.2641, G_loss: 64.5287\n",
      "Epoch [4/10], Step [125/125], D_loss: 0.1867, G_loss: 34.6245\n",
      "Epoch [5/10], Step [1/125], D_loss: 1.8224, G_loss: 27.0267\n",
      "Epoch [5/10], Step [2/125], D_loss: 0.5139, G_loss: 30.2757\n",
      "Epoch [5/10], Step [3/125], D_loss: 0.3377, G_loss: 41.4833\n",
      "Epoch [5/10], Step [4/125], D_loss: 0.8505, G_loss: 28.3749\n",
      "Epoch [5/10], Step [5/125], D_loss: 0.2977, G_loss: 33.3705\n",
      "Epoch [5/10], Step [6/125], D_loss: 1.2176, G_loss: 23.5935\n",
      "Epoch [5/10], Step [7/125], D_loss: 0.7311, G_loss: 30.1387\n",
      "Epoch [5/10], Step [8/125], D_loss: 0.2258, G_loss: 27.0497\n",
      "Epoch [5/10], Step [9/125], D_loss: 0.2024, G_loss: 30.3135\n",
      "Epoch [5/10], Step [10/125], D_loss: 0.2790, G_loss: 23.1429\n",
      "Epoch [5/10], Step [11/125], D_loss: 0.4229, G_loss: 28.3849\n",
      "Epoch [5/10], Step [12/125], D_loss: 0.4358, G_loss: 38.0925\n",
      "Epoch [5/10], Step [13/125], D_loss: 0.5235, G_loss: 31.9580\n",
      "Epoch [5/10], Step [14/125], D_loss: 0.2173, G_loss: 55.4662\n",
      "Epoch [5/10], Step [15/125], D_loss: 0.2364, G_loss: 27.7179\n",
      "Epoch [5/10], Step [16/125], D_loss: 0.7606, G_loss: 22.2926\n",
      "Epoch [5/10], Step [17/125], D_loss: 0.6341, G_loss: 18.2560\n",
      "Epoch [5/10], Step [18/125], D_loss: 0.5551, G_loss: 37.4553\n",
      "Epoch [5/10], Step [19/125], D_loss: 1.0902, G_loss: 18.5307\n",
      "Epoch [5/10], Step [20/125], D_loss: 0.5320, G_loss: 37.6775\n",
      "Epoch [5/10], Step [21/125], D_loss: 0.8386, G_loss: 22.4463\n",
      "Epoch [5/10], Step [22/125], D_loss: 0.6513, G_loss: 16.0153\n",
      "Epoch [5/10], Step [23/125], D_loss: 0.3591, G_loss: 27.1834\n",
      "Epoch [5/10], Step [24/125], D_loss: 0.4219, G_loss: 30.0492\n",
      "Epoch [5/10], Step [25/125], D_loss: 0.9274, G_loss: 20.6797\n",
      "Epoch [5/10], Step [26/125], D_loss: 0.5116, G_loss: 21.8626\n",
      "Epoch [5/10], Step [27/125], D_loss: 0.1916, G_loss: 44.0788\n",
      "Epoch [5/10], Step [28/125], D_loss: 0.6045, G_loss: 18.6563\n",
      "Epoch [5/10], Step [29/125], D_loss: 0.4970, G_loss: 24.2877\n",
      "Epoch [5/10], Step [30/125], D_loss: 0.3957, G_loss: 28.2335\n",
      "Epoch [5/10], Step [31/125], D_loss: 0.1305, G_loss: 26.7691\n",
      "Epoch [5/10], Step [32/125], D_loss: 0.1953, G_loss: 23.9123\n",
      "Epoch [5/10], Step [33/125], D_loss: 1.5754, G_loss: 14.4265\n",
      "Epoch [5/10], Step [34/125], D_loss: 0.5791, G_loss: 18.3060\n",
      "Epoch [5/10], Step [35/125], D_loss: 0.5965, G_loss: 23.5289\n",
      "Epoch [5/10], Step [36/125], D_loss: 0.3985, G_loss: 36.1547\n",
      "Epoch [5/10], Step [37/125], D_loss: 0.1596, G_loss: 28.5241\n",
      "Epoch [5/10], Step [38/125], D_loss: 1.7004, G_loss: 19.9105\n",
      "Epoch [5/10], Step [39/125], D_loss: 0.0767, G_loss: 42.8732\n",
      "Epoch [5/10], Step [40/125], D_loss: 0.4543, G_loss: 28.5827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Step [41/125], D_loss: 0.4216, G_loss: 29.9425\n",
      "Epoch [5/10], Step [42/125], D_loss: 0.3184, G_loss: 37.0852\n",
      "Epoch [5/10], Step [43/125], D_loss: 0.5411, G_loss: 37.2782\n",
      "Epoch [5/10], Step [44/125], D_loss: 0.2218, G_loss: 22.0234\n",
      "Epoch [5/10], Step [45/125], D_loss: 1.2125, G_loss: 24.2711\n",
      "Epoch [5/10], Step [46/125], D_loss: 0.6710, G_loss: 19.7378\n",
      "Epoch [5/10], Step [47/125], D_loss: 0.1366, G_loss: 31.9146\n",
      "Epoch [5/10], Step [48/125], D_loss: 0.3660, G_loss: 30.0545\n",
      "Epoch [5/10], Step [49/125], D_loss: 0.0708, G_loss: 35.0090\n",
      "Epoch [5/10], Step [50/125], D_loss: 0.3336, G_loss: 32.2116\n",
      "Epoch [5/10], Step [51/125], D_loss: 0.2150, G_loss: 45.4243\n",
      "Epoch [5/10], Step [52/125], D_loss: 0.3996, G_loss: 31.6208\n",
      "Epoch [5/10], Step [53/125], D_loss: 0.4748, G_loss: 27.8607\n",
      "Epoch [5/10], Step [54/125], D_loss: 0.7630, G_loss: 35.1560\n",
      "Epoch [5/10], Step [55/125], D_loss: 2.7549, G_loss: 25.4806\n",
      "Epoch [5/10], Step [56/125], D_loss: 0.9264, G_loss: 36.8633\n",
      "Epoch [5/10], Step [57/125], D_loss: 0.7725, G_loss: 37.3209\n",
      "Epoch [5/10], Step [58/125], D_loss: 0.8175, G_loss: 30.6038\n",
      "Epoch [5/10], Step [59/125], D_loss: 0.4266, G_loss: 38.6389\n",
      "Epoch [5/10], Step [60/125], D_loss: 0.7181, G_loss: 27.6594\n",
      "Epoch [5/10], Step [61/125], D_loss: 0.6547, G_loss: 27.2293\n",
      "Epoch [5/10], Step [62/125], D_loss: 0.5234, G_loss: 25.7538\n",
      "Epoch [5/10], Step [63/125], D_loss: 0.6150, G_loss: 29.4052\n",
      "Epoch [5/10], Step [64/125], D_loss: 0.4814, G_loss: 18.8280\n",
      "Epoch [5/10], Step [65/125], D_loss: 0.2097, G_loss: 30.3886\n",
      "Epoch [5/10], Step [66/125], D_loss: 0.5626, G_loss: 23.8167\n",
      "Epoch [5/10], Step [67/125], D_loss: 0.2108, G_loss: 29.7310\n",
      "Epoch [5/10], Step [68/125], D_loss: 0.3262, G_loss: 31.0064\n",
      "Epoch [5/10], Step [69/125], D_loss: 0.3607, G_loss: 23.6162\n",
      "Epoch [5/10], Step [70/125], D_loss: 0.6571, G_loss: 15.6357\n",
      "Epoch [5/10], Step [71/125], D_loss: 0.4228, G_loss: 21.5593\n",
      "Epoch [5/10], Step [72/125], D_loss: 0.2541, G_loss: 29.1954\n",
      "Epoch [5/10], Step [73/125], D_loss: 0.3779, G_loss: 30.6328\n",
      "Epoch [5/10], Step [74/125], D_loss: 0.1416, G_loss: 28.3610\n",
      "Epoch [5/10], Step [75/125], D_loss: 0.1443, G_loss: 37.4830\n",
      "Epoch [5/10], Step [76/125], D_loss: 0.1927, G_loss: 25.5226\n",
      "Epoch [5/10], Step [77/125], D_loss: 0.8216, G_loss: 25.1238\n",
      "Epoch [5/10], Step [78/125], D_loss: 0.4292, G_loss: 26.7666\n",
      "Epoch [5/10], Step [79/125], D_loss: 0.3855, G_loss: 42.1534\n",
      "Epoch [5/10], Step [80/125], D_loss: 0.6345, G_loss: 25.1818\n",
      "Epoch [5/10], Step [81/125], D_loss: 0.6429, G_loss: 16.6134\n",
      "Epoch [5/10], Step [82/125], D_loss: 0.7592, G_loss: 17.8625\n",
      "Epoch [5/10], Step [83/125], D_loss: 0.4606, G_loss: 39.4516\n",
      "Epoch [5/10], Step [84/125], D_loss: 0.1316, G_loss: 42.7741\n",
      "Epoch [5/10], Step [85/125], D_loss: 0.5720, G_loss: 29.7713\n",
      "Epoch [5/10], Step [86/125], D_loss: 1.2842, G_loss: 23.4399\n",
      "Epoch [5/10], Step [87/125], D_loss: 0.9212, G_loss: 22.2339\n",
      "Epoch [5/10], Step [88/125], D_loss: 1.5858, G_loss: 26.2467\n",
      "Epoch [5/10], Step [89/125], D_loss: 1.3344, G_loss: 21.7132\n",
      "Epoch [5/10], Step [90/125], D_loss: 0.5471, G_loss: 23.2848\n",
      "Epoch [5/10], Step [91/125], D_loss: 0.4852, G_loss: 25.8447\n",
      "Epoch [5/10], Step [92/125], D_loss: 0.3099, G_loss: 30.4103\n",
      "Epoch [5/10], Step [93/125], D_loss: 0.1006, G_loss: 27.8309\n",
      "Epoch [5/10], Step [94/125], D_loss: 0.2031, G_loss: 26.3546\n",
      "Epoch [5/10], Step [95/125], D_loss: 0.2958, G_loss: 20.6332\n",
      "Epoch [5/10], Step [96/125], D_loss: 0.1208, G_loss: 46.7803\n",
      "Epoch [5/10], Step [97/125], D_loss: 0.4330, G_loss: 30.2934\n",
      "Epoch [5/10], Step [98/125], D_loss: 0.8069, G_loss: 17.2573\n",
      "Epoch [5/10], Step [99/125], D_loss: 0.5531, G_loss: 26.2695\n",
      "Epoch [5/10], Step [100/125], D_loss: 0.3693, G_loss: 28.4957\n",
      "Epoch [5/10], Step [101/125], D_loss: 0.3717, G_loss: 33.0079\n",
      "Epoch [5/10], Step [102/125], D_loss: 0.2790, G_loss: 33.2823\n",
      "Epoch [5/10], Step [103/125], D_loss: 0.6416, G_loss: 24.4652\n",
      "Epoch [5/10], Step [104/125], D_loss: 0.7431, G_loss: 19.6059\n",
      "Epoch [5/10], Step [105/125], D_loss: 0.3081, G_loss: 33.2685\n",
      "Epoch [5/10], Step [106/125], D_loss: 0.1512, G_loss: 41.0074\n",
      "Epoch [5/10], Step [107/125], D_loss: 0.0862, G_loss: 32.0171\n",
      "Epoch [5/10], Step [108/125], D_loss: 0.3736, G_loss: 23.8210\n",
      "Epoch [5/10], Step [109/125], D_loss: 0.3923, G_loss: 27.6844\n",
      "Epoch [5/10], Step [110/125], D_loss: 0.7491, G_loss: 20.1501\n",
      "Epoch [5/10], Step [111/125], D_loss: 0.2771, G_loss: 31.3661\n",
      "Epoch [5/10], Step [112/125], D_loss: 0.3998, G_loss: 29.9615\n",
      "Epoch [5/10], Step [113/125], D_loss: 1.6449, G_loss: 27.3575\n",
      "Epoch [5/10], Step [114/125], D_loss: 0.3379, G_loss: 40.7103\n",
      "Epoch [5/10], Step [115/125], D_loss: 0.4171, G_loss: 34.0330\n",
      "Epoch [5/10], Step [116/125], D_loss: 0.4264, G_loss: 30.1888\n",
      "Epoch [5/10], Step [117/125], D_loss: 0.4232, G_loss: 22.7094\n",
      "Epoch [5/10], Step [118/125], D_loss: 0.4227, G_loss: 44.4917\n",
      "Epoch [5/10], Step [119/125], D_loss: 0.3812, G_loss: 37.9119\n",
      "Epoch [5/10], Step [120/125], D_loss: 0.3753, G_loss: 24.0318\n",
      "Epoch [5/10], Step [121/125], D_loss: 0.3795, G_loss: 30.9294\n",
      "Epoch [5/10], Step [122/125], D_loss: 0.2492, G_loss: 30.7988\n",
      "Epoch [5/10], Step [123/125], D_loss: 0.6216, G_loss: 34.2785\n",
      "Epoch [5/10], Step [124/125], D_loss: 1.3005, G_loss: 30.8824\n",
      "Epoch [5/10], Step [125/125], D_loss: 1.0984, G_loss: 24.2817\n",
      "Epoch [6/10], Step [1/125], D_loss: 0.1558, G_loss: 31.0912\n",
      "Epoch [6/10], Step [2/125], D_loss: 0.1273, G_loss: 30.4677\n",
      "Epoch [6/10], Step [3/125], D_loss: 1.1104, G_loss: 22.6241\n",
      "Epoch [6/10], Step [4/125], D_loss: 0.3257, G_loss: 37.1146\n",
      "Epoch [6/10], Step [5/125], D_loss: 0.6333, G_loss: 24.1186\n",
      "Epoch [6/10], Step [6/125], D_loss: 0.3866, G_loss: 23.7264\n",
      "Epoch [6/10], Step [7/125], D_loss: 0.3787, G_loss: 27.8589\n",
      "Epoch [6/10], Step [8/125], D_loss: 0.1726, G_loss: 35.8872\n",
      "Epoch [6/10], Step [9/125], D_loss: 0.6287, G_loss: 15.7379\n",
      "Epoch [6/10], Step [10/125], D_loss: 0.1523, G_loss: 38.1069\n",
      "Epoch [6/10], Step [11/125], D_loss: 0.2143, G_loss: 21.8234\n",
      "Epoch [6/10], Step [12/125], D_loss: 0.3365, G_loss: 29.5782\n",
      "Epoch [6/10], Step [13/125], D_loss: 0.7333, G_loss: 25.6822\n",
      "Epoch [6/10], Step [14/125], D_loss: 0.3261, G_loss: 23.9044\n",
      "Epoch [6/10], Step [15/125], D_loss: 0.6211, G_loss: 35.3711\n",
      "Epoch [6/10], Step [16/125], D_loss: 0.3819, G_loss: 22.6556\n",
      "Epoch [6/10], Step [17/125], D_loss: 0.7958, G_loss: 25.2491\n",
      "Epoch [6/10], Step [18/125], D_loss: 1.0366, G_loss: 24.0763\n",
      "Epoch [6/10], Step [19/125], D_loss: 0.5368, G_loss: 19.4136\n",
      "Epoch [6/10], Step [20/125], D_loss: 0.0807, G_loss: 29.9812\n",
      "Epoch [6/10], Step [21/125], D_loss: 0.3498, G_loss: 28.2350\n",
      "Epoch [6/10], Step [22/125], D_loss: 0.1725, G_loss: 38.7855\n",
      "Epoch [6/10], Step [23/125], D_loss: 0.9340, G_loss: 22.3660\n",
      "Epoch [6/10], Step [24/125], D_loss: 0.2793, G_loss: 25.9664\n",
      "Epoch [6/10], Step [25/125], D_loss: 0.4173, G_loss: 22.2583\n",
      "Epoch [6/10], Step [26/125], D_loss: 0.3120, G_loss: 24.9599\n",
      "Epoch [6/10], Step [27/125], D_loss: 1.1124, G_loss: 29.1504\n",
      "Epoch [6/10], Step [28/125], D_loss: 0.2779, G_loss: 34.1253\n",
      "Epoch [6/10], Step [29/125], D_loss: 0.2537, G_loss: 25.8344\n",
      "Epoch [6/10], Step [30/125], D_loss: 0.2771, G_loss: 18.4471\n",
      "Epoch [6/10], Step [31/125], D_loss: 0.1392, G_loss: 20.2814\n",
      "Epoch [6/10], Step [32/125], D_loss: 0.4540, G_loss: 16.3780\n",
      "Epoch [6/10], Step [33/125], D_loss: 0.4661, G_loss: 25.3156\n",
      "Epoch [6/10], Step [34/125], D_loss: 0.5257, G_loss: 29.5159\n",
      "Epoch [6/10], Step [35/125], D_loss: 0.9400, G_loss: 28.4740\n",
      "Epoch [6/10], Step [36/125], D_loss: 0.4221, G_loss: 28.0851\n",
      "Epoch [6/10], Step [37/125], D_loss: 0.0861, G_loss: 43.1211\n",
      "Epoch [6/10], Step [38/125], D_loss: 0.1739, G_loss: 33.7793\n",
      "Epoch [6/10], Step [39/125], D_loss: 0.7543, G_loss: 25.3531\n",
      "Epoch [6/10], Step [40/125], D_loss: 0.1417, G_loss: 35.9281\n",
      "Epoch [6/10], Step [41/125], D_loss: 0.5684, G_loss: 35.4427\n",
      "Epoch [6/10], Step [42/125], D_loss: 1.7416, G_loss: 18.7565\n",
      "Epoch [6/10], Step [43/125], D_loss: 0.2498, G_loss: 38.3777\n",
      "Epoch [6/10], Step [44/125], D_loss: 0.8484, G_loss: 20.1742\n",
      "Epoch [6/10], Step [45/125], D_loss: 0.9407, G_loss: 20.7792\n",
      "Epoch [6/10], Step [46/125], D_loss: 0.6931, G_loss: 30.4574\n",
      "Epoch [6/10], Step [47/125], D_loss: 0.5517, G_loss: 25.0897\n",
      "Epoch [6/10], Step [48/125], D_loss: 0.2661, G_loss: 27.5347\n",
      "Epoch [6/10], Step [49/125], D_loss: 0.2762, G_loss: 35.1148\n",
      "Epoch [6/10], Step [50/125], D_loss: 0.2452, G_loss: 22.0416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [51/125], D_loss: 0.5120, G_loss: 21.5194\n",
      "Epoch [6/10], Step [52/125], D_loss: 0.2179, G_loss: 22.3535\n",
      "Epoch [6/10], Step [53/125], D_loss: 0.5720, G_loss: 30.7626\n",
      "Epoch [6/10], Step [54/125], D_loss: 0.4758, G_loss: 25.3999\n",
      "Epoch [6/10], Step [55/125], D_loss: 0.2623, G_loss: 32.5829\n",
      "Epoch [6/10], Step [56/125], D_loss: 1.9295, G_loss: 26.9746\n",
      "Epoch [6/10], Step [57/125], D_loss: 0.2530, G_loss: 33.9490\n",
      "Epoch [6/10], Step [58/125], D_loss: 0.8434, G_loss: 20.3605\n",
      "Epoch [6/10], Step [59/125], D_loss: 0.6464, G_loss: 23.2717\n",
      "Epoch [6/10], Step [60/125], D_loss: 0.3495, G_loss: 25.6460\n",
      "Epoch [6/10], Step [61/125], D_loss: 0.2604, G_loss: 27.1028\n",
      "Epoch [6/10], Step [62/125], D_loss: 0.2350, G_loss: 43.8890\n",
      "Epoch [6/10], Step [63/125], D_loss: 0.5963, G_loss: 22.1660\n",
      "Epoch [6/10], Step [64/125], D_loss: 0.1394, G_loss: 45.5188\n",
      "Epoch [6/10], Step [65/125], D_loss: 0.9731, G_loss: 25.8334\n",
      "Epoch [6/10], Step [66/125], D_loss: 1.2677, G_loss: 16.4998\n",
      "Epoch [6/10], Step [67/125], D_loss: 1.3963, G_loss: 16.2846\n",
      "Epoch [6/10], Step [68/125], D_loss: 0.7095, G_loss: 32.1371\n",
      "Epoch [6/10], Step [69/125], D_loss: 0.6257, G_loss: 44.3841\n",
      "Epoch [6/10], Step [70/125], D_loss: 0.2706, G_loss: 27.6478\n",
      "Epoch [6/10], Step [71/125], D_loss: 0.3200, G_loss: 25.2344\n",
      "Epoch [6/10], Step [72/125], D_loss: 0.3691, G_loss: 28.7236\n",
      "Epoch [6/10], Step [73/125], D_loss: 0.3304, G_loss: 25.3673\n",
      "Epoch [6/10], Step [74/125], D_loss: 0.5984, G_loss: 32.4073\n",
      "Epoch [6/10], Step [75/125], D_loss: 0.4409, G_loss: 35.0954\n",
      "Epoch [6/10], Step [76/125], D_loss: 0.3548, G_loss: 25.4860\n",
      "Epoch [6/10], Step [77/125], D_loss: 0.6247, G_loss: 16.9670\n",
      "Epoch [6/10], Step [78/125], D_loss: 0.8005, G_loss: 26.1575\n",
      "Epoch [6/10], Step [79/125], D_loss: 0.5879, G_loss: 20.2643\n",
      "Epoch [6/10], Step [80/125], D_loss: 0.7558, G_loss: 26.2243\n",
      "Epoch [6/10], Step [81/125], D_loss: 0.3388, G_loss: 30.7094\n",
      "Epoch [6/10], Step [82/125], D_loss: 0.1590, G_loss: 30.1974\n",
      "Epoch [6/10], Step [83/125], D_loss: 0.2154, G_loss: 38.0213\n",
      "Epoch [6/10], Step [84/125], D_loss: 0.7104, G_loss: 17.4358\n",
      "Epoch [6/10], Step [85/125], D_loss: 0.7238, G_loss: 19.8291\n",
      "Epoch [6/10], Step [86/125], D_loss: 0.2073, G_loss: 28.9486\n",
      "Epoch [6/10], Step [87/125], D_loss: 0.4389, G_loss: 22.4803\n",
      "Epoch [6/10], Step [88/125], D_loss: 0.2395, G_loss: 21.9946\n",
      "Epoch [6/10], Step [89/125], D_loss: 1.1719, G_loss: 19.4471\n",
      "Epoch [6/10], Step [90/125], D_loss: 0.4241, G_loss: 17.3481\n",
      "Epoch [6/10], Step [91/125], D_loss: 0.1825, G_loss: 37.3440\n",
      "Epoch [6/10], Step [92/125], D_loss: 0.2648, G_loss: 24.9625\n",
      "Epoch [6/10], Step [93/125], D_loss: 0.0858, G_loss: 36.0393\n",
      "Epoch [6/10], Step [94/125], D_loss: 0.2028, G_loss: 19.7351\n",
      "Epoch [6/10], Step [95/125], D_loss: 0.1902, G_loss: 20.9930\n",
      "Epoch [6/10], Step [96/125], D_loss: 0.2419, G_loss: 22.6737\n",
      "Epoch [6/10], Step [97/125], D_loss: 0.4612, G_loss: 24.7557\n",
      "Epoch [6/10], Step [98/125], D_loss: 0.4075, G_loss: 33.8214\n",
      "Epoch [6/10], Step [99/125], D_loss: 0.0985, G_loss: 30.5138\n",
      "Epoch [6/10], Step [100/125], D_loss: 0.1164, G_loss: 30.5518\n",
      "Epoch [6/10], Step [101/125], D_loss: 0.2503, G_loss: 26.9189\n",
      "Epoch [6/10], Step [102/125], D_loss: 0.3277, G_loss: 24.4071\n",
      "Epoch [6/10], Step [103/125], D_loss: 0.2959, G_loss: 36.2437\n",
      "Epoch [6/10], Step [104/125], D_loss: 0.2738, G_loss: 25.4604\n",
      "Epoch [6/10], Step [105/125], D_loss: 0.2391, G_loss: 29.5412\n",
      "Epoch [6/10], Step [106/125], D_loss: 0.1328, G_loss: 44.6762\n",
      "Epoch [6/10], Step [107/125], D_loss: 0.5680, G_loss: 29.5905\n",
      "Epoch [6/10], Step [108/125], D_loss: 0.3646, G_loss: 20.5688\n",
      "Epoch [6/10], Step [109/125], D_loss: 0.3518, G_loss: 19.5694\n",
      "Epoch [6/10], Step [110/125], D_loss: 0.8374, G_loss: 24.2295\n",
      "Epoch [6/10], Step [111/125], D_loss: 0.6070, G_loss: 15.8482\n",
      "Epoch [6/10], Step [112/125], D_loss: 0.5956, G_loss: 28.8117\n",
      "Epoch [6/10], Step [113/125], D_loss: 0.5018, G_loss: 26.4957\n",
      "Epoch [6/10], Step [114/125], D_loss: 0.5047, G_loss: 26.1869\n",
      "Epoch [6/10], Step [115/125], D_loss: 0.4008, G_loss: 18.9819\n",
      "Epoch [6/10], Step [116/125], D_loss: 0.2361, G_loss: 41.7571\n",
      "Epoch [6/10], Step [117/125], D_loss: 0.2782, G_loss: 32.1743\n",
      "Epoch [6/10], Step [118/125], D_loss: 0.5469, G_loss: 20.4374\n",
      "Epoch [6/10], Step [119/125], D_loss: 0.4591, G_loss: 33.7747\n",
      "Epoch [6/10], Step [120/125], D_loss: 0.3976, G_loss: 25.3399\n",
      "Epoch [6/10], Step [121/125], D_loss: 0.2547, G_loss: 37.3361\n",
      "Epoch [6/10], Step [122/125], D_loss: 0.1631, G_loss: 32.0239\n",
      "Epoch [6/10], Step [123/125], D_loss: 0.2058, G_loss: 35.8182\n",
      "Epoch [6/10], Step [124/125], D_loss: 1.4746, G_loss: 27.1795\n",
      "Epoch [6/10], Step [125/125], D_loss: 0.8627, G_loss: 31.0227\n",
      "Epoch [7/10], Step [1/125], D_loss: 0.5093, G_loss: 39.7824\n",
      "Epoch [7/10], Step [2/125], D_loss: 1.2405, G_loss: 24.9312\n",
      "Epoch [7/10], Step [3/125], D_loss: 0.6388, G_loss: 25.5359\n",
      "Epoch [7/10], Step [4/125], D_loss: 0.4798, G_loss: 30.9622\n",
      "Epoch [7/10], Step [5/125], D_loss: 0.2641, G_loss: 24.7256\n",
      "Epoch [7/10], Step [6/125], D_loss: 0.2439, G_loss: 35.0608\n",
      "Epoch [7/10], Step [7/125], D_loss: 0.2688, G_loss: 27.0532\n",
      "Epoch [7/10], Step [8/125], D_loss: 1.3401, G_loss: 21.8198\n",
      "Epoch [7/10], Step [9/125], D_loss: 0.5950, G_loss: 21.7335\n",
      "Epoch [7/10], Step [10/125], D_loss: 0.7319, G_loss: 23.6789\n",
      "Epoch [7/10], Step [11/125], D_loss: 0.5563, G_loss: 27.1694\n",
      "Epoch [7/10], Step [12/125], D_loss: 0.9526, G_loss: 26.9162\n",
      "Epoch [7/10], Step [13/125], D_loss: 0.2529, G_loss: 47.4258\n",
      "Epoch [7/10], Step [14/125], D_loss: 1.0756, G_loss: 20.4020\n",
      "Epoch [7/10], Step [15/125], D_loss: 0.2631, G_loss: 23.3791\n",
      "Epoch [7/10], Step [16/125], D_loss: 0.4248, G_loss: 28.0963\n",
      "Epoch [7/10], Step [17/125], D_loss: 0.2069, G_loss: 25.5725\n",
      "Epoch [7/10], Step [18/125], D_loss: 0.3153, G_loss: 26.9984\n",
      "Epoch [7/10], Step [19/125], D_loss: 0.1951, G_loss: 44.5229\n",
      "Epoch [7/10], Step [20/125], D_loss: 0.4951, G_loss: 13.6370\n",
      "Epoch [7/10], Step [21/125], D_loss: 0.8185, G_loss: 24.2695\n",
      "Epoch [7/10], Step [22/125], D_loss: 0.6587, G_loss: 18.1592\n",
      "Epoch [7/10], Step [23/125], D_loss: 0.5462, G_loss: 33.1534\n",
      "Epoch [7/10], Step [24/125], D_loss: 0.1938, G_loss: 35.4446\n",
      "Epoch [7/10], Step [25/125], D_loss: 0.4291, G_loss: 25.9608\n",
      "Epoch [7/10], Step [26/125], D_loss: 0.7083, G_loss: 28.7219\n",
      "Epoch [7/10], Step [27/125], D_loss: 0.7994, G_loss: 23.6482\n",
      "Epoch [7/10], Step [28/125], D_loss: 0.2493, G_loss: 24.5473\n",
      "Epoch [7/10], Step [29/125], D_loss: 0.1550, G_loss: 33.5569\n",
      "Epoch [7/10], Step [30/125], D_loss: 0.2143, G_loss: 22.4231\n",
      "Epoch [7/10], Step [31/125], D_loss: 0.3544, G_loss: 11.7436\n",
      "Epoch [7/10], Step [32/125], D_loss: 0.6503, G_loss: 32.1477\n",
      "Epoch [7/10], Step [33/125], D_loss: 1.6992, G_loss: 25.0453\n",
      "Epoch [7/10], Step [34/125], D_loss: 0.3448, G_loss: 25.0965\n",
      "Epoch [7/10], Step [35/125], D_loss: 0.1941, G_loss: 22.4823\n",
      "Epoch [7/10], Step [36/125], D_loss: 0.5362, G_loss: 25.0248\n",
      "Epoch [7/10], Step [37/125], D_loss: 0.2925, G_loss: 32.2930\n",
      "Epoch [7/10], Step [38/125], D_loss: 0.8155, G_loss: 20.9975\n",
      "Epoch [7/10], Step [39/125], D_loss: 0.2939, G_loss: 17.1180\n",
      "Epoch [7/10], Step [40/125], D_loss: 0.3061, G_loss: 17.8734\n",
      "Epoch [7/10], Step [41/125], D_loss: 0.2293, G_loss: 32.6707\n",
      "Epoch [7/10], Step [42/125], D_loss: 0.2364, G_loss: 36.9452\n",
      "Epoch [7/10], Step [43/125], D_loss: 0.1485, G_loss: 45.3546\n",
      "Epoch [7/10], Step [44/125], D_loss: 0.1626, G_loss: 30.8030\n",
      "Epoch [7/10], Step [45/125], D_loss: 0.1165, G_loss: 32.0759\n",
      "Epoch [7/10], Step [46/125], D_loss: 0.1880, G_loss: 25.4303\n",
      "Epoch [7/10], Step [47/125], D_loss: 0.3526, G_loss: 27.0654\n",
      "Epoch [7/10], Step [48/125], D_loss: 0.6601, G_loss: 18.1734\n",
      "Epoch [7/10], Step [49/125], D_loss: 0.4704, G_loss: 22.0101\n",
      "Epoch [7/10], Step [50/125], D_loss: 0.2354, G_loss: 22.9532\n",
      "Epoch [7/10], Step [51/125], D_loss: 0.6512, G_loss: 21.3683\n",
      "Epoch [7/10], Step [52/125], D_loss: 0.1166, G_loss: 29.9722\n",
      "Epoch [7/10], Step [53/125], D_loss: 0.3623, G_loss: 24.1428\n",
      "Epoch [7/10], Step [54/125], D_loss: 0.3374, G_loss: 16.2507\n",
      "Epoch [7/10], Step [55/125], D_loss: 0.7813, G_loss: 26.3111\n",
      "Epoch [7/10], Step [56/125], D_loss: 0.5583, G_loss: 22.1800\n",
      "Epoch [7/10], Step [57/125], D_loss: 0.4307, G_loss: 17.1776\n",
      "Epoch [7/10], Step [58/125], D_loss: 0.7656, G_loss: 36.2764\n",
      "Epoch [7/10], Step [59/125], D_loss: 0.1219, G_loss: 23.9031\n",
      "Epoch [7/10], Step [60/125], D_loss: 0.1599, G_loss: 33.5326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Step [61/125], D_loss: 1.3558, G_loss: 14.9831\n",
      "Epoch [7/10], Step [62/125], D_loss: 0.4619, G_loss: 23.3790\n",
      "Epoch [7/10], Step [63/125], D_loss: 1.1183, G_loss: 20.6010\n",
      "Epoch [7/10], Step [64/125], D_loss: 0.6076, G_loss: 26.5957\n",
      "Epoch [7/10], Step [65/125], D_loss: 0.5092, G_loss: 27.4717\n",
      "Epoch [7/10], Step [66/125], D_loss: 1.2486, G_loss: 23.8506\n",
      "Epoch [7/10], Step [67/125], D_loss: 0.4606, G_loss: 39.6733\n",
      "Epoch [7/10], Step [68/125], D_loss: 0.3399, G_loss: 37.0134\n",
      "Epoch [7/10], Step [69/125], D_loss: 0.7382, G_loss: 26.5749\n",
      "Epoch [7/10], Step [70/125], D_loss: 0.8012, G_loss: 29.6810\n",
      "Epoch [7/10], Step [71/125], D_loss: 0.1606, G_loss: 39.9735\n",
      "Epoch [7/10], Step [72/125], D_loss: 0.6483, G_loss: 19.1168\n",
      "Epoch [7/10], Step [73/125], D_loss: 0.1383, G_loss: 35.5513\n",
      "Epoch [7/10], Step [74/125], D_loss: 0.7700, G_loss: 17.7228\n",
      "Epoch [7/10], Step [75/125], D_loss: 1.7778, G_loss: 32.9964\n",
      "Epoch [7/10], Step [76/125], D_loss: 0.6413, G_loss: 22.1895\n",
      "Epoch [7/10], Step [77/125], D_loss: 0.5533, G_loss: 30.4474\n",
      "Epoch [7/10], Step [78/125], D_loss: 1.0766, G_loss: 26.3499\n",
      "Epoch [7/10], Step [79/125], D_loss: 0.6599, G_loss: 17.0492\n",
      "Epoch [7/10], Step [80/125], D_loss: 0.2209, G_loss: 51.5434\n",
      "Epoch [7/10], Step [81/125], D_loss: 0.5082, G_loss: 29.4350\n",
      "Epoch [7/10], Step [82/125], D_loss: 0.5993, G_loss: 40.0859\n",
      "Epoch [7/10], Step [83/125], D_loss: 0.6354, G_loss: 28.3053\n",
      "Epoch [7/10], Step [84/125], D_loss: 0.9918, G_loss: 31.7189\n",
      "Epoch [7/10], Step [85/125], D_loss: 0.5234, G_loss: 22.4913\n",
      "Epoch [7/10], Step [86/125], D_loss: 0.2412, G_loss: 26.8753\n",
      "Epoch [7/10], Step [87/125], D_loss: 0.1815, G_loss: 31.4092\n",
      "Epoch [7/10], Step [88/125], D_loss: 1.8552, G_loss: 21.8528\n",
      "Epoch [7/10], Step [89/125], D_loss: 0.3788, G_loss: 21.2410\n",
      "Epoch [7/10], Step [90/125], D_loss: 0.3824, G_loss: 31.1572\n",
      "Epoch [7/10], Step [91/125], D_loss: 0.3524, G_loss: 25.0371\n",
      "Epoch [7/10], Step [92/125], D_loss: 0.4575, G_loss: 16.0418\n",
      "Epoch [7/10], Step [93/125], D_loss: 0.1995, G_loss: 45.3932\n",
      "Epoch [7/10], Step [94/125], D_loss: 0.4989, G_loss: 25.8213\n",
      "Epoch [7/10], Step [95/125], D_loss: 0.3782, G_loss: 37.6286\n",
      "Epoch [7/10], Step [96/125], D_loss: 0.3124, G_loss: 31.6989\n",
      "Epoch [7/10], Step [97/125], D_loss: 0.4746, G_loss: 23.7059\n",
      "Epoch [7/10], Step [98/125], D_loss: 0.5855, G_loss: 27.3861\n",
      "Epoch [7/10], Step [99/125], D_loss: 0.5526, G_loss: 25.0745\n",
      "Epoch [7/10], Step [100/125], D_loss: 0.5879, G_loss: 21.6720\n",
      "Epoch [7/10], Step [101/125], D_loss: 1.2469, G_loss: 25.5809\n",
      "Epoch [7/10], Step [102/125], D_loss: 0.5995, G_loss: 30.4579\n",
      "Epoch [7/10], Step [103/125], D_loss: 0.6549, G_loss: 19.3644\n",
      "Epoch [7/10], Step [104/125], D_loss: 0.3829, G_loss: 25.3504\n",
      "Epoch [7/10], Step [105/125], D_loss: 0.7061, G_loss: 20.7211\n",
      "Epoch [7/10], Step [106/125], D_loss: 0.4485, G_loss: 22.6022\n",
      "Epoch [7/10], Step [107/125], D_loss: 0.7086, G_loss: 20.0247\n",
      "Epoch [7/10], Step [108/125], D_loss: 1.1828, G_loss: 26.5223\n",
      "Epoch [7/10], Step [109/125], D_loss: 1.7459, G_loss: 14.5309\n",
      "Epoch [7/10], Step [110/125], D_loss: 0.4441, G_loss: 25.8974\n",
      "Epoch [7/10], Step [111/125], D_loss: 1.0197, G_loss: 21.0361\n",
      "Epoch [7/10], Step [112/125], D_loss: 0.3571, G_loss: 31.0571\n",
      "Epoch [7/10], Step [113/125], D_loss: 0.6892, G_loss: 16.5335\n",
      "Epoch [7/10], Step [114/125], D_loss: 0.1797, G_loss: 23.9301\n",
      "Epoch [7/10], Step [115/125], D_loss: 0.4038, G_loss: 28.2931\n",
      "Epoch [7/10], Step [116/125], D_loss: 0.3577, G_loss: 24.8174\n",
      "Epoch [7/10], Step [117/125], D_loss: 0.3826, G_loss: 39.3406\n",
      "Epoch [7/10], Step [118/125], D_loss: 0.2703, G_loss: 20.2582\n",
      "Epoch [7/10], Step [119/125], D_loss: 0.7799, G_loss: 17.1674\n",
      "Epoch [7/10], Step [120/125], D_loss: 0.8362, G_loss: 18.3879\n",
      "Epoch [7/10], Step [121/125], D_loss: 0.5670, G_loss: 23.0900\n",
      "Epoch [7/10], Step [122/125], D_loss: 0.2000, G_loss: 28.4490\n",
      "Epoch [7/10], Step [123/125], D_loss: 0.5972, G_loss: 16.2618\n",
      "Epoch [7/10], Step [124/125], D_loss: 0.0669, G_loss: 38.3899\n",
      "Epoch [7/10], Step [125/125], D_loss: 0.5945, G_loss: 31.4154\n",
      "Epoch [8/10], Step [1/125], D_loss: 0.6449, G_loss: 25.9049\n",
      "Epoch [8/10], Step [2/125], D_loss: 0.0828, G_loss: 36.1316\n",
      "Epoch [8/10], Step [3/125], D_loss: 0.4094, G_loss: 35.2013\n",
      "Epoch [8/10], Step [4/125], D_loss: 0.3205, G_loss: 26.3263\n",
      "Epoch [8/10], Step [5/125], D_loss: 0.0996, G_loss: 28.5254\n",
      "Epoch [8/10], Step [6/125], D_loss: 1.4014, G_loss: 17.1422\n",
      "Epoch [8/10], Step [7/125], D_loss: 0.6760, G_loss: 29.5677\n",
      "Epoch [8/10], Step [8/125], D_loss: 0.8223, G_loss: 24.2108\n",
      "Epoch [8/10], Step [9/125], D_loss: 0.7754, G_loss: 30.9507\n",
      "Epoch [8/10], Step [10/125], D_loss: 0.2589, G_loss: 33.5441\n",
      "Epoch [8/10], Step [11/125], D_loss: 0.3060, G_loss: 30.4251\n",
      "Epoch [8/10], Step [12/125], D_loss: 0.2206, G_loss: 21.2614\n",
      "Epoch [8/10], Step [13/125], D_loss: 0.4417, G_loss: 16.2040\n",
      "Epoch [8/10], Step [14/125], D_loss: 0.2228, G_loss: 29.3701\n",
      "Epoch [8/10], Step [15/125], D_loss: 0.1144, G_loss: 27.7079\n",
      "Epoch [8/10], Step [16/125], D_loss: 0.2287, G_loss: 33.0011\n",
      "Epoch [8/10], Step [17/125], D_loss: 0.4626, G_loss: 26.2442\n",
      "Epoch [8/10], Step [18/125], D_loss: 0.3551, G_loss: 35.7444\n",
      "Epoch [8/10], Step [19/125], D_loss: 0.1602, G_loss: 26.5897\n",
      "Epoch [8/10], Step [20/125], D_loss: 0.5956, G_loss: 45.4513\n",
      "Epoch [8/10], Step [21/125], D_loss: 0.3560, G_loss: 22.5283\n",
      "Epoch [8/10], Step [22/125], D_loss: 0.2471, G_loss: 37.8882\n",
      "Epoch [8/10], Step [23/125], D_loss: 0.2928, G_loss: 23.1467\n",
      "Epoch [8/10], Step [24/125], D_loss: 2.4355, G_loss: 22.2401\n",
      "Epoch [8/10], Step [25/125], D_loss: 1.2194, G_loss: 25.6390\n",
      "Epoch [8/10], Step [26/125], D_loss: 1.5497, G_loss: 18.4759\n",
      "Epoch [8/10], Step [27/125], D_loss: 0.5935, G_loss: 20.0619\n",
      "Epoch [8/10], Step [28/125], D_loss: 0.5221, G_loss: 30.3511\n",
      "Epoch [8/10], Step [29/125], D_loss: 0.3267, G_loss: 35.9149\n",
      "Epoch [8/10], Step [30/125], D_loss: 0.9027, G_loss: 23.1021\n",
      "Epoch [8/10], Step [31/125], D_loss: 0.5272, G_loss: 17.0220\n",
      "Epoch [8/10], Step [32/125], D_loss: 0.4024, G_loss: 34.7183\n",
      "Epoch [8/10], Step [33/125], D_loss: 0.9720, G_loss: 22.3437\n",
      "Epoch [8/10], Step [34/125], D_loss: 0.5865, G_loss: 15.3626\n",
      "Epoch [8/10], Step [35/125], D_loss: 1.2694, G_loss: 17.1692\n",
      "Epoch [8/10], Step [36/125], D_loss: 1.2769, G_loss: 20.2879\n",
      "Epoch [8/10], Step [37/125], D_loss: 0.9388, G_loss: 15.7159\n",
      "Epoch [8/10], Step [38/125], D_loss: 0.5009, G_loss: 25.3039\n",
      "Epoch [8/10], Step [39/125], D_loss: 1.2047, G_loss: 22.0613\n",
      "Epoch [8/10], Step [40/125], D_loss: 0.3254, G_loss: 29.9997\n",
      "Epoch [8/10], Step [41/125], D_loss: 0.2564, G_loss: 23.9664\n",
      "Epoch [8/10], Step [42/125], D_loss: 0.1850, G_loss: 33.1430\n",
      "Epoch [8/10], Step [43/125], D_loss: 1.2205, G_loss: 18.7149\n",
      "Epoch [8/10], Step [44/125], D_loss: 0.3009, G_loss: 16.5475\n",
      "Epoch [8/10], Step [45/125], D_loss: 0.6242, G_loss: 22.4578\n",
      "Epoch [8/10], Step [46/125], D_loss: 0.7857, G_loss: 17.7552\n",
      "Epoch [8/10], Step [47/125], D_loss: 0.4692, G_loss: 19.7640\n",
      "Epoch [8/10], Step [48/125], D_loss: 0.7006, G_loss: 28.3664\n",
      "Epoch [8/10], Step [49/125], D_loss: 0.3056, G_loss: 32.0363\n",
      "Epoch [8/10], Step [50/125], D_loss: 0.2107, G_loss: 29.6292\n",
      "Epoch [8/10], Step [51/125], D_loss: 0.2836, G_loss: 27.4978\n",
      "Epoch [8/10], Step [52/125], D_loss: 0.2374, G_loss: 17.6358\n",
      "Epoch [8/10], Step [53/125], D_loss: 0.1965, G_loss: 33.1715\n",
      "Epoch [8/10], Step [54/125], D_loss: 0.2200, G_loss: 24.9461\n",
      "Epoch [8/10], Step [55/125], D_loss: 0.5020, G_loss: 16.7310\n",
      "Epoch [8/10], Step [56/125], D_loss: 0.4672, G_loss: 38.8264\n",
      "Epoch [8/10], Step [57/125], D_loss: 0.2189, G_loss: 30.3108\n",
      "Epoch [8/10], Step [58/125], D_loss: 0.1997, G_loss: 41.2183\n",
      "Epoch [8/10], Step [59/125], D_loss: 0.3575, G_loss: 33.0279\n",
      "Epoch [8/10], Step [60/125], D_loss: 0.7629, G_loss: 21.9522\n",
      "Epoch [8/10], Step [61/125], D_loss: 0.2996, G_loss: 24.0055\n",
      "Epoch [8/10], Step [62/125], D_loss: 0.8500, G_loss: 23.1236\n",
      "Epoch [8/10], Step [63/125], D_loss: 0.9927, G_loss: 13.5926\n",
      "Epoch [8/10], Step [64/125], D_loss: 0.5310, G_loss: 28.4928\n",
      "Epoch [8/10], Step [65/125], D_loss: 1.0287, G_loss: 20.2764\n",
      "Epoch [8/10], Step [66/125], D_loss: 0.3527, G_loss: 30.0765\n",
      "Epoch [8/10], Step [67/125], D_loss: 0.9587, G_loss: 23.2701\n",
      "Epoch [8/10], Step [68/125], D_loss: 0.6037, G_loss: 35.8428\n",
      "Epoch [8/10], Step [69/125], D_loss: 0.1647, G_loss: 29.2911\n",
      "Epoch [8/10], Step [70/125], D_loss: 1.0253, G_loss: 15.9984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Step [71/125], D_loss: 0.5967, G_loss: 21.4030\n",
      "Epoch [8/10], Step [72/125], D_loss: 0.2719, G_loss: 31.7750\n",
      "Epoch [8/10], Step [73/125], D_loss: 0.3901, G_loss: 17.6869\n",
      "Epoch [8/10], Step [74/125], D_loss: 0.2912, G_loss: 20.3583\n",
      "Epoch [8/10], Step [75/125], D_loss: 0.9322, G_loss: 14.9074\n",
      "Epoch [8/10], Step [76/125], D_loss: 0.6532, G_loss: 29.7455\n",
      "Epoch [8/10], Step [77/125], D_loss: 0.3779, G_loss: 26.9162\n",
      "Epoch [8/10], Step [78/125], D_loss: 0.3715, G_loss: 30.9187\n",
      "Epoch [8/10], Step [79/125], D_loss: 0.3027, G_loss: 27.2999\n",
      "Epoch [8/10], Step [80/125], D_loss: 0.4594, G_loss: 26.9159\n",
      "Epoch [8/10], Step [81/125], D_loss: 0.4329, G_loss: 26.3081\n",
      "Epoch [8/10], Step [82/125], D_loss: 1.0024, G_loss: 23.6033\n",
      "Epoch [8/10], Step [83/125], D_loss: 0.8066, G_loss: 35.8305\n",
      "Epoch [8/10], Step [84/125], D_loss: 0.1740, G_loss: 23.8443\n",
      "Epoch [8/10], Step [85/125], D_loss: 0.3339, G_loss: 25.4874\n",
      "Epoch [8/10], Step [86/125], D_loss: 0.7342, G_loss: 24.2318\n",
      "Epoch [8/10], Step [87/125], D_loss: 0.4578, G_loss: 23.5228\n",
      "Epoch [8/10], Step [88/125], D_loss: 0.3103, G_loss: 46.2798\n",
      "Epoch [8/10], Step [89/125], D_loss: 0.2068, G_loss: 24.4670\n",
      "Epoch [8/10], Step [90/125], D_loss: 0.1539, G_loss: 24.1521\n",
      "Epoch [8/10], Step [91/125], D_loss: 0.2675, G_loss: 28.9440\n",
      "Epoch [8/10], Step [92/125], D_loss: 1.2367, G_loss: 24.5288\n",
      "Epoch [8/10], Step [93/125], D_loss: 0.4489, G_loss: 17.3902\n",
      "Epoch [8/10], Step [94/125], D_loss: 0.8569, G_loss: 32.2225\n",
      "Epoch [8/10], Step [95/125], D_loss: 0.8424, G_loss: 26.5540\n",
      "Epoch [8/10], Step [96/125], D_loss: 0.4769, G_loss: 23.4574\n",
      "Epoch [8/10], Step [97/125], D_loss: 0.0599, G_loss: 27.8034\n",
      "Epoch [8/10], Step [98/125], D_loss: 0.3384, G_loss: 24.9853\n",
      "Epoch [8/10], Step [99/125], D_loss: 0.4601, G_loss: 62.5395\n",
      "Epoch [8/10], Step [100/125], D_loss: 0.3959, G_loss: 36.2086\n",
      "Epoch [8/10], Step [101/125], D_loss: 0.1846, G_loss: 31.3391\n",
      "Epoch [8/10], Step [102/125], D_loss: 0.3835, G_loss: 27.7954\n",
      "Epoch [8/10], Step [103/125], D_loss: 0.9530, G_loss: 26.7947\n",
      "Epoch [8/10], Step [104/125], D_loss: 0.2304, G_loss: 31.3903\n",
      "Epoch [8/10], Step [105/125], D_loss: 0.9135, G_loss: 22.8363\n",
      "Epoch [8/10], Step [106/125], D_loss: 0.5496, G_loss: 24.7800\n",
      "Epoch [8/10], Step [107/125], D_loss: 2.5896, G_loss: 26.0286\n",
      "Epoch [8/10], Step [108/125], D_loss: 0.4859, G_loss: 26.4940\n",
      "Epoch [8/10], Step [109/125], D_loss: 0.1385, G_loss: 22.1307\n",
      "Epoch [8/10], Step [110/125], D_loss: 1.9536, G_loss: 22.1993\n",
      "Epoch [8/10], Step [111/125], D_loss: 0.1895, G_loss: 31.3716\n",
      "Epoch [8/10], Step [112/125], D_loss: 0.4419, G_loss: 21.0283\n",
      "Epoch [8/10], Step [113/125], D_loss: 0.3915, G_loss: 15.6065\n",
      "Epoch [8/10], Step [114/125], D_loss: 0.8126, G_loss: 22.5235\n",
      "Epoch [8/10], Step [115/125], D_loss: 1.0208, G_loss: 22.2311\n",
      "Epoch [8/10], Step [116/125], D_loss: 1.2963, G_loss: 17.0027\n",
      "Epoch [8/10], Step [117/125], D_loss: 0.5033, G_loss: 16.2630\n",
      "Epoch [8/10], Step [118/125], D_loss: 1.3048, G_loss: 24.3257\n",
      "Epoch [8/10], Step [119/125], D_loss: 0.0703, G_loss: 40.8888\n",
      "Epoch [8/10], Step [120/125], D_loss: 0.2172, G_loss: 23.6242\n",
      "Epoch [8/10], Step [121/125], D_loss: 0.0949, G_loss: 36.0588\n",
      "Epoch [8/10], Step [122/125], D_loss: 0.2886, G_loss: 28.6588\n",
      "Epoch [8/10], Step [123/125], D_loss: 1.2110, G_loss: 18.6117\n",
      "Epoch [8/10], Step [124/125], D_loss: 0.5443, G_loss: 18.8445\n",
      "Epoch [8/10], Step [125/125], D_loss: 0.2278, G_loss: 17.3306\n",
      "Epoch [9/10], Step [1/125], D_loss: 0.1390, G_loss: 27.7364\n",
      "Epoch [9/10], Step [2/125], D_loss: 0.3212, G_loss: 20.6059\n",
      "Epoch [9/10], Step [3/125], D_loss: 0.3689, G_loss: 17.0328\n",
      "Epoch [9/10], Step [4/125], D_loss: 0.2951, G_loss: 32.4820\n",
      "Epoch [9/10], Step [5/125], D_loss: 0.1024, G_loss: 32.7323\n",
      "Epoch [9/10], Step [6/125], D_loss: 0.2094, G_loss: 20.5551\n",
      "Epoch [9/10], Step [7/125], D_loss: 0.5740, G_loss: 16.4502\n",
      "Epoch [9/10], Step [8/125], D_loss: 0.7369, G_loss: 20.9345\n",
      "Epoch [9/10], Step [9/125], D_loss: 0.4473, G_loss: 21.0070\n",
      "Epoch [9/10], Step [10/125], D_loss: 0.8063, G_loss: 23.5213\n",
      "Epoch [9/10], Step [11/125], D_loss: 0.3825, G_loss: 20.4306\n",
      "Epoch [9/10], Step [12/125], D_loss: 0.1363, G_loss: 27.6301\n",
      "Epoch [9/10], Step [13/125], D_loss: 0.1077, G_loss: 35.5026\n",
      "Epoch [9/10], Step [14/125], D_loss: 1.6479, G_loss: 17.8336\n",
      "Epoch [9/10], Step [15/125], D_loss: 1.0869, G_loss: 25.9350\n",
      "Epoch [9/10], Step [16/125], D_loss: 0.2699, G_loss: 38.0253\n",
      "Epoch [9/10], Step [17/125], D_loss: 0.3455, G_loss: 18.5901\n",
      "Epoch [9/10], Step [18/125], D_loss: 0.3788, G_loss: 27.2238\n",
      "Epoch [9/10], Step [19/125], D_loss: 0.3268, G_loss: 19.6094\n",
      "Epoch [9/10], Step [20/125], D_loss: 0.1240, G_loss: 32.6774\n",
      "Epoch [9/10], Step [21/125], D_loss: 0.6972, G_loss: 21.2779\n",
      "Epoch [9/10], Step [22/125], D_loss: 0.2690, G_loss: 18.5847\n",
      "Epoch [9/10], Step [23/125], D_loss: 0.6909, G_loss: 39.0300\n",
      "Epoch [9/10], Step [24/125], D_loss: 0.1090, G_loss: 24.9235\n",
      "Epoch [9/10], Step [25/125], D_loss: 1.4525, G_loss: 18.6488\n",
      "Epoch [9/10], Step [26/125], D_loss: 0.4145, G_loss: 38.3414\n",
      "Epoch [9/10], Step [27/125], D_loss: 0.2644, G_loss: 25.5193\n",
      "Epoch [9/10], Step [28/125], D_loss: 0.3287, G_loss: 15.4814\n",
      "Epoch [9/10], Step [29/125], D_loss: 0.0781, G_loss: 31.4604\n",
      "Epoch [9/10], Step [30/125], D_loss: 0.8541, G_loss: 18.8019\n",
      "Epoch [9/10], Step [31/125], D_loss: 1.1684, G_loss: 18.4474\n",
      "Epoch [9/10], Step [32/125], D_loss: 0.8622, G_loss: 20.1081\n",
      "Epoch [9/10], Step [33/125], D_loss: 0.2529, G_loss: 40.7152\n",
      "Epoch [9/10], Step [34/125], D_loss: 0.8796, G_loss: 15.4813\n",
      "Epoch [9/10], Step [35/125], D_loss: 0.2247, G_loss: 33.8028\n",
      "Epoch [9/10], Step [36/125], D_loss: 1.7607, G_loss: 14.9860\n",
      "Epoch [9/10], Step [37/125], D_loss: 0.7696, G_loss: 24.6138\n",
      "Epoch [9/10], Step [38/125], D_loss: 0.8625, G_loss: 22.9965\n",
      "Epoch [9/10], Step [39/125], D_loss: 1.0256, G_loss: 16.2186\n",
      "Epoch [9/10], Step [40/125], D_loss: 0.5314, G_loss: 32.2305\n",
      "Epoch [9/10], Step [41/125], D_loss: 0.4831, G_loss: 20.9217\n",
      "Epoch [9/10], Step [42/125], D_loss: 0.6343, G_loss: 27.2579\n",
      "Epoch [9/10], Step [43/125], D_loss: 0.2339, G_loss: 40.5178\n",
      "Epoch [9/10], Step [44/125], D_loss: 0.7073, G_loss: 24.5595\n",
      "Epoch [9/10], Step [45/125], D_loss: 0.6153, G_loss: 13.6120\n",
      "Epoch [9/10], Step [46/125], D_loss: 1.0640, G_loss: 25.5375\n",
      "Epoch [9/10], Step [47/125], D_loss: 0.4162, G_loss: 25.1574\n",
      "Epoch [9/10], Step [48/125], D_loss: 0.4309, G_loss: 23.0193\n",
      "Epoch [9/10], Step [49/125], D_loss: 0.8355, G_loss: 20.0553\n",
      "Epoch [9/10], Step [50/125], D_loss: 1.1871, G_loss: 32.6429\n",
      "Epoch [9/10], Step [51/125], D_loss: 0.4085, G_loss: 31.7246\n",
      "Epoch [9/10], Step [52/125], D_loss: 0.3890, G_loss: 21.6577\n",
      "Epoch [9/10], Step [53/125], D_loss: 1.3431, G_loss: 17.5129\n",
      "Epoch [9/10], Step [54/125], D_loss: 0.3278, G_loss: 33.7531\n",
      "Epoch [9/10], Step [55/125], D_loss: 0.4599, G_loss: 23.9618\n",
      "Epoch [9/10], Step [56/125], D_loss: 0.5925, G_loss: 21.4915\n",
      "Epoch [9/10], Step [57/125], D_loss: 0.3493, G_loss: 27.8944\n",
      "Epoch [9/10], Step [58/125], D_loss: 0.1530, G_loss: 28.6008\n",
      "Epoch [9/10], Step [59/125], D_loss: 0.3469, G_loss: 42.9242\n",
      "Epoch [9/10], Step [60/125], D_loss: 0.1534, G_loss: 22.5979\n",
      "Epoch [9/10], Step [61/125], D_loss: 0.1932, G_loss: 28.8780\n",
      "Epoch [9/10], Step [62/125], D_loss: 0.2055, G_loss: 22.3719\n",
      "Epoch [9/10], Step [63/125], D_loss: 0.4226, G_loss: 18.4250\n",
      "Epoch [9/10], Step [64/125], D_loss: 0.1483, G_loss: 28.0966\n",
      "Epoch [9/10], Step [65/125], D_loss: 0.3879, G_loss: 21.2845\n",
      "Epoch [9/10], Step [66/125], D_loss: 2.6053, G_loss: 26.7974\n",
      "Epoch [9/10], Step [67/125], D_loss: 1.1330, G_loss: 22.5793\n",
      "Epoch [9/10], Step [68/125], D_loss: 0.6509, G_loss: 29.7949\n",
      "Epoch [9/10], Step [69/125], D_loss: 0.4000, G_loss: 30.6316\n",
      "Epoch [9/10], Step [70/125], D_loss: 0.7560, G_loss: 15.6414\n",
      "Epoch [9/10], Step [71/125], D_loss: 1.2063, G_loss: 18.3909\n",
      "Epoch [9/10], Step [72/125], D_loss: 0.5014, G_loss: 20.6717\n",
      "Epoch [9/10], Step [73/125], D_loss: 0.4312, G_loss: 21.9141\n",
      "Epoch [9/10], Step [74/125], D_loss: 0.6455, G_loss: 10.8083\n",
      "Epoch [9/10], Step [75/125], D_loss: 0.4809, G_loss: 33.7986\n",
      "Epoch [9/10], Step [76/125], D_loss: 0.5678, G_loss: 30.6494\n",
      "Epoch [9/10], Step [77/125], D_loss: 0.2459, G_loss: 20.8177\n",
      "Epoch [9/10], Step [78/125], D_loss: 0.3280, G_loss: 37.8268\n",
      "Epoch [9/10], Step [79/125], D_loss: 0.3529, G_loss: 26.9769\n",
      "Epoch [9/10], Step [80/125], D_loss: 0.4172, G_loss: 27.0566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Step [81/125], D_loss: 0.3618, G_loss: 27.9421\n",
      "Epoch [9/10], Step [82/125], D_loss: 0.2587, G_loss: 23.7022\n",
      "Epoch [9/10], Step [83/125], D_loss: 0.5295, G_loss: 25.9255\n",
      "Epoch [9/10], Step [84/125], D_loss: 1.0343, G_loss: 18.1159\n",
      "Epoch [9/10], Step [85/125], D_loss: 0.1790, G_loss: 37.4511\n",
      "Epoch [9/10], Step [86/125], D_loss: 0.1388, G_loss: 28.4284\n",
      "Epoch [9/10], Step [87/125], D_loss: 0.4523, G_loss: 28.4875\n",
      "Epoch [9/10], Step [88/125], D_loss: 0.2183, G_loss: 23.5357\n",
      "Epoch [9/10], Step [89/125], D_loss: 0.2894, G_loss: 26.6540\n",
      "Epoch [9/10], Step [90/125], D_loss: 0.2992, G_loss: 36.6135\n",
      "Epoch [9/10], Step [91/125], D_loss: 0.1538, G_loss: 24.4400\n",
      "Epoch [9/10], Step [92/125], D_loss: 0.6560, G_loss: 21.0161\n",
      "Epoch [9/10], Step [93/125], D_loss: 0.2256, G_loss: 30.9289\n",
      "Epoch [9/10], Step [94/125], D_loss: 0.9592, G_loss: 31.4289\n",
      "Epoch [9/10], Step [95/125], D_loss: 1.4048, G_loss: 22.2356\n",
      "Epoch [9/10], Step [96/125], D_loss: 0.2143, G_loss: 41.4937\n",
      "Epoch [9/10], Step [97/125], D_loss: 0.4334, G_loss: 22.3090\n",
      "Epoch [9/10], Step [98/125], D_loss: 1.1565, G_loss: 16.6546\n",
      "Epoch [9/10], Step [99/125], D_loss: 0.1659, G_loss: 30.0330\n",
      "Epoch [9/10], Step [100/125], D_loss: 1.2692, G_loss: 30.7588\n",
      "Epoch [9/10], Step [101/125], D_loss: 1.3370, G_loss: 20.8791\n",
      "Epoch [9/10], Step [102/125], D_loss: 0.2637, G_loss: 19.6215\n",
      "Epoch [9/10], Step [103/125], D_loss: 1.5956, G_loss: 19.7833\n",
      "Epoch [9/10], Step [104/125], D_loss: 0.2471, G_loss: 37.4497\n",
      "Epoch [9/10], Step [105/125], D_loss: 0.5108, G_loss: 34.8242\n",
      "Epoch [9/10], Step [106/125], D_loss: 0.8118, G_loss: 30.6259\n",
      "Epoch [9/10], Step [107/125], D_loss: 0.6751, G_loss: 20.1474\n",
      "Epoch [9/10], Step [108/125], D_loss: 0.3295, G_loss: 26.2421\n",
      "Epoch [9/10], Step [109/125], D_loss: 0.1196, G_loss: 34.5803\n",
      "Epoch [9/10], Step [110/125], D_loss: 0.2996, G_loss: 26.1249\n",
      "Epoch [9/10], Step [111/125], D_loss: 0.1512, G_loss: 24.8219\n",
      "Epoch [9/10], Step [112/125], D_loss: 1.1086, G_loss: 14.9840\n",
      "Epoch [9/10], Step [113/125], D_loss: 0.5121, G_loss: 33.0588\n",
      "Epoch [9/10], Step [114/125], D_loss: 0.5841, G_loss: 17.8000\n",
      "Epoch [9/10], Step [115/125], D_loss: 0.5562, G_loss: 16.8664\n",
      "Epoch [9/10], Step [116/125], D_loss: 0.3833, G_loss: 24.0987\n",
      "Epoch [9/10], Step [117/125], D_loss: 0.8365, G_loss: 26.6650\n",
      "Epoch [9/10], Step [118/125], D_loss: 0.9694, G_loss: 19.3603\n",
      "Epoch [9/10], Step [119/125], D_loss: 0.3440, G_loss: 27.9025\n",
      "Epoch [9/10], Step [120/125], D_loss: 0.5584, G_loss: 24.8404\n",
      "Epoch [9/10], Step [121/125], D_loss: 0.5650, G_loss: 16.8988\n",
      "Epoch [9/10], Step [122/125], D_loss: 0.6065, G_loss: 11.0318\n",
      "Epoch [9/10], Step [123/125], D_loss: 0.5772, G_loss: 19.1577\n",
      "Epoch [9/10], Step [124/125], D_loss: 0.3786, G_loss: 18.9599\n",
      "Epoch [9/10], Step [125/125], D_loss: 0.1885, G_loss: 38.8066\n",
      "Epoch [10/10], Step [1/125], D_loss: 0.9497, G_loss: 27.1045\n",
      "Epoch [10/10], Step [2/125], D_loss: 0.3372, G_loss: 34.4950\n",
      "Epoch [10/10], Step [3/125], D_loss: 0.3564, G_loss: 23.8886\n",
      "Epoch [10/10], Step [4/125], D_loss: 0.7177, G_loss: 19.0753\n",
      "Epoch [10/10], Step [5/125], D_loss: 0.3477, G_loss: 29.7133\n",
      "Epoch [10/10], Step [6/125], D_loss: 0.5632, G_loss: 20.3788\n",
      "Epoch [10/10], Step [7/125], D_loss: 0.5829, G_loss: 18.1012\n",
      "Epoch [10/10], Step [8/125], D_loss: 0.3741, G_loss: 20.9838\n",
      "Epoch [10/10], Step [9/125], D_loss: 0.3198, G_loss: 26.3095\n",
      "Epoch [10/10], Step [10/125], D_loss: 0.2257, G_loss: 37.7955\n",
      "Epoch [10/10], Step [11/125], D_loss: 0.5257, G_loss: 12.5098\n",
      "Epoch [10/10], Step [12/125], D_loss: 0.5009, G_loss: 25.3346\n",
      "Epoch [10/10], Step [13/125], D_loss: 0.9484, G_loss: 17.2571\n",
      "Epoch [10/10], Step [14/125], D_loss: 0.5910, G_loss: 19.0771\n",
      "Epoch [10/10], Step [15/125], D_loss: 0.4064, G_loss: 25.6605\n",
      "Epoch [10/10], Step [16/125], D_loss: 0.2374, G_loss: 32.5905\n",
      "Epoch [10/10], Step [17/125], D_loss: 0.9941, G_loss: 23.3517\n",
      "Epoch [10/10], Step [18/125], D_loss: 0.1646, G_loss: 34.3534\n",
      "Epoch [10/10], Step [19/125], D_loss: 0.4405, G_loss: 15.2442\n",
      "Epoch [10/10], Step [20/125], D_loss: 0.4900, G_loss: 24.6536\n",
      "Epoch [10/10], Step [21/125], D_loss: 0.7683, G_loss: 18.8668\n",
      "Epoch [10/10], Step [22/125], D_loss: 0.9319, G_loss: 23.8297\n",
      "Epoch [10/10], Step [23/125], D_loss: 1.5032, G_loss: 17.8841\n",
      "Epoch [10/10], Step [24/125], D_loss: 0.5211, G_loss: 19.1626\n",
      "Epoch [10/10], Step [25/125], D_loss: 0.3797, G_loss: 34.3601\n",
      "Epoch [10/10], Step [26/125], D_loss: 0.4851, G_loss: 29.6768\n",
      "Epoch [10/10], Step [27/125], D_loss: 0.9283, G_loss: 24.0102\n",
      "Epoch [10/10], Step [28/125], D_loss: 0.1110, G_loss: 39.5786\n",
      "Epoch [10/10], Step [29/125], D_loss: 0.1601, G_loss: 31.0824\n",
      "Epoch [10/10], Step [30/125], D_loss: 0.1836, G_loss: 20.7869\n",
      "Epoch [10/10], Step [31/125], D_loss: 1.3103, G_loss: 19.8790\n",
      "Epoch [10/10], Step [32/125], D_loss: 0.2731, G_loss: 31.3429\n",
      "Epoch [10/10], Step [33/125], D_loss: 0.5641, G_loss: 16.1750\n",
      "Epoch [10/10], Step [34/125], D_loss: 0.3341, G_loss: 26.0981\n",
      "Epoch [10/10], Step [35/125], D_loss: 0.2785, G_loss: 22.3965\n",
      "Epoch [10/10], Step [36/125], D_loss: 0.3316, G_loss: 13.2963\n",
      "Epoch [10/10], Step [37/125], D_loss: 0.2440, G_loss: 20.8194\n",
      "Epoch [10/10], Step [38/125], D_loss: 0.5342, G_loss: 20.8759\n",
      "Epoch [10/10], Step [39/125], D_loss: 0.8381, G_loss: 26.5904\n",
      "Epoch [10/10], Step [40/125], D_loss: 0.7013, G_loss: 15.9082\n",
      "Epoch [10/10], Step [41/125], D_loss: 0.4128, G_loss: 19.7900\n",
      "Epoch [10/10], Step [42/125], D_loss: 0.3240, G_loss: 15.2103\n",
      "Epoch [10/10], Step [43/125], D_loss: 0.1599, G_loss: 42.6716\n",
      "Epoch [10/10], Step [44/125], D_loss: 0.2691, G_loss: 24.0266\n",
      "Epoch [10/10], Step [45/125], D_loss: 0.3069, G_loss: 27.2182\n",
      "Epoch [10/10], Step [46/125], D_loss: 0.0975, G_loss: 29.0127\n",
      "Epoch [10/10], Step [47/125], D_loss: 0.7889, G_loss: 16.2615\n",
      "Epoch [10/10], Step [48/125], D_loss: 0.2367, G_loss: 26.3883\n",
      "Epoch [10/10], Step [49/125], D_loss: 0.2319, G_loss: 28.4790\n",
      "Epoch [10/10], Step [50/125], D_loss: 0.4807, G_loss: 16.6246\n",
      "Epoch [10/10], Step [51/125], D_loss: 0.3427, G_loss: 20.8723\n",
      "Epoch [10/10], Step [52/125], D_loss: 0.6658, G_loss: 19.9433\n",
      "Epoch [10/10], Step [53/125], D_loss: 0.4380, G_loss: 26.3942\n",
      "Epoch [10/10], Step [54/125], D_loss: 0.2867, G_loss: 31.7224\n",
      "Epoch [10/10], Step [55/125], D_loss: 0.3845, G_loss: 23.5373\n",
      "Epoch [10/10], Step [56/125], D_loss: 0.0482, G_loss: 37.7929\n",
      "Epoch [10/10], Step [57/125], D_loss: 1.0386, G_loss: 25.6805\n",
      "Epoch [10/10], Step [58/125], D_loss: 0.6586, G_loss: 12.1755\n",
      "Epoch [10/10], Step [59/125], D_loss: 0.3442, G_loss: 21.8713\n",
      "Epoch [10/10], Step [60/125], D_loss: 0.3709, G_loss: 19.6858\n",
      "Epoch [10/10], Step [61/125], D_loss: 0.6041, G_loss: 22.4489\n",
      "Epoch [10/10], Step [62/125], D_loss: 0.6635, G_loss: 21.9187\n",
      "Epoch [10/10], Step [63/125], D_loss: 0.2239, G_loss: 33.6534\n",
      "Epoch [10/10], Step [64/125], D_loss: 0.3125, G_loss: 31.9434\n",
      "Epoch [10/10], Step [65/125], D_loss: 0.1656, G_loss: 36.0294\n",
      "Epoch [10/10], Step [66/125], D_loss: 0.5577, G_loss: 17.4284\n",
      "Epoch [10/10], Step [67/125], D_loss: 0.4490, G_loss: 18.9950\n",
      "Epoch [10/10], Step [68/125], D_loss: 0.3831, G_loss: 26.7923\n",
      "Epoch [10/10], Step [69/125], D_loss: 0.8978, G_loss: 20.2127\n",
      "Epoch [10/10], Step [70/125], D_loss: 0.8820, G_loss: 17.7952\n",
      "Epoch [10/10], Step [71/125], D_loss: 0.8492, G_loss: 25.9981\n",
      "Epoch [10/10], Step [72/125], D_loss: 0.0784, G_loss: 40.9784\n",
      "Epoch [10/10], Step [73/125], D_loss: 1.0118, G_loss: 17.4791\n",
      "Epoch [10/10], Step [74/125], D_loss: 0.2400, G_loss: 24.1072\n",
      "Epoch [10/10], Step [75/125], D_loss: 0.2395, G_loss: 17.3694\n",
      "Epoch [10/10], Step [76/125], D_loss: 0.3657, G_loss: 26.5590\n",
      "Epoch [10/10], Step [77/125], D_loss: 0.3527, G_loss: 29.5097\n",
      "Epoch [10/10], Step [78/125], D_loss: 0.4175, G_loss: 21.2918\n",
      "Epoch [10/10], Step [79/125], D_loss: 1.2513, G_loss: 16.9171\n",
      "Epoch [10/10], Step [80/125], D_loss: 0.5039, G_loss: 13.2778\n",
      "Epoch [10/10], Step [81/125], D_loss: 0.7082, G_loss: 13.3729\n",
      "Epoch [10/10], Step [82/125], D_loss: 0.4024, G_loss: 27.9782\n",
      "Epoch [10/10], Step [83/125], D_loss: 0.2426, G_loss: 28.5647\n",
      "Epoch [10/10], Step [84/125], D_loss: 0.3155, G_loss: 39.0190\n",
      "Epoch [10/10], Step [85/125], D_loss: 0.6438, G_loss: 23.2129\n",
      "Epoch [10/10], Step [86/125], D_loss: 1.5882, G_loss: 26.0401\n",
      "Epoch [10/10], Step [87/125], D_loss: 0.4594, G_loss: 26.7464\n",
      "Epoch [10/10], Step [88/125], D_loss: 0.3974, G_loss: 18.8976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Step [89/125], D_loss: 0.1808, G_loss: 30.3701\n",
      "Epoch [10/10], Step [90/125], D_loss: 0.3344, G_loss: 18.4775\n",
      "Epoch [10/10], Step [91/125], D_loss: 0.3086, G_loss: 24.3789\n",
      "Epoch [10/10], Step [92/125], D_loss: 0.3960, G_loss: 25.7738\n",
      "Epoch [10/10], Step [93/125], D_loss: 0.0990, G_loss: 21.0158\n",
      "Epoch [10/10], Step [94/125], D_loss: 0.5896, G_loss: 21.1525\n",
      "Epoch [10/10], Step [95/125], D_loss: 0.8381, G_loss: 30.9998\n",
      "Epoch [10/10], Step [96/125], D_loss: 0.3467, G_loss: 15.7210\n",
      "Epoch [10/10], Step [97/125], D_loss: 0.3405, G_loss: 46.2921\n",
      "Epoch [10/10], Step [98/125], D_loss: 1.7054, G_loss: 20.7359\n",
      "Epoch [10/10], Step [99/125], D_loss: 0.4328, G_loss: 22.9821\n",
      "Epoch [10/10], Step [100/125], D_loss: 0.3469, G_loss: 19.4675\n",
      "Epoch [10/10], Step [101/125], D_loss: 1.1468, G_loss: 11.0787\n",
      "Epoch [10/10], Step [102/125], D_loss: 0.2926, G_loss: 24.7575\n",
      "Epoch [10/10], Step [103/125], D_loss: 0.4013, G_loss: 28.0345\n",
      "Epoch [10/10], Step [104/125], D_loss: 0.2574, G_loss: 20.8618\n",
      "Epoch [10/10], Step [105/125], D_loss: 0.2289, G_loss: 50.3027\n",
      "Epoch [10/10], Step [106/125], D_loss: 0.2499, G_loss: 19.5382\n",
      "Epoch [10/10], Step [107/125], D_loss: 0.3688, G_loss: 30.6840\n",
      "Epoch [10/10], Step [108/125], D_loss: 0.1643, G_loss: 25.3693\n",
      "Epoch [10/10], Step [109/125], D_loss: 0.6043, G_loss: 19.2491\n",
      "Epoch [10/10], Step [110/125], D_loss: 0.3952, G_loss: 24.9401\n",
      "Epoch [10/10], Step [111/125], D_loss: 1.2479, G_loss: 21.8068\n",
      "Epoch [10/10], Step [112/125], D_loss: 1.0921, G_loss: 24.6091\n",
      "Epoch [10/10], Step [113/125], D_loss: 0.2758, G_loss: 28.5410\n",
      "Epoch [10/10], Step [114/125], D_loss: 0.2509, G_loss: 29.9790\n",
      "Epoch [10/10], Step [115/125], D_loss: 0.1618, G_loss: 25.2051\n",
      "Epoch [10/10], Step [116/125], D_loss: 0.4225, G_loss: 17.8876\n",
      "Epoch [10/10], Step [117/125], D_loss: 0.4587, G_loss: 17.8946\n",
      "Epoch [10/10], Step [118/125], D_loss: 0.0957, G_loss: 35.2016\n",
      "Epoch [10/10], Step [119/125], D_loss: 0.5903, G_loss: 28.9889\n",
      "Epoch [10/10], Step [120/125], D_loss: 0.3820, G_loss: 19.5708\n",
      "Epoch [10/10], Step [121/125], D_loss: 0.1681, G_loss: 20.9182\n",
      "Epoch [10/10], Step [122/125], D_loss: 0.1693, G_loss: 22.5150\n",
      "Epoch [10/10], Step [123/125], D_loss: 0.2122, G_loss: 31.5890\n",
      "Epoch [10/10], Step [124/125], D_loss: 0.2805, G_loss: 24.0887\n",
      "Epoch [10/10], Step [125/125], D_loss: 0.6450, G_loss: 31.3762\n"
     ]
    }
   ],
   "source": [
    "# Training GAN\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "num_epochs=10\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_dir=ex_dir+str(epoch)+\"/\"\n",
    "    if not os.path.exists(epoch_dir):\n",
    "        os.mkdir(epoch_dir)\n",
    "\n",
    "\n",
    "    epoch_im_dir=epoch_dir+\"images/\"\n",
    "\n",
    "\n",
    "    epoch_msk_dir=epoch_dir+\"masks/\"\n",
    "\n",
    "\n",
    "    epoch_genmsk_dir=epoch_dir+\"gen_masks/\"\n",
    "    if not os.path.exists(epoch_genmsk_dir):\n",
    "        os.mkdir(epoch_genmsk_dir)\n",
    "\n",
    "\n",
    "    epoch_feat_dir=epoch_dir+\"features/\"\n",
    "    if not os.path.exists(epoch_feat_dir):\n",
    "        os.mkdir(epoch_feat_dir)\n",
    "\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # training\n",
    "    for i, (input, target, im_file_names, msk_file_names) in enumerate(train_loader):\n",
    "\n",
    "        # input & target image data\n",
    "        x_ = Variable(input)#.permute(0,3,1,2)#Variable(input.cuda())\n",
    "        y_ = Variable(target)#.permute(0,3,1,2)#Variable(target.cuda())\n",
    "\n",
    "        x_ = x_.float()#Variable(input.cuda())\n",
    "        y_ = y_.float()#Variable(target.cuda())\n",
    "\n",
    "        # Train discriminator with real data\n",
    "        D_real_decision = D(x_, y_).squeeze()\n",
    "        real_ = Variable(torch.ones(D_real_decision.size())) #Variable(torch.ones(D_real_decision.size()).cuda())\n",
    "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
    "\n",
    "        # Train discriminator with fake data\n",
    "        gen_image, _ = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        fake_ = Variable(torch.zeros(D_fake_decision.size()))#Variable(torch.zeros(D_fake_decision.size()).cuda())\n",
    "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
    "\n",
    "        # Back propagation\n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "#         gen_image, features = G(x_)\n",
    "        gen_image, _ = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        G_fake_loss = BCE_loss(D_fake_decision, real_)\n",
    "\n",
    "        # L1 loss\n",
    "        l1_loss = lamb * L1_loss(gen_image, y_)\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_fake_loss + l1_loss\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "        \n",
    "        #features_proj=F.interpolate(features,scale_factor=(2,2), mode='nearest')\n",
    "\n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "              % (epoch+1, num_epochs, i+1, len(train_loader), D_loss.item(), G_loss.item()))\n",
    "        \n",
    "\n",
    "#         if epoch>=0:\n",
    "#             save_epoch_pairs(x_,\n",
    "#                              y_,\n",
    "#                              gen_image,\n",
    "#                              features_proj,\n",
    "#                              im_file_names,\n",
    "#                              msk_file_names,\n",
    "#                              epoch_im_dir,\n",
    "#                              epoch_msk_dir,\n",
    "#                              epoch_genmsk_dir,\n",
    "#                              epoch_feat_dir)\n",
    "\n",
    "        \n",
    "        if epoch>=4:\n",
    "            save_epoch_pairs(x_,\n",
    "                             y_,\n",
    "                             gen_image,\n",
    "                             im_file_names,\n",
    "                             msk_file_names,\n",
    "                             epoch_im_dir,\n",
    "                             epoch_msk_dir,\n",
    "                             epoch_genmsk_dir,\n",
    "                             epoch_feat_dir)\n",
    "\n",
    "        \n",
    "#         save_epoch_pairs(imgs_batch,\n",
    "#                      masks_batch,\n",
    "#                      gen_masks_batch, \n",
    "#                      im_fn_batch,\n",
    "#                      msk_fn_batch,\n",
    "#                      im_dir_name,\n",
    "#                      msk_dir_name,\n",
    "#                      gen_msk_dir_name):\n",
    "\n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    \n",
    "#     val_input, val_target, val_img_names, val_msk_names = val_loader.__iter__().__next__()\n",
    "\n",
    "#     gen_image = G(Variable(test_input))\n",
    "#     gen_image = gen_image.cpu().data\n",
    "#     plot_test_result(test_input, test_target, gen_image, epoch, save=True, ex_dir=ex_dir)\n",
    "    \n",
    "    save_model_fn=ex_dir+\"Seg_GAN.pth\"\n",
    "        \n",
    "    torch.save({\"g_state_dict\":G.state_dict(),\n",
    "                \"d_state_dict\":D.state_dict(),\n",
    "                \"g_opt_dict\":G_optimizer.state_dict(),\n",
    "                \"d_opt_dict\":G_optimizer.state_dict(),\n",
    "                       },\n",
    "                       save_model_fn\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# save_model_fn=ex_dir+\"Seg_GAN.pth\"\n",
    "\n",
    "# torch.save({\"g_state_dict\":G.state_dict(),\n",
    "#             \"d_state_dict\":D.state_dict(),\n",
    "#             \"g_opt_dict\":G_optimizer.state_dict(),\n",
    "#             \"d_opt_dict\":G_optimizer.state_dict(),\n",
    "#                    },\n",
    "#                    save_model_fn\n",
    "#                   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
