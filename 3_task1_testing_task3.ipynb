{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1:  Poyp Segmentation  (Testing and Interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import image\n",
    "from numpy import savez_compressed\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "from gan import Generator, Discriminator\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf= 64\n",
    "ndf= 64\n",
    "lrG=0.002 #0.0002\n",
    "lrD=0.002\n",
    "lamb=100\n",
    "beta1=0.5\n",
    "beta2=0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(3, ngf, 3)\n",
    "D = Discriminator(6, ndf, 1)\n",
    "G.normal_weight_init(mean=0.0, std=0.02)\n",
    "D.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "print(D)\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mask_batch(gen_masks_batch, \n",
    "                     msk_fn_batch,\n",
    "                     gen_msk_dir_name,\n",
    "                    original_size):\n",
    "    \n",
    "   \n",
    "    for i in range(len(gen_masks_batch)):\n",
    "        \n",
    "        msk_fn=msk_fn_batch\n",
    "        \n",
    "        gen_mask=gen_masks_batch[i].detach().numpy()#.permute(1,2,0).detach().numpy().astype(np.uint8)\n",
    "\n",
    "       \n",
    "        save_genmsk_fn = gen_msk_dir_name + msk_fn\n",
    "#         print(save_genmsk_fn)\n",
    "        gen_mask_ = (((gen_mask -gen_mask.min()) * 255) / (gen_mask.max() - gen_mask.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        msk_gen = Image.fromarray(gen_mask_)\n",
    "        msk_gen_original_size= msk_gen.resize((original_size[0],original_size[1]), Image.ANTIALIAS)\n",
    "        msk_gen_original_size.save(save_genmsk_fn)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std =[0.5, 0.5, 0.5],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading test data\n",
    "\n",
    "test_data_dir = \"./data/task1/test_data/\"\n",
    "test_results_dir = \"./data/task1/test_data/masks/\"\n",
    "\n",
    "test_img_files = glob.glob(test_data_dir+\"images/*.jpg\")\n",
    "#msk_files=glob.glob(data_dir+\"masks/*.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading trained model, we only need the generator\n",
    "\n",
    "PATH=\"saved_models/Seg_GAN_task1.pth\"\n",
    "model_ckp = torch.load(PATH)\n",
    "G.load_state_dict(model_ckp['g_state_dict'],strict=True)\n",
    "#D.load_state_dict(checkpoint['d_state_dict'],strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate  masks\n",
    "\n",
    "target_w=512\n",
    "target_h=512\n",
    "\n",
    "for  i,f in enumerate(test_img_files):\n",
    "    img = Image.open(f)\n",
    "# 1. resize images to the inputs size\n",
    "    image_size=list(img.size) # we need to keep the original image size\n",
    "    img_resized = img.resize((target_w,target_h), Image.ANTIALIAS)\n",
    "    img_tr=transform(img_resized.copy()).numpy()\n",
    "    test_image= torch.tensor(img_tr)\n",
    "    test_image = torch.unsqueeze(test_image, 0)\n",
    "# 2. generate  512 * 512 mask\n",
    "    \n",
    "    gen_msak_sample,_= G(test_image)\n",
    "    \n",
    "    \n",
    "    im_file_name=os.path.basename(f)\n",
    "# save the generated mask, we also pass the original size to get the original shape    \n",
    "    save_mask_batch(gen_msak_sample,im_file_name,test_results_dir,image_size)\n",
    "\n",
    "   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Model Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reference the paper for more details about layer relevance propagation, the approach we used to interperate the models with their implementation guides:\n",
    "\n",
    "* Montavon, Grégoire, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert Müller. \"Layer-wise relevance propagation: an overview.\" Explainable AI: interpreting, explaining and visualizing deep learning (2019): 193-209."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_back(im):\n",
    "    im_ = (((im -im.min()) * 255) / (im.max() - im.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "    return(im_)\n",
    "\n",
    "def new_layer(layer, g):\n",
    "    #copy a layer and pass its parameters through the function g.\"\n",
    "    layer = copy.deepcopy(layer)\n",
    "    try: layer.weight = torch.nn.Parameter(g(layer.weight))\n",
    "    except AttributeError: pass\n",
    "    \n",
    "    try: layer.bias = torch.nn.Parameter(g(layer.bias))\n",
    "    except AttributeError: pass\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_w=512\n",
    "target_h=512\n",
    "\n",
    "\n",
    "for  i,f in enumerate(test_img_files):\n",
    "    img = Image.open(f)\n",
    "    image_size=list(img.size)\n",
    "    img_resized = img.resize((target_w,target_h), Image.ANTIALIAS)\n",
    "    img_tr=transform(img_resized.copy()).numpy()\n",
    "    test_image= torch.tensor(img_tr)\n",
    "    test_image = torch.unsqueeze(test_image, 0)\n",
    "    \n",
    "    gen_msak_sample,_= G(test_image)\n",
    "    \n",
    "    \n",
    "    im_file_name=os.path.basename(f)\n",
    "    \n",
    "\n",
    "    layers=list(G._modules) # get generator trained layers (parameters)\n",
    "\n",
    "    \n",
    "    n_layers = len(layers)\n",
    "    \n",
    "# set a place holder for the activations to calculates next\n",
    "    activations = [test_image] + [None] * n_layers # list of activations\n",
    "\n",
    "\n",
    "    # for each layer  in the conv part of the generator\n",
    "    for layer in range(9):\n",
    "        \n",
    "        #get the activations\n",
    "        activation = G._modules[layers[layer]].forward(activations[layer])\n",
    "        \n",
    "        #pass it for the next layer\n",
    "        activations[layer+1] = activation\n",
    "    \n",
    "    # for each layer  in the deconv part of the generator\n",
    "    for layer in range(9,len(layers)):\n",
    "        \n",
    "        #convatenate the corresponding conv layer based on the skip connections\n",
    "        cat_in=torch.cat([activations[layer], activations[16-layer]], 1) \n",
    "        activation = G._modules[layers[layer]].forward(cat_in)\n",
    "        activations[layer+1] = activation\n",
    "\n",
    "            \n",
    "    # calculate the output as we use tanh in the model\n",
    "    output_activation = torch.nn.Tanh()(activations[-1]).detach().cpu().numpy()#[0,:,:,:]\n",
    "\n",
    "    # convert the RGB mask output to grey scale\n",
    "    outpu_activation=np.asarray(Image.fromarray(output_activation[0,:,:,:].transpose(1,2,0).astype('uint8'),\n",
    "                                               'RGB').convert('L'))\n",
    "    \n",
    "    # set predicted mask pixels to 1\n",
    "    output_activation[output_activation>=0]= 1\n",
    "    # set predicted normal pixels to 0\n",
    "    output_activation[output_activation!=1]= 0\n",
    "    \n",
    "    # save the output activation to the last element of the activations placeholder\n",
    "    activations[-1]=torch.FloatTensor(output_activation) \n",
    "\n",
    "    # Backpropagate relevvance scores\n",
    "    relevances = [None] * n_layers + [activations[-1]]\n",
    "    \n",
    "    # go over the layers in reverse order\n",
    "    \n",
    "    # define the rho function based on the model of haveing <17 layers (refer to the above paper)\n",
    "    rho = lambda p: p + 0.25*p.clamp(min=0); incr = lambda z: z+1e-9\n",
    "\n",
    "    for layer in range(0, n_layers)[::-1]:\n",
    "        # inreverse order we start by the deconv section\n",
    "        if layer>=9:\n",
    "\n",
    "            activations[layer] = activations[layer].data.requires_grad_(True)\n",
    "\n",
    "            cat_in2=torch.cat([activations[layer], activations[16-layer]], 1)\n",
    "            z = incr(new_layer(G._modules[layers[layer]],rho).forward(cat_in2))\n",
    "            \n",
    "            # devide relevance / z element wise\n",
    "            s = (relevances[layer+1]/z).data    \n",
    "            \n",
    "            # Calculate the gradient and multiply it by the activation,\n",
    "            # we should retain the graph as we will pass through it multiple times\n",
    "            (z * s).sum().backward(retain_graph=True); \n",
    "            c = activations[layer].grad       \n",
    "            relevances[layer] = (activations[layer]*c).data\n",
    "        # we do the same for the conv section\n",
    "        else:\n",
    "            activations[layer] = activations[layer].data.requires_grad_(True)\n",
    "\n",
    "            \n",
    "            cat_in2=torch.cat([activations[layer], activations[layer]], 1)\n",
    "            z = incr(new_layer(G._modules[layers[layer]],rho).forward(cat_in2[:,:int(cat_in2.shape[1]/2),:,:]))\n",
    "\n",
    "            s = (relevances[layer+1]/z).data                                     \n",
    "            (z * s).sum().backward(retain_graph=True); \n",
    "            c = activations[layer].grad       \n",
    "            relevances[layer] = (activations[layer]*c).data                          \n",
    "\n",
    "    \n",
    "    \n",
    "    # plotting and saving relevance scores\n",
    "    img_to_plot=transform_back(test_image[0,:].detach().numpy())\n",
    "    gen_msak_sample_to_plot=transform_back(gen_msak_sample[0,:].detach().numpy())\n",
    "    fig, axs= plt.subplots(1,3,figsize=(30,20))\n",
    "\n",
    "    axs[0].imshow(img_to_plot)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Image\", size=24)\n",
    "\n",
    "    image_rlp=relevances[0][0,:,:,:].permute(1,2,0).detach().numpy()\n",
    "    image_rlp = np.interp(image_rlp, (image_rlp.min(),\n",
    "                                                    image_rlp.max()), \n",
    "                                                    (0, 1))\n",
    "    axs[1].imshow(image_rlp[:,:,0], cmap=\"seismic\")\n",
    "\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[1].set_title(\"Input Pixels Relevance \", size=24)\n",
    "\n",
    "    axs[2].imshow(gen_msak_sample_to_plot)\n",
    "    axs[2].axis('off')\n",
    "    axs[2].set_title(\"Predicted mask\", size=24)\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    fig.savefig('rlp_results_task1/input_pix/'+im_file_name, dpi=200)\n",
    "\n",
    "    \n",
    "    selected_layers=[0,4,6,12,13,15]\n",
    "\n",
    "    fig, axs= plt.subplots(1,len(selected_layers)+1,figsize=(30,20))\n",
    "\n",
    "    axs[0].imshow(img_to_plot)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title(\"Image\", size=24)\n",
    "    for l,layer in enumerate(selected_layers):\n",
    "\n",
    "        image_rlp=relevances[layer][0,:,:,:].permute(1,2,0).detach().numpy()\n",
    "        image_rlp = np.interp(image_rlp, (image_rlp.min(),\n",
    "                                                        image_rlp.max()), \n",
    "                                                        (0, 1))\n",
    "        axs[l+1].imshow(image_rlp[:,:,0], cmap=\"seismic\")\n",
    "\n",
    "\n",
    "        axs[l+1].axis('off')\n",
    "\n",
    "\n",
    "        axs[l+1].set_title(\"layer \"+str(layer), size=24)\n",
    "\n",
    "    axs[l+1].imshow(gen_msak_sample_to_plot)\n",
    "    axs[l+1].axis('off')\n",
    "    axs[l+1].set_title(\"predicted mask\", size=24)\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    fig.savefig('rlp_results_task1/all_layers/'+im_file_name, dpi=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
