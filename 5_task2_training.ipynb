{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2:  Instrument Segmentation Task (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from matplotlib import image\n",
    "from numpy import savez_compressed\n",
    "from torchvision import transforms\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "import torch\n",
    "from gan import Generator, Discriminator  # models are in gan.py file\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_im_msk(img_files,msk_files):\n",
    "    \n",
    "    pure_img_names=[]\n",
    "    pure_msk_names=[]\n",
    "\n",
    "    for im in img_files:\n",
    "        pure_img_names+= [os.path.basename(im)]\n",
    "    for msk in msk_files:\n",
    "        pure_msk_names+= [os.path.basename(msk)]\n",
    "    \n",
    "    return(pure_img_names,pure_msk_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading data\n",
    "data_dir = \"./data/task2/resized/\"\n",
    "img_files = glob.glob(data_dir+\"images/*.jpg\")\n",
    "msk_files=glob.glob(data_dir+\"masks/*.png\")\n",
    "pure_img_names,pure_msk_names=match_im_msk(img_files,msk_files)\n",
    "pure_img_names==pure_msk_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std =[0.5, 0.5, 0.5],\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loaded image ./data/task2/resized/images\\ckcu8ty6z00003b5yzfaezbs5.jpg (3, 512, 512)\n",
      "> loaded image ./data/task2/resized/images\\ckd4gwbyy000r3b5ydyl6gfto.jpg (3, 512, 512)\n",
      "> loaded mask ./data/task2/resized/masks\\ckcu8ty6z00003b5yzfaezbs5.png (3, 512, 512)\n",
      "> loaded mask ./data/task2/resized/masks\\ckd4gwbyy000r3b5ydyl6gfto.png (3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "loaded_images = list()\n",
    "loaded_masks = list()\n",
    "\n",
    "for i,filename in enumerate(img_files):\n",
    "    # load image\n",
    "    img_data = image.imread(filename)\n",
    "    # store loaded image\n",
    "    img_data_tr=transform(img_data.copy()).numpy()\n",
    "    loaded_images.append(img_data_tr)\n",
    "    if i%500 ==0:\n",
    "        print('> loaded image %s %s' % (filename, img_data_tr.shape))\n",
    "for i,filename in enumerate(msk_files):\n",
    "    # load image\n",
    "    msk_data = image.imread(filename)\n",
    "    msk_data_tr=transform(msk_data.copy()).numpy()\n",
    "    # store loaded image\n",
    "    loaded_masks.append(msk_data_tr)\n",
    "    if i%500 ==0:\n",
    "        print('> loaded mask %s %s' % (filename, msk_data_tr.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can show some samples\n",
    "# s=np.random.randint(0,900)\n",
    "# print(s)\n",
    "# for j in range(s,s+1):\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     ax1.imshow(loaded_images[j])\n",
    "#     ax2.imshow(loaded_masks[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a SegGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngf= 64   # generator number of filters\n",
    "ndf= 64   # discriminator number of filters\n",
    "lrG=0.002 #0.0002\n",
    "lrD=0.002\n",
    "lamb=100  # loss weighting parameters\n",
    "beta1=0.5 \n",
    "beta2=0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples 590\n",
      "image dim (3, 512, 512)\n",
      "mask  dim (3, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples\",len(loaded_images))\n",
    "print(\"image dim\", loaded_images[0].shape)\n",
    "print(\"mask  dim\",loaded_masks[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): ConvBlock(\n",
      "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): ConvBlock(\n",
      "    (conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "G = Generator(3, ngf, 3)\n",
    "D = Discriminator(6, ndf, 1)\n",
    "G.normal_weight_init(mean=0.0, std=0.02)\n",
    "D.normal_weight_init(mean=0.0, std=0.02)\n",
    "\n",
    "print(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (conv1): ConvBlock(\n",
      "    (conv): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): ConvBlock(\n",
      "    (conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): ConvBlock(\n",
      "    (conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): ConvBlock(\n",
      "    (conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv6): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv7): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv8): ConvBlock(\n",
      "    (conv): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (lrelu): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (deconv1): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv2): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv3): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv4): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv5): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv6): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv7): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (deconv8): DeconvBlock(\n",
      "    (deconv): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('id_log_task2.txt'):\n",
    "    with open('id_log_task2.txt','w') as f:\n",
    "        f.write('0')\n",
    "with open('id_log_task2.txt','r') as f:\n",
    "    run_id = int(f.read())\n",
    "    run_id+=1 \n",
    "\n",
    "with open('id_log_task2.txt','w') as f:\n",
    "    f.write(str(run_id))\n",
    "\n",
    "model_dir = 'GAN_model_task2/'\n",
    "save_dir='GAN_results_task2/'\n",
    "ex_dir=save_dir+str(run_id)+\"/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "if not os.path.exists(ex_dir):\n",
    "    os.makedirs(ex_dir)\n",
    "    \n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "BCE_loss = torch.nn.BCELoss()#.cuda()\n",
    "L1_loss = torch.nn.L1Loss()#.cuda()\n",
    "\n",
    "# I used my laptop, cuda is an option\n",
    "#G.cuda()\n",
    "#D.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: 590 590\n"
     ]
    }
   ],
   "source": [
    "# this is the last training trial, I used the whole dataset, but I sued 900:100 training:validation for parameters tuning\n",
    "\n",
    "split_point=len(loaded_images)\n",
    "X_train=np.array(loaded_images[:split_point])\n",
    "Y_train=np.array(loaded_masks[:split_point])\n",
    "im_train_names=np.array(pure_img_names[:split_point])\n",
    "msk_train_names=np.array(pure_msk_names[:split_point])\n",
    "\n",
    "# X_val=np.array(loaded_images[split_point:])\n",
    "# Y_val=np.array(loaded_masks[split_point:])\n",
    "# im_val_names=np.array(pure_img_names[split_point:])\n",
    "# msk_val_names=np.array(pure_msk_names[split_point:])\n",
    "\n",
    "print(\"train set:\", len(Y_train),len(X_train))\n",
    "# print(\"val   set:\", len(Y_val),len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seg(Dataset):\n",
    "    def __init__(self,X,Y,im_names,mask_names):\n",
    "\n",
    "        self.len=X.shape[0]\n",
    "        self.xdata=torch.from_numpy(X)\n",
    "        self.ydata=torch.from_numpy(Y)\n",
    "        \n",
    "        self.xnames=im_names\n",
    "        self.ynames=mask_names\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return (self.xdata[index],self.ydata[index],self.xnames[index],self.ynames[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to save sample during training\n",
    "# I created the function to save features from the middle of the netork\n",
    "# the purpouse was to use them in creating graph later, however I stick to the GANs in this phase\n",
    "\n",
    "def save_epoch_pairs(imgs_batch,\n",
    "                     masks_batch,\n",
    "                     gen_masks_batch, \n",
    "                     im_fn_batch,\n",
    "                     msk_fn_batch,\n",
    "                     im_dir_name,\n",
    "                     msk_dir_name,\n",
    "                     gen_msk_dir_name,\n",
    "                     feat_dir_name):\n",
    "    \n",
    "   \n",
    "    for i in range(len(imgs_batch)):\n",
    "        \n",
    "        im_fn =im_fn_batch[i]\n",
    "        msk_fn=msk_fn_batch[i]\n",
    "        \n",
    "\n",
    "        \n",
    "        gen_mask=gen_masks_batch[i].detach().numpy()#.permute(1,2,0).detach().numpy().astype(np.uint8)\n",
    "\n",
    "        save_genmsk_fn = gen_msk_dir_name + msk_fn\n",
    "        gen_mask_ = (((gen_mask -gen_mask.min()) * 255) / (gen_mask.max() - gen_mask.min())).transpose(1, 2, 0).astype(np.uint8)\n",
    "        msk_gen = Image.fromarray(gen_mask_)\n",
    "        msk_gen.save(save_genmsk_fn)\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "        #save_msk_fn = msk_dir_name + msk_fn\n",
    "        #msk_real = Image.fromarray(target_msk)\n",
    "       # msk_real.save(save_msk_fn)\n",
    "        #plt.show()\n",
    "        #plt.close()\n",
    "        \n",
    "        \n",
    "        # Saving features....\n",
    "#         save_feat_fn = feat_dir_name + msk_fn[:-3]+\"npz\"\n",
    "#         feat = features_batch[i].detach().numpy()\n",
    "#         savez_compressed(save_feat_fn, feat)\n",
    "        #torch.save(feat, save_feat_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "\n",
    "train_seg=Seg(X_train,Y_train,im_train_names,msk_train_names)\n",
    "train_loader=DataLoader(dataset=train_seg,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "# val_seg=Seg(X_val,Y_val,im_val_names,msk_val_names)\n",
    "# val_loader=DataLoader(dataset=val_seg,batch_size=2,shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (image_pix, mask_pix,im_nn,msk_nn) in enumerate(train_loader):\n",
    "#     fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "#     #ax1.imshow(image_pix[0])\n",
    "#     #ax2.imshow(mask_pix[0])\n",
    "#     print(im_nn[0])\n",
    "#     print(msk_nn[0])\n",
    "#     print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optimizers\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=lrG, betas=(beta1, beta2))\n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=lrD, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/12], Step [1/74], D_loss: 0.8164, G_loss: 104.6766\n",
      "Epoch [1/12], Step [2/74], D_loss: 3.6631, G_loss: 87.7071\n",
      "Epoch [1/12], Step [3/74], D_loss: 7.4452, G_loss: 66.7118\n",
      "Epoch [1/12], Step [4/74], D_loss: 2.5863, G_loss: 50.7814\n",
      "Epoch [1/12], Step [5/74], D_loss: 1.7079, G_loss: 36.9020\n",
      "Epoch [1/12], Step [6/74], D_loss: 1.1948, G_loss: 25.1190\n",
      "Epoch [1/12], Step [7/74], D_loss: 1.2417, G_loss: 23.3659\n",
      "Epoch [1/12], Step [8/74], D_loss: 0.9004, G_loss: 34.9979\n",
      "Epoch [1/12], Step [9/74], D_loss: 0.6892, G_loss: 16.1769\n",
      "Epoch [1/12], Step [10/74], D_loss: 0.6801, G_loss: 13.4316\n",
      "Epoch [1/12], Step [11/74], D_loss: 0.6520, G_loss: 17.6263\n",
      "Epoch [1/12], Step [12/74], D_loss: 0.6990, G_loss: 22.5414\n",
      "Epoch [1/12], Step [13/74], D_loss: 0.9663, G_loss: 18.5735\n",
      "Epoch [1/12], Step [14/74], D_loss: 0.7712, G_loss: 23.5054\n",
      "Epoch [1/12], Step [15/74], D_loss: 0.7646, G_loss: 21.2442\n",
      "Epoch [1/12], Step [16/74], D_loss: 0.6975, G_loss: 10.5546\n",
      "Epoch [1/12], Step [17/74], D_loss: 0.5772, G_loss: 17.2439\n",
      "Epoch [1/12], Step [18/74], D_loss: 0.5412, G_loss: 18.3085\n",
      "Epoch [1/12], Step [19/74], D_loss: 0.4974, G_loss: 17.1514\n",
      "Epoch [1/12], Step [20/74], D_loss: 0.5950, G_loss: 19.6951\n",
      "Epoch [1/12], Step [21/74], D_loss: 0.8887, G_loss: 24.7190\n",
      "Epoch [1/12], Step [22/74], D_loss: 0.8663, G_loss: 18.6977\n",
      "Epoch [1/12], Step [23/74], D_loss: 0.9358, G_loss: 16.5923\n",
      "Epoch [1/12], Step [24/74], D_loss: 0.7365, G_loss: 21.8343\n",
      "Epoch [1/12], Step [25/74], D_loss: 0.6470, G_loss: 25.5756\n",
      "Epoch [1/12], Step [26/74], D_loss: 0.5089, G_loss: 20.1331\n",
      "Epoch [1/12], Step [27/74], D_loss: 0.5223, G_loss: 23.9557\n",
      "Epoch [1/12], Step [28/74], D_loss: 0.4233, G_loss: 28.6758\n",
      "Epoch [1/12], Step [29/74], D_loss: 0.3976, G_loss: 17.0105\n",
      "Epoch [1/12], Step [30/74], D_loss: 0.4470, G_loss: 23.0980\n",
      "Epoch [1/12], Step [31/74], D_loss: 0.6947, G_loss: 17.2672\n",
      "Epoch [1/12], Step [32/74], D_loss: 0.4910, G_loss: 24.9640\n",
      "Epoch [1/12], Step [33/74], D_loss: 0.4401, G_loss: 22.8462\n",
      "Epoch [1/12], Step [34/74], D_loss: 0.5870, G_loss: 18.4696\n",
      "Epoch [1/12], Step [35/74], D_loss: 0.3847, G_loss: 27.8597\n",
      "Epoch [1/12], Step [36/74], D_loss: 0.3215, G_loss: 22.5967\n",
      "Epoch [1/12], Step [37/74], D_loss: 0.4108, G_loss: 20.5038\n",
      "Epoch [1/12], Step [38/74], D_loss: 0.4389, G_loss: 26.5024\n",
      "Epoch [1/12], Step [39/74], D_loss: 0.9334, G_loss: 33.1176\n",
      "Epoch [1/12], Step [40/74], D_loss: 2.0912, G_loss: 16.6923\n",
      "Epoch [1/12], Step [41/74], D_loss: 0.6567, G_loss: 19.3123\n",
      "Epoch [1/12], Step [42/74], D_loss: 0.8255, G_loss: 16.5362\n",
      "Epoch [1/12], Step [43/74], D_loss: 0.3172, G_loss: 20.1254\n",
      "Epoch [1/12], Step [44/74], D_loss: 0.3262, G_loss: 20.0446\n",
      "Epoch [1/12], Step [45/74], D_loss: 0.2525, G_loss: 31.1231\n",
      "Epoch [1/12], Step [46/74], D_loss: 0.2382, G_loss: 31.2025\n",
      "Epoch [1/12], Step [47/74], D_loss: 0.2072, G_loss: 25.2799\n",
      "Epoch [1/12], Step [48/74], D_loss: 0.3766, G_loss: 20.9970\n",
      "Epoch [1/12], Step [49/74], D_loss: 0.4558, G_loss: 21.1582\n",
      "Epoch [1/12], Step [50/74], D_loss: 0.1757, G_loss: 24.7890\n",
      "Epoch [1/12], Step [51/74], D_loss: 0.2820, G_loss: 31.6247\n",
      "Epoch [1/12], Step [52/74], D_loss: 0.5804, G_loss: 21.6509\n",
      "Epoch [1/12], Step [53/74], D_loss: 0.9364, G_loss: 26.4325\n",
      "Epoch [1/12], Step [54/74], D_loss: 0.5403, G_loss: 23.1552\n",
      "Epoch [1/12], Step [55/74], D_loss: 0.7832, G_loss: 17.0716\n",
      "Epoch [1/12], Step [56/74], D_loss: 1.0680, G_loss: 26.8489\n",
      "Epoch [1/12], Step [57/74], D_loss: 0.6459, G_loss: 19.9068\n",
      "Epoch [1/12], Step [58/74], D_loss: 0.5735, G_loss: 25.5532\n",
      "Epoch [1/12], Step [59/74], D_loss: 0.3752, G_loss: 17.9588\n",
      "Epoch [1/12], Step [60/74], D_loss: 0.2428, G_loss: 17.8389\n",
      "Epoch [1/12], Step [61/74], D_loss: 0.2943, G_loss: 24.3544\n",
      "Epoch [1/12], Step [62/74], D_loss: 0.2155, G_loss: 19.3770\n",
      "Epoch [1/12], Step [63/74], D_loss: 0.1996, G_loss: 22.7154\n",
      "Epoch [1/12], Step [64/74], D_loss: 0.1393, G_loss: 16.8415\n",
      "Epoch [1/12], Step [65/74], D_loss: 0.1566, G_loss: 29.3834\n",
      "Epoch [1/12], Step [66/74], D_loss: 0.1053, G_loss: 20.2296\n",
      "Epoch [1/12], Step [67/74], D_loss: 0.0939, G_loss: 19.1653\n",
      "Epoch [1/12], Step [68/74], D_loss: 0.0682, G_loss: 19.9597\n",
      "Epoch [1/12], Step [69/74], D_loss: 0.0691, G_loss: 28.5163\n",
      "Epoch [1/12], Step [70/74], D_loss: 0.0800, G_loss: 25.5647\n",
      "Epoch [1/12], Step [71/74], D_loss: 0.0664, G_loss: 27.4172\n",
      "Epoch [1/12], Step [72/74], D_loss: 0.0466, G_loss: 21.0718\n",
      "Epoch [1/12], Step [73/74], D_loss: 0.0496, G_loss: 27.9484\n",
      "Epoch [1/12], Step [74/74], D_loss: 0.0358, G_loss: 26.6021\n",
      "Epoch [2/12], Step [1/74], D_loss: 0.0359, G_loss: 25.3726\n",
      "Epoch [2/12], Step [2/74], D_loss: 0.0346, G_loss: 30.7171\n",
      "Epoch [2/12], Step [3/74], D_loss: 0.0315, G_loss: 19.4415\n",
      "Epoch [2/12], Step [4/74], D_loss: 0.0283, G_loss: 30.3487\n",
      "Epoch [2/12], Step [5/74], D_loss: 0.0645, G_loss: 25.7207\n",
      "Epoch [2/12], Step [6/74], D_loss: 0.0517, G_loss: 25.6458\n",
      "Epoch [2/12], Step [7/74], D_loss: 0.1322, G_loss: 20.6671\n",
      "Epoch [2/12], Step [8/74], D_loss: 0.2871, G_loss: 22.2426\n",
      "Epoch [2/12], Step [9/74], D_loss: 0.6656, G_loss: 25.9860\n",
      "Epoch [2/12], Step [10/74], D_loss: 1.0200, G_loss: 20.3299\n",
      "Epoch [2/12], Step [11/74], D_loss: 1.4493, G_loss: 26.0890\n",
      "Epoch [2/12], Step [12/74], D_loss: 0.2980, G_loss: 26.7131\n",
      "Epoch [2/12], Step [13/74], D_loss: 0.8656, G_loss: 25.3823\n",
      "Epoch [2/12], Step [14/74], D_loss: 0.2805, G_loss: 24.5952\n",
      "Epoch [2/12], Step [15/74], D_loss: 0.1763, G_loss: 20.2726\n",
      "Epoch [2/12], Step [16/74], D_loss: 0.0764, G_loss: 23.0683\n",
      "Epoch [2/12], Step [17/74], D_loss: 0.0737, G_loss: 24.7682\n",
      "Epoch [2/12], Step [18/74], D_loss: 0.0769, G_loss: 23.6260\n",
      "Epoch [2/12], Step [19/74], D_loss: 0.0580, G_loss: 24.7844\n",
      "Epoch [2/12], Step [20/74], D_loss: 0.0981, G_loss: 19.9935\n",
      "Epoch [2/12], Step [21/74], D_loss: 0.0525, G_loss: 27.3077\n",
      "Epoch [2/12], Step [22/74], D_loss: 0.0379, G_loss: 23.4532\n",
      "Epoch [2/12], Step [23/74], D_loss: 0.0441, G_loss: 17.5899\n",
      "Epoch [2/12], Step [24/74], D_loss: 0.0441, G_loss: 21.9030\n",
      "Epoch [2/12], Step [25/74], D_loss: 0.0278, G_loss: 22.8690\n",
      "Epoch [2/12], Step [26/74], D_loss: 0.0252, G_loss: 24.1786\n",
      "Epoch [2/12], Step [27/74], D_loss: 0.0322, G_loss: 20.8956\n",
      "Epoch [2/12], Step [28/74], D_loss: 0.0273, G_loss: 29.1999\n",
      "Epoch [2/12], Step [29/74], D_loss: 0.0196, G_loss: 25.8727\n",
      "Epoch [2/12], Step [30/74], D_loss: 0.0476, G_loss: 30.7548\n",
      "Epoch [2/12], Step [31/74], D_loss: 0.0211, G_loss: 31.4122\n",
      "Epoch [2/12], Step [32/74], D_loss: 0.0239, G_loss: 18.9006\n",
      "Epoch [2/12], Step [33/74], D_loss: 0.0383, G_loss: 20.1973\n",
      "Epoch [2/12], Step [34/74], D_loss: 0.0210, G_loss: 15.4679\n",
      "Epoch [2/12], Step [35/74], D_loss: 0.0189, G_loss: 33.0899\n",
      "Epoch [2/12], Step [36/74], D_loss: 0.0154, G_loss: 18.2371\n",
      "Epoch [2/12], Step [37/74], D_loss: 0.0146, G_loss: 24.5031\n",
      "Epoch [2/12], Step [38/74], D_loss: 0.0185, G_loss: 17.4165\n",
      "Epoch [2/12], Step [39/74], D_loss: 0.0169, G_loss: 26.3168\n",
      "Epoch [2/12], Step [40/74], D_loss: 0.0114, G_loss: 18.5204\n",
      "Epoch [2/12], Step [41/74], D_loss: 0.0161, G_loss: 22.1769\n",
      "Epoch [2/12], Step [42/74], D_loss: 0.0206, G_loss: 35.8572\n",
      "Epoch [2/12], Step [43/74], D_loss: 0.0157, G_loss: 27.0532\n",
      "Epoch [2/12], Step [44/74], D_loss: 0.0198, G_loss: 27.1418\n",
      "Epoch [2/12], Step [45/74], D_loss: 0.0147, G_loss: 17.4498\n",
      "Epoch [2/12], Step [46/74], D_loss: 0.0112, G_loss: 24.2004\n",
      "Epoch [2/12], Step [47/74], D_loss: 0.0273, G_loss: 29.7972\n",
      "Epoch [2/12], Step [48/74], D_loss: 0.0563, G_loss: 36.4265\n",
      "Epoch [2/12], Step [49/74], D_loss: 0.0263, G_loss: 25.7606\n",
      "Epoch [2/12], Step [50/74], D_loss: 0.0199, G_loss: 31.9899\n",
      "Epoch [2/12], Step [51/74], D_loss: 0.0436, G_loss: 20.6667\n",
      "Epoch [2/12], Step [52/74], D_loss: 0.0447, G_loss: 19.0871\n",
      "Epoch [2/12], Step [53/74], D_loss: 0.0408, G_loss: 17.1349\n",
      "Epoch [2/12], Step [54/74], D_loss: 0.0320, G_loss: 23.0671\n",
      "Epoch [2/12], Step [55/74], D_loss: 0.0627, G_loss: 26.7780\n",
      "Epoch [2/12], Step [56/74], D_loss: 0.1280, G_loss: 23.0584\n",
      "Epoch [2/12], Step [57/74], D_loss: 0.0610, G_loss: 28.2181\n",
      "Epoch [2/12], Step [58/74], D_loss: 0.2689, G_loss: 25.1409\n",
      "Epoch [2/12], Step [59/74], D_loss: 0.5484, G_loss: 38.7826\n",
      "Epoch [2/12], Step [60/74], D_loss: 2.1764, G_loss: 23.6791\n",
      "Epoch [2/12], Step [61/74], D_loss: 1.1606, G_loss: 16.8425\n",
      "Epoch [2/12], Step [62/74], D_loss: 0.3752, G_loss: 22.7631\n",
      "Epoch [2/12], Step [63/74], D_loss: 1.1530, G_loss: 27.1064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/12], Step [64/74], D_loss: 3.6273, G_loss: 22.4163\n",
      "Epoch [2/12], Step [65/74], D_loss: 1.4541, G_loss: 19.5868\n",
      "Epoch [2/12], Step [66/74], D_loss: 0.5142, G_loss: 22.5306\n",
      "Epoch [2/12], Step [67/74], D_loss: 0.6417, G_loss: 22.3412\n",
      "Epoch [2/12], Step [68/74], D_loss: 0.3395, G_loss: 23.6242\n",
      "Epoch [2/12], Step [69/74], D_loss: 0.0951, G_loss: 22.2257\n",
      "Epoch [2/12], Step [70/74], D_loss: 0.0840, G_loss: 19.6008\n",
      "Epoch [2/12], Step [71/74], D_loss: 0.0491, G_loss: 22.9954\n",
      "Epoch [2/12], Step [72/74], D_loss: 0.0509, G_loss: 16.1982\n",
      "Epoch [2/12], Step [73/74], D_loss: 0.0444, G_loss: 22.6112\n",
      "Epoch [2/12], Step [74/74], D_loss: 0.0821, G_loss: 21.9692\n",
      "Epoch [3/12], Step [1/74], D_loss: 0.0976, G_loss: 27.1448\n",
      "Epoch [3/12], Step [2/74], D_loss: 0.0728, G_loss: 22.9230\n",
      "Epoch [3/12], Step [3/74], D_loss: 0.0777, G_loss: 26.4270\n",
      "Epoch [3/12], Step [4/74], D_loss: 0.0539, G_loss: 19.5949\n",
      "Epoch [3/12], Step [5/74], D_loss: 0.0335, G_loss: 20.6488\n",
      "Epoch [3/12], Step [6/74], D_loss: 0.0337, G_loss: 18.9890\n",
      "Epoch [3/12], Step [7/74], D_loss: 0.0711, G_loss: 22.5060\n",
      "Epoch [3/12], Step [8/74], D_loss: 0.0349, G_loss: 17.1865\n",
      "Epoch [3/12], Step [9/74], D_loss: 0.0261, G_loss: 18.8387\n",
      "Epoch [3/12], Step [10/74], D_loss: 0.0520, G_loss: 19.3446\n",
      "Epoch [3/12], Step [11/74], D_loss: 0.0205, G_loss: 21.4440\n",
      "Epoch [3/12], Step [12/74], D_loss: 0.3368, G_loss: 19.1476\n",
      "Epoch [3/12], Step [13/74], D_loss: 3.3758, G_loss: 21.4673\n",
      "Epoch [3/12], Step [14/74], D_loss: 3.8083, G_loss: 20.4927\n",
      "Epoch [3/12], Step [15/74], D_loss: 2.1249, G_loss: 14.5549\n",
      "Epoch [3/12], Step [16/74], D_loss: 1.4828, G_loss: 21.3749\n",
      "Epoch [3/12], Step [17/74], D_loss: 3.2149, G_loss: 24.4288\n",
      "Epoch [3/12], Step [18/74], D_loss: 1.3654, G_loss: 23.2319\n",
      "Epoch [3/12], Step [19/74], D_loss: 1.4001, G_loss: 18.0792\n",
      "Epoch [3/12], Step [20/74], D_loss: 2.1429, G_loss: 14.5774\n",
      "Epoch [3/12], Step [21/74], D_loss: 1.6155, G_loss: 17.7709\n",
      "Epoch [3/12], Step [22/74], D_loss: 1.2833, G_loss: 20.3452\n",
      "Epoch [3/12], Step [23/74], D_loss: 1.6567, G_loss: 16.3922\n",
      "Epoch [3/12], Step [24/74], D_loss: 1.4487, G_loss: 23.2828\n",
      "Epoch [3/12], Step [25/74], D_loss: 1.5071, G_loss: 15.8094\n",
      "Epoch [3/12], Step [26/74], D_loss: 1.3114, G_loss: 16.0400\n",
      "Epoch [3/12], Step [27/74], D_loss: 1.2260, G_loss: 11.2553\n",
      "Epoch [3/12], Step [28/74], D_loss: 1.0902, G_loss: 13.5099\n",
      "Epoch [3/12], Step [29/74], D_loss: 1.4456, G_loss: 14.5024\n",
      "Epoch [3/12], Step [30/74], D_loss: 1.1242, G_loss: 12.2812\n",
      "Epoch [3/12], Step [31/74], D_loss: 0.8059, G_loss: 10.4615\n",
      "Epoch [3/12], Step [32/74], D_loss: 0.8658, G_loss: 11.5643\n",
      "Epoch [3/12], Step [33/74], D_loss: 1.3914, G_loss: 22.6022\n",
      "Epoch [3/12], Step [34/74], D_loss: 1.2995, G_loss: 12.5213\n",
      "Epoch [3/12], Step [35/74], D_loss: 0.8876, G_loss: 13.7772\n",
      "Epoch [3/12], Step [36/74], D_loss: 0.8492, G_loss: 11.9492\n",
      "Epoch [3/12], Step [37/74], D_loss: 1.0585, G_loss: 14.2922\n",
      "Epoch [3/12], Step [38/74], D_loss: 1.0912, G_loss: 8.9457\n",
      "Epoch [3/12], Step [39/74], D_loss: 0.8139, G_loss: 22.0819\n",
      "Epoch [3/12], Step [40/74], D_loss: 0.8409, G_loss: 10.9007\n",
      "Epoch [3/12], Step [41/74], D_loss: 0.8069, G_loss: 8.6282\n",
      "Epoch [3/12], Step [42/74], D_loss: 0.7873, G_loss: 11.4829\n",
      "Epoch [3/12], Step [43/74], D_loss: 0.7749, G_loss: 12.2262\n",
      "Epoch [3/12], Step [44/74], D_loss: 0.8978, G_loss: 11.3180\n",
      "Epoch [3/12], Step [45/74], D_loss: 1.1192, G_loss: 12.3499\n",
      "Epoch [3/12], Step [46/74], D_loss: 1.0512, G_loss: 19.7943\n",
      "Epoch [3/12], Step [47/74], D_loss: 0.7581, G_loss: 9.5702\n",
      "Epoch [3/12], Step [48/74], D_loss: 0.6504, G_loss: 11.9662\n",
      "Epoch [3/12], Step [49/74], D_loss: 0.6427, G_loss: 15.2192\n",
      "Epoch [3/12], Step [50/74], D_loss: 0.6853, G_loss: 16.1476\n",
      "Epoch [3/12], Step [51/74], D_loss: 0.7366, G_loss: 15.6543\n",
      "Epoch [3/12], Step [52/74], D_loss: 0.7933, G_loss: 10.9561\n",
      "Epoch [3/12], Step [53/74], D_loss: 0.8957, G_loss: 16.3875\n",
      "Epoch [3/12], Step [54/74], D_loss: 0.6758, G_loss: 11.8741\n",
      "Epoch [3/12], Step [55/74], D_loss: 0.6492, G_loss: 26.7229\n",
      "Epoch [3/12], Step [56/74], D_loss: 0.7306, G_loss: 18.8565\n",
      "Epoch [3/12], Step [57/74], D_loss: 0.7640, G_loss: 17.7400\n",
      "Epoch [3/12], Step [58/74], D_loss: 0.5502, G_loss: 13.5821\n",
      "Epoch [3/12], Step [59/74], D_loss: 0.6057, G_loss: 17.0881\n",
      "Epoch [3/12], Step [60/74], D_loss: 0.5860, G_loss: 13.4429\n",
      "Epoch [3/12], Step [61/74], D_loss: 0.6123, G_loss: 7.1413\n",
      "Epoch [3/12], Step [62/74], D_loss: 0.6838, G_loss: 9.2077\n",
      "Epoch [3/12], Step [63/74], D_loss: 1.0149, G_loss: 15.7519\n",
      "Epoch [3/12], Step [64/74], D_loss: 1.0554, G_loss: 13.3519\n",
      "Epoch [3/12], Step [65/74], D_loss: 0.7840, G_loss: 15.8789\n",
      "Epoch [3/12], Step [66/74], D_loss: 0.8538, G_loss: 11.9942\n",
      "Epoch [3/12], Step [67/74], D_loss: 0.8849, G_loss: 13.4612\n",
      "Epoch [3/12], Step [68/74], D_loss: 0.7268, G_loss: 20.4809\n",
      "Epoch [3/12], Step [69/74], D_loss: 0.5666, G_loss: 13.8484\n",
      "Epoch [3/12], Step [70/74], D_loss: 0.5816, G_loss: 15.3915\n",
      "Epoch [3/12], Step [71/74], D_loss: 0.7144, G_loss: 11.0097\n",
      "Epoch [3/12], Step [72/74], D_loss: 0.7534, G_loss: 10.8815\n",
      "Epoch [3/12], Step [73/74], D_loss: 0.9630, G_loss: 17.1776\n",
      "Epoch [3/12], Step [74/74], D_loss: 0.9821, G_loss: 12.8003\n",
      "Epoch [4/12], Step [1/74], D_loss: 0.9354, G_loss: 12.4680\n",
      "Epoch [4/12], Step [2/74], D_loss: 0.7301, G_loss: 11.5978\n",
      "Epoch [4/12], Step [3/74], D_loss: 0.6507, G_loss: 10.6953\n",
      "Epoch [4/12], Step [4/74], D_loss: 0.7886, G_loss: 16.7999\n",
      "Epoch [4/12], Step [5/74], D_loss: 0.9442, G_loss: 11.6556\n",
      "Epoch [4/12], Step [6/74], D_loss: 0.6020, G_loss: 13.1213\n",
      "Epoch [4/12], Step [7/74], D_loss: 0.7994, G_loss: 10.4452\n",
      "Epoch [4/12], Step [8/74], D_loss: 1.2459, G_loss: 7.5162\n",
      "Epoch [4/12], Step [9/74], D_loss: 0.9616, G_loss: 11.1978\n",
      "Epoch [4/12], Step [10/74], D_loss: 0.7930, G_loss: 16.3707\n",
      "Epoch [4/12], Step [11/74], D_loss: 0.6588, G_loss: 19.7162\n",
      "Epoch [4/12], Step [12/74], D_loss: 0.8348, G_loss: 14.9917\n",
      "Epoch [4/12], Step [13/74], D_loss: 0.7404, G_loss: 15.2660\n",
      "Epoch [4/12], Step [14/74], D_loss: 0.7184, G_loss: 15.6945\n",
      "Epoch [4/12], Step [15/74], D_loss: 0.8284, G_loss: 17.7375\n",
      "Epoch [4/12], Step [16/74], D_loss: 0.5872, G_loss: 12.1090\n",
      "Epoch [4/12], Step [17/74], D_loss: 0.7558, G_loss: 8.6009\n",
      "Epoch [4/12], Step [18/74], D_loss: 0.4786, G_loss: 12.2970\n",
      "Epoch [4/12], Step [19/74], D_loss: 0.5563, G_loss: 10.7391\n",
      "Epoch [4/12], Step [20/74], D_loss: 0.9290, G_loss: 7.0680\n",
      "Epoch [4/12], Step [21/74], D_loss: 0.9906, G_loss: 9.6292\n",
      "Epoch [4/12], Step [22/74], D_loss: 0.7373, G_loss: 13.2589\n",
      "Epoch [4/12], Step [23/74], D_loss: 0.6885, G_loss: 9.9340\n",
      "Epoch [4/12], Step [24/74], D_loss: 0.9478, G_loss: 12.2333\n",
      "Epoch [4/12], Step [25/74], D_loss: 0.7424, G_loss: 15.0467\n",
      "Epoch [4/12], Step [26/74], D_loss: 0.7906, G_loss: 7.2604\n",
      "Epoch [4/12], Step [27/74], D_loss: 0.8741, G_loss: 14.9210\n",
      "Epoch [4/12], Step [28/74], D_loss: 0.5913, G_loss: 8.5144\n",
      "Epoch [4/12], Step [29/74], D_loss: 0.7098, G_loss: 7.8808\n",
      "Epoch [4/12], Step [30/74], D_loss: 0.7073, G_loss: 8.9667\n",
      "Epoch [4/12], Step [31/74], D_loss: 0.7160, G_loss: 11.2235\n",
      "Epoch [4/12], Step [32/74], D_loss: 0.5701, G_loss: 11.8984\n",
      "Epoch [4/12], Step [33/74], D_loss: 0.5983, G_loss: 19.8939\n",
      "Epoch [4/12], Step [34/74], D_loss: 0.5860, G_loss: 9.8770\n",
      "Epoch [4/12], Step [35/74], D_loss: 0.6273, G_loss: 15.2596\n",
      "Epoch [4/12], Step [36/74], D_loss: 0.7753, G_loss: 15.3192\n",
      "Epoch [4/12], Step [37/74], D_loss: 0.5505, G_loss: 11.0447\n",
      "Epoch [4/12], Step [38/74], D_loss: 0.6288, G_loss: 8.0770\n",
      "Epoch [4/12], Step [39/74], D_loss: 0.5528, G_loss: 12.2463\n",
      "Epoch [4/12], Step [40/74], D_loss: 0.5315, G_loss: 13.6232\n",
      "Epoch [4/12], Step [41/74], D_loss: 0.7032, G_loss: 16.0139\n",
      "Epoch [4/12], Step [42/74], D_loss: 0.5371, G_loss: 9.6487\n",
      "Epoch [4/12], Step [43/74], D_loss: 0.7535, G_loss: 13.0099\n",
      "Epoch [4/12], Step [44/74], D_loss: 0.9388, G_loss: 6.7696\n",
      "Epoch [4/12], Step [45/74], D_loss: 0.7310, G_loss: 11.9821\n",
      "Epoch [4/12], Step [46/74], D_loss: 0.8493, G_loss: 8.4902\n",
      "Epoch [4/12], Step [47/74], D_loss: 0.6532, G_loss: 16.6005\n",
      "Epoch [4/12], Step [48/74], D_loss: 0.7715, G_loss: 15.2653\n",
      "Epoch [4/12], Step [49/74], D_loss: 0.8233, G_loss: 10.1261\n",
      "Epoch [4/12], Step [50/74], D_loss: 0.8803, G_loss: 8.8266\n",
      "Epoch [4/12], Step [51/74], D_loss: 0.8058, G_loss: 10.5246\n",
      "Epoch [4/12], Step [52/74], D_loss: 0.9596, G_loss: 10.3618\n",
      "Epoch [4/12], Step [53/74], D_loss: 1.0567, G_loss: 8.7928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/12], Step [54/74], D_loss: 0.7715, G_loss: 8.6341\n",
      "Epoch [4/12], Step [55/74], D_loss: 0.6750, G_loss: 11.3408\n",
      "Epoch [4/12], Step [56/74], D_loss: 0.6674, G_loss: 12.5870\n",
      "Epoch [4/12], Step [57/74], D_loss: 0.8071, G_loss: 10.9132\n",
      "Epoch [4/12], Step [58/74], D_loss: 0.6026, G_loss: 8.0134\n",
      "Epoch [4/12], Step [59/74], D_loss: 0.6569, G_loss: 8.3448\n",
      "Epoch [4/12], Step [60/74], D_loss: 0.6902, G_loss: 8.2494\n",
      "Epoch [4/12], Step [61/74], D_loss: 1.0621, G_loss: 7.3979\n",
      "Epoch [4/12], Step [62/74], D_loss: 0.4948, G_loss: 16.0555\n",
      "Epoch [4/12], Step [63/74], D_loss: 0.7669, G_loss: 9.7748\n",
      "Epoch [4/12], Step [64/74], D_loss: 0.6799, G_loss: 17.0471\n",
      "Epoch [4/12], Step [65/74], D_loss: 0.7125, G_loss: 9.2283\n",
      "Epoch [4/12], Step [66/74], D_loss: 0.7376, G_loss: 18.9375\n",
      "Epoch [4/12], Step [67/74], D_loss: 0.6544, G_loss: 17.5404\n",
      "Epoch [4/12], Step [68/74], D_loss: 0.6991, G_loss: 8.9946\n",
      "Epoch [4/12], Step [69/74], D_loss: 0.4979, G_loss: 14.2648\n",
      "Epoch [4/12], Step [70/74], D_loss: 0.4605, G_loss: 7.5678\n",
      "Epoch [4/12], Step [71/74], D_loss: 0.4548, G_loss: 8.5784\n",
      "Epoch [4/12], Step [72/74], D_loss: 0.8134, G_loss: 12.6199\n",
      "Epoch [4/12], Step [73/74], D_loss: 1.1454, G_loss: 13.7260\n",
      "Epoch [4/12], Step [74/74], D_loss: 0.8476, G_loss: 8.5723\n",
      "Epoch [5/12], Step [1/74], D_loss: 0.8100, G_loss: 12.6447\n",
      "Epoch [5/12], Step [2/74], D_loss: 0.5540, G_loss: 7.9660\n",
      "Epoch [5/12], Step [3/74], D_loss: 0.6349, G_loss: 7.4695\n",
      "Epoch [5/12], Step [4/74], D_loss: 0.5649, G_loss: 6.9461\n",
      "Epoch [5/12], Step [5/74], D_loss: 0.5867, G_loss: 15.0845\n",
      "Epoch [5/12], Step [6/74], D_loss: 0.6549, G_loss: 16.5873\n",
      "Epoch [5/12], Step [7/74], D_loss: 0.9028, G_loss: 10.5877\n",
      "Epoch [5/12], Step [8/74], D_loss: 1.1252, G_loss: 7.0964\n",
      "Epoch [5/12], Step [9/74], D_loss: 0.7939, G_loss: 9.9304\n",
      "Epoch [5/12], Step [10/74], D_loss: 1.0353, G_loss: 8.4730\n",
      "Epoch [5/12], Step [11/74], D_loss: 0.7422, G_loss: 8.3764\n",
      "Epoch [5/12], Step [12/74], D_loss: 0.6597, G_loss: 8.7655\n",
      "Epoch [5/12], Step [13/74], D_loss: 0.5620, G_loss: 8.4565\n",
      "Epoch [5/12], Step [14/74], D_loss: 0.6830, G_loss: 8.3935\n",
      "Epoch [5/12], Step [15/74], D_loss: 0.6468, G_loss: 11.6271\n",
      "Epoch [5/12], Step [16/74], D_loss: 0.6841, G_loss: 13.8044\n",
      "Epoch [5/12], Step [17/74], D_loss: 0.6613, G_loss: 10.6113\n",
      "Epoch [5/12], Step [18/74], D_loss: 0.7989, G_loss: 19.4401\n",
      "Epoch [5/12], Step [19/74], D_loss: 0.5017, G_loss: 10.8118\n",
      "Epoch [5/12], Step [20/74], D_loss: 0.6922, G_loss: 10.3319\n",
      "Epoch [5/12], Step [21/74], D_loss: 0.7583, G_loss: 9.9157\n",
      "Epoch [5/12], Step [22/74], D_loss: 0.5350, G_loss: 11.5861\n",
      "Epoch [5/12], Step [23/74], D_loss: 1.0232, G_loss: 8.6840\n",
      "Epoch [5/12], Step [24/74], D_loss: 0.8263, G_loss: 7.0290\n",
      "Epoch [5/12], Step [25/74], D_loss: 0.8186, G_loss: 12.5356\n",
      "Epoch [5/12], Step [26/74], D_loss: 0.4964, G_loss: 11.0112\n",
      "Epoch [5/12], Step [27/74], D_loss: 0.7634, G_loss: 10.3757\n",
      "Epoch [5/12], Step [28/74], D_loss: 0.6736, G_loss: 13.8811\n",
      "Epoch [5/12], Step [29/74], D_loss: 1.0000, G_loss: 12.8495\n",
      "Epoch [5/12], Step [30/74], D_loss: 0.6399, G_loss: 9.3959\n",
      "Epoch [5/12], Step [31/74], D_loss: 0.6995, G_loss: 11.5729\n",
      "Epoch [5/12], Step [32/74], D_loss: 0.5540, G_loss: 13.9942\n",
      "Epoch [5/12], Step [33/74], D_loss: 0.6293, G_loss: 14.9620\n",
      "Epoch [5/12], Step [34/74], D_loss: 0.8449, G_loss: 8.1949\n",
      "Epoch [5/12], Step [35/74], D_loss: 0.9640, G_loss: 9.6328\n",
      "Epoch [5/12], Step [36/74], D_loss: 0.8969, G_loss: 7.9568\n",
      "Epoch [5/12], Step [37/74], D_loss: 0.8753, G_loss: 5.8067\n",
      "Epoch [5/12], Step [38/74], D_loss: 0.7862, G_loss: 11.5620\n",
      "Epoch [5/12], Step [39/74], D_loss: 0.6868, G_loss: 7.7929\n",
      "Epoch [5/12], Step [40/74], D_loss: 0.8093, G_loss: 7.6400\n",
      "Epoch [5/12], Step [41/74], D_loss: 0.7710, G_loss: 16.9606\n",
      "Epoch [5/12], Step [42/74], D_loss: 1.0646, G_loss: 8.1254\n",
      "Epoch [5/12], Step [43/74], D_loss: 0.8639, G_loss: 9.8187\n",
      "Epoch [5/12], Step [44/74], D_loss: 0.9810, G_loss: 7.9422\n",
      "Epoch [5/12], Step [45/74], D_loss: 0.6208, G_loss: 19.3521\n",
      "Epoch [5/12], Step [46/74], D_loss: 0.6467, G_loss: 9.6799\n",
      "Epoch [5/12], Step [47/74], D_loss: 0.7949, G_loss: 12.1409\n",
      "Epoch [5/12], Step [48/74], D_loss: 0.8013, G_loss: 7.7920\n",
      "Epoch [5/12], Step [49/74], D_loss: 0.7656, G_loss: 15.4342\n",
      "Epoch [5/12], Step [50/74], D_loss: 0.7966, G_loss: 11.8285\n",
      "Epoch [5/12], Step [51/74], D_loss: 0.7216, G_loss: 6.1684\n",
      "Epoch [5/12], Step [52/74], D_loss: 0.8166, G_loss: 11.6816\n",
      "Epoch [5/12], Step [53/74], D_loss: 0.7044, G_loss: 6.8430\n",
      "Epoch [5/12], Step [54/74], D_loss: 0.6557, G_loss: 7.1277\n",
      "Epoch [5/12], Step [55/74], D_loss: 0.7356, G_loss: 11.3696\n",
      "Epoch [5/12], Step [56/74], D_loss: 0.8076, G_loss: 9.1495\n",
      "Epoch [5/12], Step [57/74], D_loss: 0.8116, G_loss: 5.1824\n",
      "Epoch [5/12], Step [58/74], D_loss: 0.7898, G_loss: 11.1026\n",
      "Epoch [5/12], Step [59/74], D_loss: 0.6119, G_loss: 14.8546\n",
      "Epoch [5/12], Step [60/74], D_loss: 0.5062, G_loss: 10.5724\n",
      "Epoch [5/12], Step [61/74], D_loss: 0.5612, G_loss: 10.1120\n",
      "Epoch [5/12], Step [62/74], D_loss: 0.6836, G_loss: 6.6261\n",
      "Epoch [5/12], Step [63/74], D_loss: 0.6227, G_loss: 5.4642\n",
      "Epoch [5/12], Step [64/74], D_loss: 0.7260, G_loss: 9.1763\n",
      "Epoch [5/12], Step [65/74], D_loss: 0.7691, G_loss: 5.4915\n",
      "Epoch [5/12], Step [66/74], D_loss: 0.9654, G_loss: 10.5406\n",
      "Epoch [5/12], Step [67/74], D_loss: 0.8191, G_loss: 4.8773\n",
      "Epoch [5/12], Step [68/74], D_loss: 0.7454, G_loss: 6.3287\n",
      "Epoch [5/12], Step [69/74], D_loss: 0.6735, G_loss: 9.0111\n",
      "Epoch [5/12], Step [70/74], D_loss: 0.6906, G_loss: 5.3471\n",
      "Epoch [5/12], Step [71/74], D_loss: 0.7126, G_loss: 7.7202\n",
      "Epoch [5/12], Step [72/74], D_loss: 0.8838, G_loss: 7.5354\n",
      "Epoch [5/12], Step [73/74], D_loss: 0.8115, G_loss: 8.5189\n",
      "Epoch [5/12], Step [74/74], D_loss: 0.8593, G_loss: 9.7417\n",
      "Epoch [6/12], Step [1/74], D_loss: 0.8549, G_loss: 9.9023\n",
      "Epoch [6/12], Step [2/74], D_loss: 0.7175, G_loss: 6.4789\n",
      "Epoch [6/12], Step [3/74], D_loss: 0.8807, G_loss: 13.8685\n",
      "Epoch [6/12], Step [4/74], D_loss: 0.7794, G_loss: 5.5519\n",
      "Epoch [6/12], Step [5/74], D_loss: 0.8012, G_loss: 3.7125\n",
      "Epoch [6/12], Step [6/74], D_loss: 0.7489, G_loss: 9.3267\n",
      "Epoch [6/12], Step [7/74], D_loss: 0.7787, G_loss: 5.5751\n",
      "Epoch [6/12], Step [8/74], D_loss: 0.6866, G_loss: 11.9634\n",
      "Epoch [6/12], Step [9/74], D_loss: 0.8688, G_loss: 7.6180\n",
      "Epoch [6/12], Step [10/74], D_loss: 0.5539, G_loss: 13.3170\n",
      "Epoch [6/12], Step [11/74], D_loss: 0.5914, G_loss: 9.1036\n",
      "Epoch [6/12], Step [12/74], D_loss: 0.6925, G_loss: 17.0854\n",
      "Epoch [6/12], Step [13/74], D_loss: 0.8784, G_loss: 5.6912\n",
      "Epoch [6/12], Step [14/74], D_loss: 0.7193, G_loss: 4.1081\n",
      "Epoch [6/12], Step [15/74], D_loss: 0.9179, G_loss: 9.9826\n",
      "Epoch [6/12], Step [16/74], D_loss: 0.7018, G_loss: 6.2026\n",
      "Epoch [6/12], Step [17/74], D_loss: 0.7630, G_loss: 10.2561\n",
      "Epoch [6/12], Step [18/74], D_loss: 0.6422, G_loss: 10.0565\n",
      "Epoch [6/12], Step [19/74], D_loss: 0.7041, G_loss: 6.8738\n",
      "Epoch [6/12], Step [20/74], D_loss: 0.8579, G_loss: 10.5616\n",
      "Epoch [6/12], Step [21/74], D_loss: 0.7054, G_loss: 9.4890\n",
      "Epoch [6/12], Step [22/74], D_loss: 0.9385, G_loss: 6.3747\n",
      "Epoch [6/12], Step [23/74], D_loss: 0.5999, G_loss: 7.1734\n",
      "Epoch [6/12], Step [24/74], D_loss: 0.8160, G_loss: 10.8766\n",
      "Epoch [6/12], Step [25/74], D_loss: 0.6242, G_loss: 7.4147\n",
      "Epoch [6/12], Step [26/74], D_loss: 0.7399, G_loss: 10.2850\n",
      "Epoch [6/12], Step [27/74], D_loss: 0.6694, G_loss: 10.4218\n",
      "Epoch [6/12], Step [28/74], D_loss: 0.7072, G_loss: 8.7055\n",
      "Epoch [6/12], Step [29/74], D_loss: 0.7143, G_loss: 9.3929\n",
      "Epoch [6/12], Step [30/74], D_loss: 0.7212, G_loss: 8.8600\n",
      "Epoch [6/12], Step [31/74], D_loss: 0.7675, G_loss: 6.9763\n",
      "Epoch [6/12], Step [32/74], D_loss: 0.6267, G_loss: 9.8834\n",
      "Epoch [6/12], Step [33/74], D_loss: 0.5261, G_loss: 9.2259\n",
      "Epoch [6/12], Step [34/74], D_loss: 0.7489, G_loss: 7.9620\n",
      "Epoch [6/12], Step [35/74], D_loss: 0.5964, G_loss: 7.7090\n",
      "Epoch [6/12], Step [36/74], D_loss: 0.7381, G_loss: 8.1025\n",
      "Epoch [6/12], Step [37/74], D_loss: 0.8348, G_loss: 17.3457\n",
      "Epoch [6/12], Step [38/74], D_loss: 0.8637, G_loss: 7.7326\n",
      "Epoch [6/12], Step [39/74], D_loss: 0.7614, G_loss: 4.7152\n",
      "Epoch [6/12], Step [40/74], D_loss: 0.7545, G_loss: 12.8318\n",
      "Epoch [6/12], Step [41/74], D_loss: 0.7379, G_loss: 8.9978\n",
      "Epoch [6/12], Step [42/74], D_loss: 0.7344, G_loss: 11.0422\n",
      "Epoch [6/12], Step [43/74], D_loss: 0.6673, G_loss: 7.2996\n",
      "Epoch [6/12], Step [44/74], D_loss: 0.6076, G_loss: 9.7973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/12], Step [45/74], D_loss: 0.8788, G_loss: 16.1560\n",
      "Epoch [6/12], Step [46/74], D_loss: 0.6009, G_loss: 6.7689\n",
      "Epoch [6/12], Step [47/74], D_loss: 0.7228, G_loss: 7.5266\n",
      "Epoch [6/12], Step [48/74], D_loss: 0.4761, G_loss: 11.5514\n",
      "Epoch [6/12], Step [49/74], D_loss: 0.5714, G_loss: 7.7961\n",
      "Epoch [6/12], Step [50/74], D_loss: 0.5994, G_loss: 14.0744\n",
      "Epoch [6/12], Step [51/74], D_loss: 0.8918, G_loss: 4.4935\n",
      "Epoch [6/12], Step [52/74], D_loss: 0.7048, G_loss: 12.2766\n",
      "Epoch [6/12], Step [53/74], D_loss: 0.6400, G_loss: 11.0051\n",
      "Epoch [6/12], Step [54/74], D_loss: 0.9147, G_loss: 8.1246\n",
      "Epoch [6/12], Step [55/74], D_loss: 0.6672, G_loss: 7.9979\n",
      "Epoch [6/12], Step [56/74], D_loss: 0.7966, G_loss: 5.6790\n",
      "Epoch [6/12], Step [57/74], D_loss: 0.5949, G_loss: 15.5744\n",
      "Epoch [6/12], Step [58/74], D_loss: 0.7919, G_loss: 12.9706\n",
      "Epoch [6/12], Step [59/74], D_loss: 1.1981, G_loss: 5.5482\n",
      "Epoch [6/12], Step [60/74], D_loss: 0.7021, G_loss: 7.2868\n",
      "Epoch [6/12], Step [61/74], D_loss: 0.7570, G_loss: 9.8109\n",
      "Epoch [6/12], Step [62/74], D_loss: 0.8713, G_loss: 13.0735\n",
      "Epoch [6/12], Step [63/74], D_loss: 0.5816, G_loss: 8.0561\n",
      "Epoch [6/12], Step [64/74], D_loss: 0.6538, G_loss: 14.5176\n",
      "Epoch [6/12], Step [65/74], D_loss: 0.7074, G_loss: 5.7557\n",
      "Epoch [6/12], Step [66/74], D_loss: 0.4393, G_loss: 7.6282\n",
      "Epoch [6/12], Step [67/74], D_loss: 0.6016, G_loss: 5.5419\n",
      "Epoch [6/12], Step [68/74], D_loss: 0.7182, G_loss: 5.9861\n",
      "Epoch [6/12], Step [69/74], D_loss: 0.8148, G_loss: 5.1952\n",
      "Epoch [6/12], Step [70/74], D_loss: 0.6597, G_loss: 8.2135\n",
      "Epoch [6/12], Step [71/74], D_loss: 0.5954, G_loss: 12.2468\n",
      "Epoch [6/12], Step [72/74], D_loss: 0.7158, G_loss: 14.6248\n",
      "Epoch [6/12], Step [73/74], D_loss: 0.5783, G_loss: 7.4892\n",
      "Epoch [6/12], Step [74/74], D_loss: 0.8466, G_loss: 9.5260\n",
      "Epoch [7/12], Step [1/74], D_loss: 0.8106, G_loss: 4.8658\n",
      "Epoch [7/12], Step [2/74], D_loss: 0.9253, G_loss: 9.1185\n",
      "Epoch [7/12], Step [3/74], D_loss: 0.5751, G_loss: 15.5465\n",
      "Epoch [7/12], Step [4/74], D_loss: 0.5774, G_loss: 9.3002\n",
      "Epoch [7/12], Step [5/74], D_loss: 0.5193, G_loss: 9.1255\n",
      "Epoch [7/12], Step [6/74], D_loss: 0.7678, G_loss: 7.8738\n",
      "Epoch [7/12], Step [7/74], D_loss: 0.7323, G_loss: 7.5514\n",
      "Epoch [7/12], Step [8/74], D_loss: 0.8612, G_loss: 15.0632\n",
      "Epoch [7/12], Step [9/74], D_loss: 1.2521, G_loss: 6.5092\n",
      "Epoch [7/12], Step [10/74], D_loss: 0.6807, G_loss: 8.3966\n",
      "Epoch [7/12], Step [11/74], D_loss: 0.8286, G_loss: 8.8342\n",
      "Epoch [7/12], Step [12/74], D_loss: 0.5925, G_loss: 6.4767\n",
      "Epoch [7/12], Step [13/74], D_loss: 0.7139, G_loss: 7.2800\n",
      "Epoch [7/12], Step [14/74], D_loss: 0.6380, G_loss: 7.4189\n",
      "Epoch [7/12], Step [15/74], D_loss: 0.5423, G_loss: 16.1434\n",
      "Epoch [7/12], Step [16/74], D_loss: 1.0040, G_loss: 9.0339\n",
      "Epoch [7/12], Step [17/74], D_loss: 0.5840, G_loss: 8.8270\n",
      "Epoch [7/12], Step [18/74], D_loss: 0.5038, G_loss: 8.0014\n",
      "Epoch [7/12], Step [19/74], D_loss: 0.5555, G_loss: 7.9432\n",
      "Epoch [7/12], Step [20/74], D_loss: 0.6608, G_loss: 8.9375\n",
      "Epoch [7/12], Step [21/74], D_loss: 0.7576, G_loss: 4.2205\n",
      "Epoch [7/12], Step [22/74], D_loss: 0.6323, G_loss: 9.9886\n",
      "Epoch [7/12], Step [23/74], D_loss: 0.7212, G_loss: 4.9569\n",
      "Epoch [7/12], Step [24/74], D_loss: 0.7925, G_loss: 5.3448\n",
      "Epoch [7/12], Step [25/74], D_loss: 0.7699, G_loss: 6.8687\n",
      "Epoch [7/12], Step [26/74], D_loss: 0.7951, G_loss: 9.6215\n",
      "Epoch [7/12], Step [27/74], D_loss: 0.6984, G_loss: 4.7848\n",
      "Epoch [7/12], Step [28/74], D_loss: 0.7124, G_loss: 14.0638\n",
      "Epoch [7/12], Step [29/74], D_loss: 0.7896, G_loss: 12.9542\n",
      "Epoch [7/12], Step [30/74], D_loss: 0.6555, G_loss: 11.0689\n",
      "Epoch [7/12], Step [31/74], D_loss: 1.0289, G_loss: 7.5300\n",
      "Epoch [7/12], Step [32/74], D_loss: 0.7644, G_loss: 9.6471\n",
      "Epoch [7/12], Step [33/74], D_loss: 0.7694, G_loss: 6.4404\n",
      "Epoch [7/12], Step [34/74], D_loss: 0.5820, G_loss: 14.7064\n",
      "Epoch [7/12], Step [35/74], D_loss: 0.9856, G_loss: 6.5213\n",
      "Epoch [7/12], Step [36/74], D_loss: 0.7343, G_loss: 8.9584\n",
      "Epoch [7/12], Step [37/74], D_loss: 0.6861, G_loss: 12.8232\n",
      "Epoch [7/12], Step [38/74], D_loss: 0.9069, G_loss: 7.3238\n",
      "Epoch [7/12], Step [39/74], D_loss: 0.7871, G_loss: 13.3407\n",
      "Epoch [7/12], Step [40/74], D_loss: 0.5567, G_loss: 5.1664\n",
      "Epoch [7/12], Step [41/74], D_loss: 0.8378, G_loss: 9.7045\n",
      "Epoch [7/12], Step [42/74], D_loss: 0.7082, G_loss: 5.5182\n",
      "Epoch [7/12], Step [43/74], D_loss: 0.6429, G_loss: 5.1614\n",
      "Epoch [7/12], Step [44/74], D_loss: 0.6695, G_loss: 8.2318\n",
      "Epoch [7/12], Step [45/74], D_loss: 0.7285, G_loss: 6.4475\n",
      "Epoch [7/12], Step [46/74], D_loss: 0.7962, G_loss: 10.4445\n",
      "Epoch [7/12], Step [47/74], D_loss: 0.7174, G_loss: 10.4520\n",
      "Epoch [7/12], Step [48/74], D_loss: 0.6809, G_loss: 5.1026\n",
      "Epoch [7/12], Step [49/74], D_loss: 0.7720, G_loss: 16.7095\n",
      "Epoch [7/12], Step [50/74], D_loss: 0.4918, G_loss: 12.3830\n",
      "Epoch [7/12], Step [51/74], D_loss: 0.6698, G_loss: 5.1397\n",
      "Epoch [7/12], Step [52/74], D_loss: 0.4462, G_loss: 8.1554\n",
      "Epoch [7/12], Step [53/74], D_loss: 0.6579, G_loss: 11.1439\n",
      "Epoch [7/12], Step [54/74], D_loss: 0.5125, G_loss: 6.6172\n",
      "Epoch [7/12], Step [55/74], D_loss: 0.5681, G_loss: 6.2897\n",
      "Epoch [7/12], Step [56/74], D_loss: 0.5280, G_loss: 7.4340\n",
      "Epoch [7/12], Step [57/74], D_loss: 0.7320, G_loss: 5.7251\n",
      "Epoch [7/12], Step [58/74], D_loss: 0.9981, G_loss: 4.6407\n",
      "Epoch [7/12], Step [59/74], D_loss: 0.7629, G_loss: 15.3958\n",
      "Epoch [7/12], Step [60/74], D_loss: 0.7043, G_loss: 9.6856\n",
      "Epoch [7/12], Step [61/74], D_loss: 0.7148, G_loss: 12.3054\n",
      "Epoch [7/12], Step [62/74], D_loss: 0.6546, G_loss: 12.7368\n",
      "Epoch [7/12], Step [63/74], D_loss: 0.7917, G_loss: 8.2631\n",
      "Epoch [7/12], Step [64/74], D_loss: 0.9024, G_loss: 5.9473\n",
      "Epoch [7/12], Step [65/74], D_loss: 0.7514, G_loss: 10.9864\n",
      "Epoch [7/12], Step [66/74], D_loss: 0.9170, G_loss: 5.6405\n",
      "Epoch [7/12], Step [67/74], D_loss: 0.5113, G_loss: 9.0846\n",
      "Epoch [7/12], Step [68/74], D_loss: 0.5364, G_loss: 6.3754\n",
      "Epoch [7/12], Step [69/74], D_loss: 0.5401, G_loss: 6.4746\n",
      "Epoch [7/12], Step [70/74], D_loss: 0.6893, G_loss: 6.3307\n",
      "Epoch [7/12], Step [71/74], D_loss: 0.7066, G_loss: 9.9249\n",
      "Epoch [7/12], Step [72/74], D_loss: 0.5827, G_loss: 6.3164\n",
      "Epoch [7/12], Step [73/74], D_loss: 0.9039, G_loss: 20.0725\n",
      "Epoch [7/12], Step [74/74], D_loss: 0.6097, G_loss: 13.1716\n",
      "Epoch [8/12], Step [1/74], D_loss: 0.7245, G_loss: 9.1544\n",
      "Epoch [8/12], Step [2/74], D_loss: 0.6045, G_loss: 7.2146\n",
      "Epoch [8/12], Step [3/74], D_loss: 0.9670, G_loss: 7.1273\n",
      "Epoch [8/12], Step [4/74], D_loss: 1.6667, G_loss: 5.0846\n",
      "Epoch [8/12], Step [5/74], D_loss: 1.0552, G_loss: 7.0873\n",
      "Epoch [8/12], Step [6/74], D_loss: 1.0658, G_loss: 5.5577\n",
      "Epoch [8/12], Step [7/74], D_loss: 0.7386, G_loss: 9.9222\n",
      "Epoch [8/12], Step [8/74], D_loss: 0.7490, G_loss: 7.0604\n",
      "Epoch [8/12], Step [9/74], D_loss: 0.6264, G_loss: 6.1673\n",
      "Epoch [8/12], Step [10/74], D_loss: 0.6641, G_loss: 10.4126\n",
      "Epoch [8/12], Step [11/74], D_loss: 0.6582, G_loss: 11.8335\n",
      "Epoch [8/12], Step [12/74], D_loss: 0.7625, G_loss: 7.5437\n",
      "Epoch [8/12], Step [13/74], D_loss: 0.6352, G_loss: 8.7235\n",
      "Epoch [8/12], Step [14/74], D_loss: 0.4809, G_loss: 12.0156\n",
      "Epoch [8/12], Step [15/74], D_loss: 0.6356, G_loss: 8.6795\n",
      "Epoch [8/12], Step [16/74], D_loss: 0.7706, G_loss: 6.1363\n",
      "Epoch [8/12], Step [17/74], D_loss: 0.8258, G_loss: 11.0592\n",
      "Epoch [8/12], Step [18/74], D_loss: 0.9069, G_loss: 12.5273\n",
      "Epoch [8/12], Step [19/74], D_loss: 0.7349, G_loss: 5.0319\n",
      "Epoch [8/12], Step [20/74], D_loss: 0.4758, G_loss: 15.7494\n",
      "Epoch [8/12], Step [21/74], D_loss: 0.5558, G_loss: 7.2814\n",
      "Epoch [8/12], Step [22/74], D_loss: 0.5046, G_loss: 5.8763\n",
      "Epoch [8/12], Step [23/74], D_loss: 0.7433, G_loss: 9.8264\n",
      "Epoch [8/12], Step [24/74], D_loss: 0.7284, G_loss: 9.1559\n",
      "Epoch [8/12], Step [25/74], D_loss: 0.7841, G_loss: 4.3486\n",
      "Epoch [8/12], Step [26/74], D_loss: 0.6712, G_loss: 9.7144\n",
      "Epoch [8/12], Step [27/74], D_loss: 0.4040, G_loss: 9.7132\n",
      "Epoch [8/12], Step [28/74], D_loss: 0.6125, G_loss: 10.7483\n",
      "Epoch [8/12], Step [29/74], D_loss: 0.6023, G_loss: 14.1089\n",
      "Epoch [8/12], Step [30/74], D_loss: 0.6116, G_loss: 9.5826\n",
      "Epoch [8/12], Step [31/74], D_loss: 0.7758, G_loss: 7.3860\n",
      "Epoch [8/12], Step [32/74], D_loss: 0.5914, G_loss: 4.6758\n",
      "Epoch [8/12], Step [33/74], D_loss: 0.7741, G_loss: 7.5216\n",
      "Epoch [8/12], Step [34/74], D_loss: 0.4836, G_loss: 7.4058\n",
      "Epoch [8/12], Step [35/74], D_loss: 0.6314, G_loss: 8.4225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/12], Step [36/74], D_loss: 0.6878, G_loss: 6.0949\n",
      "Epoch [8/12], Step [37/74], D_loss: 1.0966, G_loss: 8.0461\n",
      "Epoch [8/12], Step [38/74], D_loss: 0.5147, G_loss: 6.6833\n",
      "Epoch [8/12], Step [39/74], D_loss: 0.6271, G_loss: 8.3480\n",
      "Epoch [8/12], Step [40/74], D_loss: 0.8736, G_loss: 12.9121\n",
      "Epoch [8/12], Step [41/74], D_loss: 1.0000, G_loss: 17.4640\n",
      "Epoch [8/12], Step [42/74], D_loss: 0.6725, G_loss: 10.1333\n",
      "Epoch [8/12], Step [43/74], D_loss: 0.7200, G_loss: 3.8886\n",
      "Epoch [8/12], Step [44/74], D_loss: 0.7295, G_loss: 9.7493\n",
      "Epoch [8/12], Step [45/74], D_loss: 0.5711, G_loss: 9.2779\n",
      "Epoch [8/12], Step [46/74], D_loss: 0.3516, G_loss: 15.7249\n",
      "Epoch [8/12], Step [47/74], D_loss: 0.5459, G_loss: 8.5758\n",
      "Epoch [8/12], Step [48/74], D_loss: 0.9416, G_loss: 4.1107\n",
      "Epoch [8/12], Step [49/74], D_loss: 1.1392, G_loss: 8.8272\n",
      "Epoch [8/12], Step [50/74], D_loss: 0.9394, G_loss: 10.1726\n",
      "Epoch [8/12], Step [51/74], D_loss: 1.1061, G_loss: 6.6870\n",
      "Epoch [8/12], Step [52/74], D_loss: 0.6304, G_loss: 9.7014\n",
      "Epoch [8/12], Step [53/74], D_loss: 0.5565, G_loss: 5.4893\n",
      "Epoch [8/12], Step [54/74], D_loss: 0.6334, G_loss: 12.0854\n",
      "Epoch [8/12], Step [55/74], D_loss: 0.7418, G_loss: 11.0386\n",
      "Epoch [8/12], Step [56/74], D_loss: 0.8771, G_loss: 3.7777\n",
      "Epoch [8/12], Step [57/74], D_loss: 0.6487, G_loss: 10.3304\n",
      "Epoch [8/12], Step [58/74], D_loss: 0.7178, G_loss: 9.3568\n",
      "Epoch [8/12], Step [59/74], D_loss: 0.5806, G_loss: 7.7813\n",
      "Epoch [8/12], Step [60/74], D_loss: 0.6587, G_loss: 4.8486\n",
      "Epoch [8/12], Step [61/74], D_loss: 1.0290, G_loss: 3.8265\n",
      "Epoch [8/12], Step [62/74], D_loss: 0.7361, G_loss: 9.4684\n",
      "Epoch [8/12], Step [63/74], D_loss: 0.7948, G_loss: 4.7298\n",
      "Epoch [8/12], Step [64/74], D_loss: 0.6605, G_loss: 6.7942\n",
      "Epoch [8/12], Step [65/74], D_loss: 0.4980, G_loss: 4.2240\n",
      "Epoch [8/12], Step [66/74], D_loss: 0.7179, G_loss: 4.7405\n",
      "Epoch [8/12], Step [67/74], D_loss: 0.6518, G_loss: 10.9209\n",
      "Epoch [8/12], Step [68/74], D_loss: 1.1568, G_loss: 7.4778\n",
      "Epoch [8/12], Step [69/74], D_loss: 0.7995, G_loss: 5.3262\n",
      "Epoch [8/12], Step [70/74], D_loss: 0.7974, G_loss: 5.2825\n",
      "Epoch [8/12], Step [71/74], D_loss: 0.7698, G_loss: 6.0848\n",
      "Epoch [8/12], Step [72/74], D_loss: 0.5064, G_loss: 14.0212\n",
      "Epoch [8/12], Step [73/74], D_loss: 0.8081, G_loss: 20.8240\n",
      "Epoch [8/12], Step [74/74], D_loss: 0.7926, G_loss: 16.5457\n",
      "Epoch [9/12], Step [1/74], D_loss: 1.0508, G_loss: 8.5597\n",
      "Epoch [9/12], Step [2/74], D_loss: 0.7637, G_loss: 6.8161\n",
      "Epoch [9/12], Step [3/74], D_loss: 0.7274, G_loss: 8.6456\n",
      "Epoch [9/12], Step [4/74], D_loss: 0.7627, G_loss: 5.8205\n",
      "Epoch [9/12], Step [5/74], D_loss: 0.5997, G_loss: 4.6151\n",
      "Epoch [9/12], Step [6/74], D_loss: 0.6492, G_loss: 5.1172\n",
      "Epoch [9/12], Step [7/74], D_loss: 0.5304, G_loss: 12.1289\n",
      "Epoch [9/12], Step [8/74], D_loss: 0.7000, G_loss: 10.9978\n",
      "Epoch [9/12], Step [9/74], D_loss: 0.5385, G_loss: 6.1520\n",
      "Epoch [9/12], Step [10/74], D_loss: 0.5359, G_loss: 6.8873\n",
      "Epoch [9/12], Step [11/74], D_loss: 0.5458, G_loss: 7.9344\n",
      "Epoch [9/12], Step [12/74], D_loss: 0.4784, G_loss: 11.0744\n",
      "Epoch [9/12], Step [13/74], D_loss: 0.5635, G_loss: 14.9187\n",
      "Epoch [9/12], Step [14/74], D_loss: 0.6174, G_loss: 9.3957\n",
      "Epoch [9/12], Step [15/74], D_loss: 0.7215, G_loss: 12.8907\n",
      "Epoch [9/12], Step [16/74], D_loss: 0.9511, G_loss: 9.6383\n",
      "Epoch [9/12], Step [17/74], D_loss: 1.0162, G_loss: 10.8588\n",
      "Epoch [9/12], Step [18/74], D_loss: 1.0226, G_loss: 10.1159\n",
      "Epoch [9/12], Step [19/74], D_loss: 0.9931, G_loss: 10.4409\n",
      "Epoch [9/12], Step [20/74], D_loss: 0.7130, G_loss: 8.9687\n",
      "Epoch [9/12], Step [21/74], D_loss: 0.8344, G_loss: 7.2758\n",
      "Epoch [9/12], Step [22/74], D_loss: 0.7113, G_loss: 4.0888\n",
      "Epoch [9/12], Step [23/74], D_loss: 0.7505, G_loss: 8.7121\n",
      "Epoch [9/12], Step [24/74], D_loss: 0.9888, G_loss: 5.5304\n",
      "Epoch [9/12], Step [25/74], D_loss: 0.6804, G_loss: 7.5925\n",
      "Epoch [9/12], Step [26/74], D_loss: 0.7897, G_loss: 7.0090\n",
      "Epoch [9/12], Step [27/74], D_loss: 0.8823, G_loss: 10.8878\n",
      "Epoch [9/12], Step [28/74], D_loss: 0.6960, G_loss: 9.8904\n",
      "Epoch [9/12], Step [29/74], D_loss: 0.6520, G_loss: 10.2160\n",
      "Epoch [9/12], Step [30/74], D_loss: 0.6895, G_loss: 10.6040\n",
      "Epoch [9/12], Step [31/74], D_loss: 0.6658, G_loss: 7.9099\n",
      "Epoch [9/12], Step [32/74], D_loss: 0.7376, G_loss: 5.4389\n",
      "Epoch [9/12], Step [33/74], D_loss: 0.8021, G_loss: 7.7957\n",
      "Epoch [9/12], Step [34/74], D_loss: 0.6804, G_loss: 11.3612\n",
      "Epoch [9/12], Step [35/74], D_loss: 0.5778, G_loss: 4.4853\n",
      "Epoch [9/12], Step [36/74], D_loss: 0.6436, G_loss: 10.5768\n",
      "Epoch [9/12], Step [37/74], D_loss: 0.7596, G_loss: 6.2103\n",
      "Epoch [9/12], Step [38/74], D_loss: 0.9803, G_loss: 10.2008\n",
      "Epoch [9/12], Step [39/74], D_loss: 0.6516, G_loss: 6.6972\n",
      "Epoch [9/12], Step [40/74], D_loss: 0.7336, G_loss: 6.5548\n",
      "Epoch [9/12], Step [41/74], D_loss: 0.6034, G_loss: 6.5789\n",
      "Epoch [9/12], Step [42/74], D_loss: 0.5912, G_loss: 9.1663\n",
      "Epoch [9/12], Step [43/74], D_loss: 0.7338, G_loss: 7.6941\n",
      "Epoch [9/12], Step [44/74], D_loss: 0.4854, G_loss: 12.6961\n",
      "Epoch [9/12], Step [45/74], D_loss: 0.8098, G_loss: 7.6919\n",
      "Epoch [9/12], Step [46/74], D_loss: 0.6700, G_loss: 5.0880\n",
      "Epoch [9/12], Step [47/74], D_loss: 0.7361, G_loss: 8.0151\n",
      "Epoch [9/12], Step [48/74], D_loss: 0.7114, G_loss: 15.6700\n",
      "Epoch [9/12], Step [49/74], D_loss: 0.6706, G_loss: 5.4754\n",
      "Epoch [9/12], Step [50/74], D_loss: 0.8177, G_loss: 9.7915\n",
      "Epoch [9/12], Step [51/74], D_loss: 0.5040, G_loss: 10.8186\n",
      "Epoch [9/12], Step [52/74], D_loss: 0.6206, G_loss: 7.5430\n",
      "Epoch [9/12], Step [53/74], D_loss: 0.8484, G_loss: 6.5696\n",
      "Epoch [9/12], Step [54/74], D_loss: 0.7678, G_loss: 7.7931\n",
      "Epoch [9/12], Step [55/74], D_loss: 0.6205, G_loss: 5.1061\n",
      "Epoch [9/12], Step [56/74], D_loss: 0.5919, G_loss: 6.7723\n",
      "Epoch [9/12], Step [57/74], D_loss: 0.4078, G_loss: 14.7031\n",
      "Epoch [9/12], Step [58/74], D_loss: 0.7627, G_loss: 8.3239\n",
      "Epoch [9/12], Step [59/74], D_loss: 0.7278, G_loss: 4.6930\n",
      "Epoch [9/12], Step [60/74], D_loss: 1.1193, G_loss: 4.3447\n",
      "Epoch [9/12], Step [61/74], D_loss: 0.8011, G_loss: 6.0842\n",
      "Epoch [9/12], Step [62/74], D_loss: 0.8169, G_loss: 4.6542\n",
      "Epoch [9/12], Step [63/74], D_loss: 0.5738, G_loss: 6.2741\n",
      "Epoch [9/12], Step [64/74], D_loss: 0.5974, G_loss: 6.2742\n",
      "Epoch [9/12], Step [65/74], D_loss: 0.6925, G_loss: 7.6257\n",
      "Epoch [9/12], Step [66/74], D_loss: 0.5625, G_loss: 6.0069\n",
      "Epoch [9/12], Step [67/74], D_loss: 0.4735, G_loss: 6.1179\n",
      "Epoch [9/12], Step [68/74], D_loss: 0.4649, G_loss: 6.1410\n",
      "Epoch [9/12], Step [69/74], D_loss: 0.3384, G_loss: 7.9410\n",
      "Epoch [9/12], Step [70/74], D_loss: 0.5492, G_loss: 4.8166\n",
      "Epoch [9/12], Step [71/74], D_loss: 0.9307, G_loss: 9.0548\n",
      "Epoch [9/12], Step [72/74], D_loss: 0.8736, G_loss: 9.1781\n",
      "Epoch [9/12], Step [73/74], D_loss: 0.7912, G_loss: 7.5570\n",
      "Epoch [9/12], Step [74/74], D_loss: 0.9288, G_loss: 3.6236\n",
      "Epoch [10/12], Step [1/74], D_loss: 0.7595, G_loss: 7.4409\n",
      "Epoch [10/12], Step [2/74], D_loss: 0.9311, G_loss: 5.7306\n",
      "Epoch [10/12], Step [3/74], D_loss: 0.8725, G_loss: 3.0533\n",
      "Epoch [10/12], Step [4/74], D_loss: 0.8197, G_loss: 7.0800\n",
      "Epoch [10/12], Step [5/74], D_loss: 0.6544, G_loss: 10.6135\n",
      "Epoch [10/12], Step [6/74], D_loss: 0.6964, G_loss: 6.5872\n",
      "Epoch [10/12], Step [7/74], D_loss: 0.7717, G_loss: 12.7676\n",
      "Epoch [10/12], Step [8/74], D_loss: 0.6450, G_loss: 6.0481\n",
      "Epoch [10/12], Step [9/74], D_loss: 0.6279, G_loss: 11.0464\n",
      "Epoch [10/12], Step [10/74], D_loss: 0.6790, G_loss: 5.8704\n",
      "Epoch [10/12], Step [11/74], D_loss: 0.5640, G_loss: 6.6634\n",
      "Epoch [10/12], Step [12/74], D_loss: 0.6756, G_loss: 10.6324\n",
      "Epoch [10/12], Step [13/74], D_loss: 0.6577, G_loss: 6.4726\n",
      "Epoch [10/12], Step [14/74], D_loss: 0.6967, G_loss: 8.0912\n",
      "Epoch [10/12], Step [15/74], D_loss: 0.7142, G_loss: 11.3976\n",
      "Epoch [10/12], Step [16/74], D_loss: 0.5284, G_loss: 9.2398\n",
      "Epoch [10/12], Step [17/74], D_loss: 0.6011, G_loss: 8.0256\n",
      "Epoch [10/12], Step [18/74], D_loss: 0.7229, G_loss: 7.2668\n",
      "Epoch [10/12], Step [19/74], D_loss: 0.6666, G_loss: 10.8303\n",
      "Epoch [10/12], Step [20/74], D_loss: 0.5652, G_loss: 6.1332\n",
      "Epoch [10/12], Step [21/74], D_loss: 0.8909, G_loss: 5.1719\n",
      "Epoch [10/12], Step [22/74], D_loss: 0.6353, G_loss: 6.1159\n",
      "Epoch [10/12], Step [23/74], D_loss: 0.5433, G_loss: 7.0440\n",
      "Epoch [10/12], Step [24/74], D_loss: 0.6946, G_loss: 6.6112\n",
      "Epoch [10/12], Step [25/74], D_loss: 0.5999, G_loss: 5.1774\n",
      "Epoch [10/12], Step [26/74], D_loss: 0.6196, G_loss: 6.4847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/12], Step [27/74], D_loss: 0.6183, G_loss: 4.4356\n",
      "Epoch [10/12], Step [28/74], D_loss: 0.7104, G_loss: 5.7274\n",
      "Epoch [10/12], Step [29/74], D_loss: 0.6678, G_loss: 12.2581\n",
      "Epoch [10/12], Step [30/74], D_loss: 0.5930, G_loss: 5.6117\n",
      "Epoch [10/12], Step [31/74], D_loss: 0.8921, G_loss: 5.6545\n",
      "Epoch [10/12], Step [32/74], D_loss: 0.7874, G_loss: 6.6932\n",
      "Epoch [10/12], Step [33/74], D_loss: 0.7563, G_loss: 7.2058\n",
      "Epoch [10/12], Step [34/74], D_loss: 0.7804, G_loss: 7.7556\n",
      "Epoch [10/12], Step [35/74], D_loss: 0.6069, G_loss: 7.7265\n",
      "Epoch [10/12], Step [36/74], D_loss: 0.4801, G_loss: 9.7121\n",
      "Epoch [10/12], Step [37/74], D_loss: 0.8368, G_loss: 4.7090\n",
      "Epoch [10/12], Step [38/74], D_loss: 0.6917, G_loss: 5.6494\n",
      "Epoch [10/12], Step [39/74], D_loss: 0.5479, G_loss: 9.2519\n",
      "Epoch [10/12], Step [40/74], D_loss: 0.4741, G_loss: 7.2720\n",
      "Epoch [10/12], Step [41/74], D_loss: 0.7977, G_loss: 17.9827\n",
      "Epoch [10/12], Step [42/74], D_loss: 0.6317, G_loss: 12.1301\n",
      "Epoch [10/12], Step [43/74], D_loss: 0.8574, G_loss: 5.2666\n",
      "Epoch [10/12], Step [44/74], D_loss: 0.8475, G_loss: 7.2904\n",
      "Epoch [10/12], Step [45/74], D_loss: 0.8489, G_loss: 5.9854\n",
      "Epoch [10/12], Step [46/74], D_loss: 0.7294, G_loss: 6.3952\n",
      "Epoch [10/12], Step [47/74], D_loss: 0.6956, G_loss: 7.5758\n",
      "Epoch [10/12], Step [48/74], D_loss: 0.7737, G_loss: 7.7859\n",
      "Epoch [10/12], Step [49/74], D_loss: 0.7533, G_loss: 6.0021\n",
      "Epoch [10/12], Step [50/74], D_loss: 0.9155, G_loss: 10.8829\n",
      "Epoch [10/12], Step [51/74], D_loss: 0.6751, G_loss: 5.9574\n",
      "Epoch [10/12], Step [52/74], D_loss: 0.5718, G_loss: 5.2770\n",
      "Epoch [10/12], Step [53/74], D_loss: 0.7488, G_loss: 5.2317\n",
      "Epoch [10/12], Step [54/74], D_loss: 0.7968, G_loss: 11.1442\n",
      "Epoch [10/12], Step [55/74], D_loss: 0.4709, G_loss: 10.9923\n",
      "Epoch [10/12], Step [56/74], D_loss: 0.5966, G_loss: 8.2446\n",
      "Epoch [10/12], Step [57/74], D_loss: 0.7439, G_loss: 3.3222\n",
      "Epoch [10/12], Step [58/74], D_loss: 0.8560, G_loss: 4.7259\n",
      "Epoch [10/12], Step [59/74], D_loss: 0.7400, G_loss: 7.1956\n",
      "Epoch [10/12], Step [60/74], D_loss: 0.7520, G_loss: 8.0449\n",
      "Epoch [10/12], Step [61/74], D_loss: 0.6805, G_loss: 6.5883\n",
      "Epoch [10/12], Step [62/74], D_loss: 0.5519, G_loss: 10.3974\n",
      "Epoch [10/12], Step [63/74], D_loss: 0.5674, G_loss: 6.2731\n",
      "Epoch [10/12], Step [64/74], D_loss: 0.6698, G_loss: 7.2193\n",
      "Epoch [10/12], Step [65/74], D_loss: 0.8472, G_loss: 4.8543\n",
      "Epoch [10/12], Step [66/74], D_loss: 0.5345, G_loss: 7.8928\n",
      "Epoch [10/12], Step [67/74], D_loss: 0.4865, G_loss: 5.6447\n",
      "Epoch [10/12], Step [68/74], D_loss: 0.6910, G_loss: 7.4812\n",
      "Epoch [10/12], Step [69/74], D_loss: 0.6591, G_loss: 10.1814\n",
      "Epoch [10/12], Step [70/74], D_loss: 0.9186, G_loss: 5.5419\n",
      "Epoch [10/12], Step [71/74], D_loss: 0.5380, G_loss: 4.5529\n",
      "Epoch [10/12], Step [72/74], D_loss: 0.7575, G_loss: 8.3881\n",
      "Epoch [10/12], Step [73/74], D_loss: 0.7643, G_loss: 7.0623\n",
      "Epoch [10/12], Step [74/74], D_loss: 0.8340, G_loss: 6.6280\n",
      "Epoch [11/12], Step [1/74], D_loss: 0.6743, G_loss: 7.1138\n",
      "Epoch [11/12], Step [2/74], D_loss: 0.6314, G_loss: 6.2108\n",
      "Epoch [11/12], Step [3/74], D_loss: 0.5301, G_loss: 6.1794\n",
      "Epoch [11/12], Step [4/74], D_loss: 0.3804, G_loss: 12.6758\n",
      "Epoch [11/12], Step [5/74], D_loss: 0.4218, G_loss: 6.1991\n",
      "Epoch [11/12], Step [6/74], D_loss: 0.9486, G_loss: 6.3954\n",
      "Epoch [11/12], Step [7/74], D_loss: 0.7951, G_loss: 8.0586\n",
      "Epoch [11/12], Step [8/74], D_loss: 0.8842, G_loss: 3.7277\n",
      "Epoch [11/12], Step [9/74], D_loss: 0.5490, G_loss: 6.8430\n",
      "Epoch [11/12], Step [10/74], D_loss: 0.7500, G_loss: 6.9044\n",
      "Epoch [11/12], Step [11/74], D_loss: 0.9540, G_loss: 4.6054\n",
      "Epoch [11/12], Step [12/74], D_loss: 0.7471, G_loss: 5.5209\n",
      "Epoch [11/12], Step [13/74], D_loss: 0.9412, G_loss: 5.0233\n",
      "Epoch [11/12], Step [14/74], D_loss: 0.7397, G_loss: 4.2480\n",
      "Epoch [11/12], Step [15/74], D_loss: 0.7807, G_loss: 3.9843\n",
      "Epoch [11/12], Step [16/74], D_loss: 0.6818, G_loss: 10.5928\n",
      "Epoch [11/12], Step [17/74], D_loss: 0.7063, G_loss: 5.3699\n",
      "Epoch [11/12], Step [18/74], D_loss: 0.6496, G_loss: 6.4133\n",
      "Epoch [11/12], Step [19/74], D_loss: 0.5479, G_loss: 4.6570\n",
      "Epoch [11/12], Step [20/74], D_loss: 0.4919, G_loss: 8.0778\n",
      "Epoch [11/12], Step [21/74], D_loss: 0.7756, G_loss: 6.6113\n",
      "Epoch [11/12], Step [22/74], D_loss: 0.4601, G_loss: 5.9516\n",
      "Epoch [11/12], Step [23/74], D_loss: 0.5195, G_loss: 6.2846\n",
      "Epoch [11/12], Step [24/74], D_loss: 0.7338, G_loss: 5.3375\n",
      "Epoch [11/12], Step [25/74], D_loss: 0.7364, G_loss: 5.5046\n",
      "Epoch [11/12], Step [26/74], D_loss: 0.9490, G_loss: 5.1661\n",
      "Epoch [11/12], Step [27/74], D_loss: 0.6799, G_loss: 5.8101\n",
      "Epoch [11/12], Step [28/74], D_loss: 0.4359, G_loss: 7.5617\n",
      "Epoch [11/12], Step [29/74], D_loss: 0.6171, G_loss: 7.5972\n",
      "Epoch [11/12], Step [30/74], D_loss: 0.4941, G_loss: 5.8742\n",
      "Epoch [11/12], Step [31/74], D_loss: 0.9267, G_loss: 8.6245\n",
      "Epoch [11/12], Step [32/74], D_loss: 0.4686, G_loss: 8.7668\n",
      "Epoch [11/12], Step [33/74], D_loss: 0.6526, G_loss: 5.7413\n",
      "Epoch [11/12], Step [34/74], D_loss: 0.5730, G_loss: 8.3867\n",
      "Epoch [11/12], Step [35/74], D_loss: 0.4905, G_loss: 9.1074\n",
      "Epoch [11/12], Step [36/74], D_loss: 0.5481, G_loss: 6.6333\n",
      "Epoch [11/12], Step [37/74], D_loss: 0.6705, G_loss: 9.0100\n",
      "Epoch [11/12], Step [38/74], D_loss: 0.8047, G_loss: 5.5448\n",
      "Epoch [11/12], Step [39/74], D_loss: 0.4828, G_loss: 5.7185\n",
      "Epoch [11/12], Step [40/74], D_loss: 0.5081, G_loss: 4.3947\n",
      "Epoch [11/12], Step [41/74], D_loss: 0.7719, G_loss: 4.9550\n",
      "Epoch [11/12], Step [42/74], D_loss: 0.6749, G_loss: 4.8545\n",
      "Epoch [11/12], Step [43/74], D_loss: 0.8666, G_loss: 3.4681\n",
      "Epoch [11/12], Step [44/74], D_loss: 0.8092, G_loss: 4.3016\n",
      "Epoch [11/12], Step [45/74], D_loss: 0.7438, G_loss: 6.2136\n",
      "Epoch [11/12], Step [46/74], D_loss: 0.5243, G_loss: 17.6101\n",
      "Epoch [11/12], Step [47/74], D_loss: 0.5505, G_loss: 10.2532\n",
      "Epoch [11/12], Step [48/74], D_loss: 0.4546, G_loss: 6.2053\n",
      "Epoch [11/12], Step [49/74], D_loss: 0.6915, G_loss: 8.7392\n",
      "Epoch [11/12], Step [50/74], D_loss: 0.6183, G_loss: 8.7854\n",
      "Epoch [11/12], Step [51/74], D_loss: 0.5169, G_loss: 10.2042\n",
      "Epoch [11/12], Step [52/74], D_loss: 0.7308, G_loss: 5.8976\n",
      "Epoch [11/12], Step [53/74], D_loss: 0.9544, G_loss: 4.8291\n",
      "Epoch [11/12], Step [54/74], D_loss: 0.7845, G_loss: 6.5176\n",
      "Epoch [11/12], Step [55/74], D_loss: 0.8053, G_loss: 5.0618\n",
      "Epoch [11/12], Step [56/74], D_loss: 0.7243, G_loss: 9.2585\n",
      "Epoch [11/12], Step [57/74], D_loss: 0.5676, G_loss: 5.1208\n",
      "Epoch [11/12], Step [58/74], D_loss: 0.5106, G_loss: 5.7347\n",
      "Epoch [11/12], Step [59/74], D_loss: 0.5478, G_loss: 7.8954\n",
      "Epoch [11/12], Step [60/74], D_loss: 0.3911, G_loss: 5.8183\n",
      "Epoch [11/12], Step [61/74], D_loss: 0.4760, G_loss: 4.9596\n",
      "Epoch [11/12], Step [62/74], D_loss: 0.6149, G_loss: 5.6592\n",
      "Epoch [11/12], Step [63/74], D_loss: 0.2861, G_loss: 9.1386\n",
      "Epoch [11/12], Step [64/74], D_loss: 1.2725, G_loss: 16.0249\n",
      "Epoch [11/12], Step [65/74], D_loss: 1.4604, G_loss: 4.0440\n",
      "Epoch [11/12], Step [66/74], D_loss: 1.0395, G_loss: 9.7746\n",
      "Epoch [11/12], Step [67/74], D_loss: 0.8046, G_loss: 9.5829\n",
      "Epoch [11/12], Step [68/74], D_loss: 0.7921, G_loss: 5.7994\n",
      "Epoch [11/12], Step [69/74], D_loss: 0.6835, G_loss: 4.1733\n",
      "Epoch [11/12], Step [70/74], D_loss: 0.6664, G_loss: 9.1490\n",
      "Epoch [11/12], Step [71/74], D_loss: 0.7028, G_loss: 8.1324\n",
      "Epoch [11/12], Step [72/74], D_loss: 0.6477, G_loss: 5.7898\n",
      "Epoch [11/12], Step [73/74], D_loss: 0.7022, G_loss: 8.6708\n",
      "Epoch [11/12], Step [74/74], D_loss: 0.6807, G_loss: 10.2388\n",
      "Epoch [12/12], Step [1/74], D_loss: 0.6603, G_loss: 4.1630\n",
      "Epoch [12/12], Step [2/74], D_loss: 0.7263, G_loss: 5.7853\n",
      "Epoch [12/12], Step [3/74], D_loss: 0.7138, G_loss: 8.0902\n",
      "Epoch [12/12], Step [4/74], D_loss: 0.7435, G_loss: 4.9880\n",
      "Epoch [12/12], Step [5/74], D_loss: 0.8795, G_loss: 3.1227\n",
      "Epoch [12/12], Step [6/74], D_loss: 0.7987, G_loss: 7.6706\n",
      "Epoch [12/12], Step [7/74], D_loss: 0.6990, G_loss: 5.0298\n",
      "Epoch [12/12], Step [8/74], D_loss: 0.7674, G_loss: 12.2068\n",
      "Epoch [12/12], Step [9/74], D_loss: 0.6652, G_loss: 9.5188\n",
      "Epoch [12/12], Step [10/74], D_loss: 0.6822, G_loss: 7.0066\n",
      "Epoch [12/12], Step [11/74], D_loss: 0.7020, G_loss: 4.0405\n",
      "Epoch [12/12], Step [12/74], D_loss: 0.7507, G_loss: 5.3840\n",
      "Epoch [12/12], Step [13/74], D_loss: 0.7493, G_loss: 5.5255\n",
      "Epoch [12/12], Step [14/74], D_loss: 0.4957, G_loss: 7.2406\n",
      "Epoch [12/12], Step [15/74], D_loss: 0.7383, G_loss: 7.4224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/12], Step [16/74], D_loss: 0.5773, G_loss: 8.4710\n",
      "Epoch [12/12], Step [17/74], D_loss: 0.8332, G_loss: 9.0241\n",
      "Epoch [12/12], Step [18/74], D_loss: 0.8402, G_loss: 4.3470\n",
      "Epoch [12/12], Step [19/74], D_loss: 0.5848, G_loss: 6.6491\n",
      "Epoch [12/12], Step [20/74], D_loss: 0.6901, G_loss: 4.3593\n",
      "Epoch [12/12], Step [21/74], D_loss: 0.5334, G_loss: 7.0920\n",
      "Epoch [12/12], Step [22/74], D_loss: 0.5670, G_loss: 3.7653\n",
      "Epoch [12/12], Step [23/74], D_loss: 0.5192, G_loss: 7.6613\n",
      "Epoch [12/12], Step [24/74], D_loss: 0.4774, G_loss: 9.5605\n",
      "Epoch [12/12], Step [25/74], D_loss: 0.5175, G_loss: 6.2934\n",
      "Epoch [12/12], Step [26/74], D_loss: 0.5838, G_loss: 6.2299\n",
      "Epoch [12/12], Step [27/74], D_loss: 0.3385, G_loss: 11.3816\n",
      "Epoch [12/12], Step [28/74], D_loss: 0.8856, G_loss: 6.7797\n",
      "Epoch [12/12], Step [29/74], D_loss: 0.8156, G_loss: 5.3041\n",
      "Epoch [12/12], Step [30/74], D_loss: 0.6974, G_loss: 7.4900\n",
      "Epoch [12/12], Step [31/74], D_loss: 0.7089, G_loss: 4.5035\n",
      "Epoch [12/12], Step [32/74], D_loss: 0.6439, G_loss: 9.1748\n",
      "Epoch [12/12], Step [33/74], D_loss: 0.7171, G_loss: 3.9809\n",
      "Epoch [12/12], Step [34/74], D_loss: 0.5935, G_loss: 12.6762\n",
      "Epoch [12/12], Step [35/74], D_loss: 0.5403, G_loss: 7.5279\n",
      "Epoch [12/12], Step [36/74], D_loss: 0.3799, G_loss: 10.7392\n",
      "Epoch [12/12], Step [37/74], D_loss: 0.6326, G_loss: 5.3047\n",
      "Epoch [12/12], Step [38/74], D_loss: 0.6978, G_loss: 7.3419\n",
      "Epoch [12/12], Step [39/74], D_loss: 0.7283, G_loss: 7.5791\n",
      "Epoch [12/12], Step [40/74], D_loss: 0.5630, G_loss: 9.8118\n",
      "Epoch [12/12], Step [41/74], D_loss: 0.6888, G_loss: 5.6778\n",
      "Epoch [12/12], Step [42/74], D_loss: 0.3034, G_loss: 7.9666\n",
      "Epoch [12/12], Step [43/74], D_loss: 0.6663, G_loss: 4.5997\n",
      "Epoch [12/12], Step [44/74], D_loss: 0.4737, G_loss: 8.4242\n",
      "Epoch [12/12], Step [45/74], D_loss: 0.8545, G_loss: 6.4539\n",
      "Epoch [12/12], Step [46/74], D_loss: 0.6829, G_loss: 9.6495\n",
      "Epoch [12/12], Step [47/74], D_loss: 0.6458, G_loss: 6.3207\n",
      "Epoch [12/12], Step [48/74], D_loss: 0.6205, G_loss: 5.4462\n",
      "Epoch [12/12], Step [49/74], D_loss: 0.6834, G_loss: 5.7447\n",
      "Epoch [12/12], Step [50/74], D_loss: 0.7207, G_loss: 9.3665\n",
      "Epoch [12/12], Step [51/74], D_loss: 0.8039, G_loss: 7.5154\n",
      "Epoch [12/12], Step [52/74], D_loss: 0.6827, G_loss: 5.1365\n",
      "Epoch [12/12], Step [53/74], D_loss: 0.5171, G_loss: 6.5325\n",
      "Epoch [12/12], Step [54/74], D_loss: 0.3830, G_loss: 5.7713\n",
      "Epoch [12/12], Step [55/74], D_loss: 0.7581, G_loss: 5.2619\n",
      "Epoch [12/12], Step [56/74], D_loss: 0.8369, G_loss: 5.6629\n",
      "Epoch [12/12], Step [57/74], D_loss: 0.3706, G_loss: 5.8881\n",
      "Epoch [12/12], Step [58/74], D_loss: 0.4075, G_loss: 10.1208\n",
      "Epoch [12/12], Step [59/74], D_loss: 0.7456, G_loss: 4.4677\n",
      "Epoch [12/12], Step [60/74], D_loss: 0.5795, G_loss: 9.0952\n",
      "Epoch [12/12], Step [61/74], D_loss: 0.7709, G_loss: 14.6453\n",
      "Epoch [12/12], Step [62/74], D_loss: 0.7356, G_loss: 6.1911\n",
      "Epoch [12/12], Step [63/74], D_loss: 0.5184, G_loss: 5.4399\n",
      "Epoch [12/12], Step [64/74], D_loss: 0.5096, G_loss: 6.7947\n",
      "Epoch [12/12], Step [65/74], D_loss: 0.5512, G_loss: 5.3808\n",
      "Epoch [12/12], Step [66/74], D_loss: 0.6070, G_loss: 5.7727\n",
      "Epoch [12/12], Step [67/74], D_loss: 0.7312, G_loss: 4.8943\n",
      "Epoch [12/12], Step [68/74], D_loss: 0.6238, G_loss: 4.7004\n",
      "Epoch [12/12], Step [69/74], D_loss: 0.8590, G_loss: 6.0229\n",
      "Epoch [12/12], Step [70/74], D_loss: 0.9340, G_loss: 5.2826\n",
      "Epoch [12/12], Step [71/74], D_loss: 0.7228, G_loss: 4.7316\n",
      "Epoch [12/12], Step [72/74], D_loss: 0.5789, G_loss: 9.2949\n",
      "Epoch [12/12], Step [73/74], D_loss: 0.6358, G_loss: 6.0694\n",
      "Epoch [12/12], Step [74/74], D_loss: 0.5305, G_loss: 12.9049\n"
     ]
    }
   ],
   "source": [
    "# Training GAN\n",
    "D_avg_losses = []\n",
    "G_avg_losses = []\n",
    "num_epochs=10\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    epoch_dir=ex_dir+str(epoch)+\"/\"\n",
    "    if not os.path.exists(epoch_dir):\n",
    "        os.mkdir(epoch_dir)\n",
    "\n",
    "\n",
    "    epoch_im_dir=epoch_dir+\"images/\"\n",
    "\n",
    "\n",
    "    epoch_msk_dir=epoch_dir+\"masks/\"\n",
    "\n",
    "\n",
    "    epoch_genmsk_dir=epoch_dir+\"gen_masks/\"\n",
    "    if not os.path.exists(epoch_genmsk_dir):\n",
    "        os.mkdir(epoch_genmsk_dir)\n",
    "\n",
    "\n",
    "    epoch_feat_dir=epoch_dir+\"features/\"\n",
    "    if not os.path.exists(epoch_feat_dir):\n",
    "        os.mkdir(epoch_feat_dir)\n",
    "\n",
    "    D_losses = []\n",
    "    G_losses = []\n",
    "\n",
    "    # training\n",
    "    for i, (input, target, im_file_names, msk_file_names) in enumerate(train_loader):\n",
    "\n",
    "        # input & target image data\n",
    "        x_ = Variable(input)#.permute(0,3,1,2)#Variable(input.cuda())\n",
    "        y_ = Variable(target)#.permute(0,3,1,2)#Variable(target.cuda())\n",
    "\n",
    "        x_ = x_.float()#Variable(input.cuda())\n",
    "        y_ = y_.float()#Variable(target.cuda())\n",
    "\n",
    "        # Train discriminator with real data\n",
    "        D_real_decision = D(x_, y_).squeeze()\n",
    "        real_ = Variable(torch.ones(D_real_decision.size())) #Variable(torch.ones(D_real_decision.size()).cuda())\n",
    "        D_real_loss = BCE_loss(D_real_decision, real_)\n",
    "\n",
    "        # Train discriminator with fake data\n",
    "        gen_image, _ = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        fake_ = Variable(torch.zeros(D_fake_decision.size()))#Variable(torch.zeros(D_fake_decision.size()).cuda())\n",
    "        D_fake_loss = BCE_loss(D_fake_decision, fake_)\n",
    "\n",
    "        # Back propagation\n",
    "        D_loss = (D_real_loss + D_fake_loss) * 0.5\n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        # Train generator\n",
    "#         gen_image, features = G(x_)\n",
    "        gen_image, _ = G(x_)\n",
    "        D_fake_decision = D(x_, gen_image).squeeze()\n",
    "        G_fake_loss = BCE_loss(D_fake_decision, real_)\n",
    "\n",
    "        # L1 loss\n",
    "        l1_loss = lamb * L1_loss(gen_image, y_)\n",
    "\n",
    "        # Back propagation\n",
    "        G_loss = G_fake_loss + l1_loss\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        # loss values\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "        \n",
    "        #features_proj=F.interpolate(features,scale_factor=(2,2), mode='nearest')\n",
    "\n",
    "        print('Epoch [%d/%d], Step [%d/%d], D_loss: %.4f, G_loss: %.4f'\n",
    "              % (epoch+1, num_epochs, i+1, len(train_loader), D_loss.item(), G_loss.item()))\n",
    "        \n",
    "\n",
    "#         if epoch>=0:\n",
    "#             save_epoch_pairs(x_,\n",
    "#                              y_,\n",
    "#                              gen_image,\n",
    "#                              features_proj,\n",
    "#                              im_file_names,\n",
    "#                              msk_file_names,\n",
    "#                              epoch_im_dir,\n",
    "#                              epoch_msk_dir,\n",
    "#                              epoch_genmsk_dir,\n",
    "#                              epoch_feat_dir)\n",
    "\n",
    "        \n",
    "        if epoch>=4:\n",
    "            save_epoch_pairs(x_,\n",
    "                             y_,\n",
    "                             gen_image,\n",
    "                             im_file_names,\n",
    "                             msk_file_names,\n",
    "                             epoch_im_dir,\n",
    "                             epoch_msk_dir,\n",
    "                             epoch_genmsk_dir,\n",
    "                             epoch_feat_dir)\n",
    "\n",
    "        \n",
    "#         save_epoch_pairs(imgs_batch,\n",
    "#                      masks_batch,\n",
    "#                      gen_masks_batch, \n",
    "#                      im_fn_batch,\n",
    "#                      msk_fn_batch,\n",
    "#                      im_dir_name,\n",
    "#                      msk_dir_name,\n",
    "#                      gen_msk_dir_name):\n",
    "\n",
    "    D_avg_loss = torch.mean(torch.FloatTensor(D_losses))\n",
    "    G_avg_loss = torch.mean(torch.FloatTensor(G_losses))\n",
    "\n",
    "    # avg loss values for plot\n",
    "    D_avg_losses.append(D_avg_loss)\n",
    "    G_avg_losses.append(G_avg_loss)\n",
    "\n",
    "    # Show result for test image\n",
    "    \n",
    "#     val_input, val_target, val_img_names, val_msk_names = val_loader.__iter__().__next__()\n",
    "\n",
    "#     gen_image = G(Variable(test_input))\n",
    "#     gen_image = gen_image.cpu().data\n",
    "#     plot_test_result(test_input, test_target, gen_image, epoch, save=True, ex_dir=ex_dir)\n",
    "    \n",
    "    save_model_fn=ex_dir+\"Seg_GAN_task2.pth\"\n",
    "        \n",
    "    torch.save({\"g_state_dict\":G.state_dict(),\n",
    "                \"d_state_dict\":D.state_dict(),\n",
    "                \"g_opt_dict\":G_optimizer.state_dict(),\n",
    "                \"d_opt_dict\":G_optimizer.state_dict(),\n",
    "                       },\n",
    "                       save_model_fn\n",
    "                      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
